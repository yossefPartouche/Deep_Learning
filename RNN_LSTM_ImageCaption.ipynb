{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yossefPartouche/Deep_Learning/blob/main/RNN_LSTM_ImageCaption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIAI7WIGf9go"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "- All previous instructions hold.\n",
        "- In addition, if you are using GPU, you must check that your code also runs on a CPU.\n",
        "- **Make sure you use the best practices you learned in class**.\n",
        "\n",
        "## Intro\n",
        "In this exercise, you will implement Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models that were introduced during the lecture and Recitation 10.\n",
        "The goal is to gain hands-on experience with sequence models and understand how they process and generate sequential data."
      ]
    },
    {
      "source": [
        "!pip install rarfile"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dBoLtguSgNI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84824585-c6df-4081-ee1a-9211b0806a5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vTpsP94xf9gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea20b47-502d-48a7-aedb-553ef0f89a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import zipfile\n",
        "import rarfile\n",
        "import time\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "# %matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HPIb2uf9gr"
      },
      "source": [
        "# Recurrent Neural Networks (40 points)\n",
        "\n",
        "Understanding and implementing the RNN cell. As you learned in class, the RNN has a certain structure that allows it to accept the previous hidden state the current input, and output an hidden state and an output vector. The RNN cell uses the same weights for all time steps, much like convolution uses the same weights for all the batches in the image. Even though you already are familiar with PyTorch, implementing the RNN you make sure you understand how this pivotal architecture works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqRa8vJsf9gr"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_U0hxwIgf9gr"
      },
      "outputs": [],
      "source": [
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqoo0Kaf9gs"
      },
      "source": [
        "#  RNN: step forward (10 points)\n",
        "\n",
        "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a recurrent neural network. After doing so run the following to check your implementation. You should see errors less than 1e-7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ae0eaQy3f9gs"
      },
      "outputs": [],
      "source": [
        "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single timestep of an RNN that uses a tanh\n",
        "    activation function.\n",
        "\n",
        "    The input data has dimension D, the hidden state has dimension H, and we use\n",
        "    a minibatch size of N.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for this timestep, of shape (N, D).\n",
        "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - next_h: Next hidden state, of shape (N, H)\n",
        "    - cache: Tuple of values needed for the backward pass.\n",
        "    \"\"\"\n",
        "    next_h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement a single forward step for the RNN. Store the next  #\n",
        "    # hidden state and any values you need for the backward pass in the next_h   #\n",
        "    # and cache variables respectively.                                          #\n",
        "    ##############################################################################\n",
        "    next_h = np.tanh((x @ Wx + prev_h @ Wh) + b)\n",
        "    cache = (x, next_h, Wx, Wh, prev_h)\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return next_h, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyc5GMR9f9gs",
        "outputId": "daeab229-7b46-4efa-ca4f-2e593cf58bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_h error:  6.292421426471037e-09\n"
          ]
        }
      ],
      "source": [
        "N, D, H = 3, 10, 4\n",
        "\n",
        "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
        "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.2, 0.4, num=H)\n",
        "\n",
        "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
        "expected_next_h = np.asarray([\n",
        "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
        "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
        "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
        "\n",
        "print('next_h error: ', rel_error(expected_next_h, next_h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29yUo5UWf9gs"
      },
      "source": [
        "#RNN: step backward (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A939ZyOOf9gs"
      },
      "outputs": [],
      "source": [
        "def rnn_step_backward(dnext_h, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for a single timestep of an RNN.\n",
        "\n",
        "    Inputs:\n",
        "    - dnext_h: Gradient of loss with respect to next hidden state\n",
        "    - cache: Cache object from the forward pass\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradients of input data, of shape (N, D)\n",
        "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
        "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradients of bias vector, of shape (H,)\n",
        "\n",
        "    PERSONAL INPUT: If you were to compute the loss at time t then:\n",
        "    supposed we calulated the loss of nex_h AND then worked backwards...\n",
        "    L = L(next_h) = L(tanh(a)) --->  âˆ‚L/âˆ‚next_h = dnext_h\n",
        "\n",
        "    where a = (x @ Wx + prev_h @ Wh) + b\n",
        "    \"\"\"\n",
        "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a single step of a RNN.      #\n",
        "    #                                                                            #\n",
        "    # HINT: For the tanh function, you can compute the local derivative in terms #\n",
        "    # of the output value from tanh.                                             #\n",
        "    ##############################################################################\n",
        "    x, next_h, Wx, Wh, prev_h = cache\n",
        "\n",
        "    # dtanh = âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚(next_h)â€¢âˆ‚(next_h)/âˆ‚a\n",
        "    dtanh = dnext_h * (1 - next_h**2)\n",
        "\n",
        "    #âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚tanh â€¢ âˆ‚tanh / âˆ‚x\n",
        "    dx = dtanh @ Wx.T\n",
        "\n",
        "    #âˆ‚L/âˆ‚prev_h = âˆ‚L/âˆ‚tanh â€¢ âˆ‚tangh/âˆ‚prev_h\n",
        "    dprev_h = dtanh @ Wh.T\n",
        "\n",
        "    #âˆ‚L/âˆ‚Wx = âˆ‚L/âˆ‚tanh â€¢ âˆ‚tanh/âˆ‚Wx\n",
        "    dWx = x.T @ dtanh\n",
        "\n",
        "    #âˆ‚L/âˆ‚Wh = âˆ‚L/âˆ‚tanh â€¢ âˆ‚tanh/âˆ‚Wh\n",
        "    dWh = prev_h.T @ dtanh\n",
        "\n",
        "    #âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚tanh â€¢ âˆ‚tanh/âˆ‚b\n",
        "    #Note here that since ^^^ becomes 1 vector then âˆ‚L/âˆ‚tanh is just summed up\n",
        "    db = np.sum(dtanh, axis=0)\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dprev_h, dWx, dWh, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXvqAb8tf9gs",
        "outputId": "9e819776-92e2-4393-b978-beb083a10b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  2.8730509825914645e-10\n",
            "dprev_h error:  1.931634610323001e-08\n",
            "dWx error:  6.586630396892865e-10\n",
            "dWh error:  9.131820369191576e-10\n",
            "db error:  8.569581669830872e-10\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1337)\n",
        "N, D, H = 4, 5, 6\n",
        "x = np.random.randn(N, D)\n",
        "h = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
        "\n",
        "dnext_h = np.random.randn(*out.shape)\n",
        "\n",
        "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
        "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
        "\n",
        "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXXpei43f9gt"
      },
      "source": [
        "# RNN: forward (10 points)\n",
        "Now that you have implemented the forward and backward passes for a single timestep of an RNN, you will combine these pieces to implement an RNN that process an entire sequence of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JnMSwFd4f9gt"
      },
      "outputs": [],
      "source": [
        "def rnn_forward(x, h0, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run an RNN forward on an entire sequence of data. We assume an input\n",
        "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
        "    size of H, and we work over a minibatch containing N sequences. After running\n",
        "    the RNN forward, we return the hidden states for all timesteps.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
        "    - h0: Initial hidden state, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
        "    - cache: Values needed in the backward pass\n",
        "    \"\"\"\n",
        "    h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for a RNN running on a sequence of    #\n",
        "    # input data. You should use the rnn_step_forward function that you defined  #\n",
        "    # above. You can use a for loop to help compute the forward pass.            #\n",
        "    ##############################################################################\n",
        "    N, T, D = x.shape\n",
        "    H = h0.shape[1]\n",
        "\n",
        "    h = np.zeros((N, T, H))\n",
        "    cache = []\n",
        "    prev_h = h0\n",
        "    for t in range(T):\n",
        "      x_t = x[:, t, :]\n",
        "      next_h, step_cache = rnn_step_forward(x_t, prev_h, Wx, Wh, b)\n",
        "      h[:, t, :] = next_h\n",
        "      cache.append(step_cache)\n",
        "      prev_h = next_h\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PlTKWrhf9gt",
        "outputId": "75184ec3-f97d-42d6-fea7-2850ad937019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h error:  7.728466151011529e-08\n"
          ]
        }
      ],
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "\n",
        "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
        "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.7, 0.1, num=H)\n",
        "\n",
        "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
        "expected_h = np.asarray([\n",
        "  [\n",
        "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
        "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
        "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
        "  ],\n",
        "  [\n",
        "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
        "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
        "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
        "print('h error: ', rel_error(expected_h, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxD7h4GDf9gt"
      },
      "source": [
        "# RNN: backward (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z-gv_bMPf9gt"
      },
      "outputs": [],
      "source": [
        "def rnn_backward(dh, cache):\n",
        "    \"\"\"\n",
        "    Compute the backward pass for an RNN over an entire sequence of data.\n",
        "\n",
        "    Inputs:\n",
        "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient of inputs, of shape (N, T, D)\n",
        "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
        "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradient of biases, of shape (H,)\n",
        "    \"\"\"\n",
        "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a RNN running an entire      #\n",
        "    # sequence of data. You should use the rnn_step_backward function that you   #\n",
        "    # defined above. You can use a for loop to help compute the backward pass.   #\n",
        "    ##############################################################################\n",
        "    N, T, H = dh.shape\n",
        "\n",
        "    D = x.shape[2]\n",
        "    dx = np.zeros((N, T, D))\n",
        "    dWx = np.zeros_like(Wx)\n",
        "    dWh = np.zeros_like(Wh)\n",
        "    db = np.zeros_like(b)\n",
        "    dprev_h = np.zeros((N, H))\n",
        "\n",
        "    for t in range(T-1, -1, -1):\n",
        "      dh_total = dh[:, t ,:] + dprev_h\n",
        "      dx_t, dprev_h, dWx_t, dWh_t, db_t = rnn_step_backward(dh_total, cache[t])\n",
        "\n",
        "      dx[:, t, :] = dx_t\n",
        "      dWx += dWx_t\n",
        "      dWh += dWh_t\n",
        "      db += db_t\n",
        "\n",
        "    dh0 = dprev_h\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dh0, dWx, dWh, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHhvTNqif9gt",
        "outputId": "2db27f94-7efe-4784-fd27-7693b2657e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  2.5571085568563326e-09\n",
            "dh0 error:  5.423206039675471e-10\n",
            "dWx error:  3.579829809212802e-09\n",
            "dWh error:  7.994186410119177e-09\n",
            "db error:  5.755566528286172e-10\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1337)\n",
        "\n",
        "N, D, T, H = 2, 3, 10, 5\n",
        "\n",
        "x = np.random.randn(N, T, D)\n",
        "h0 = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
        "\n",
        "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:18:23.856501Z",
          "start_time": "2022-11-04T12:18:23.764777Z"
        },
        "id": "0bvtOWB3f9gt"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:18:23.856501Z",
          "start_time": "2022-11-04T12:18:23.764777Z"
        },
        "id": "sckGwB5Of9gt"
      },
      "source": [
        "##**Question 1 :**\n",
        "When using an RNN on long sequences, what could happen? What causes this to happen?   **(3 Points)**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "your answer\n",
        "When using an RNN on long sequences, the model tens to **forget early information**. This happend because the hidden state is updated recursively using a **bounded, non-linear transformation**.\n",
        "\n",
        "$h_t = \\tanh(W_hh_{t-1} + W_xx_t + b)$\n",
        "\n",
        "Over time, the hidden state is repearedly transformed by the same weighty matrix andf nonlinearity. Since `tanh` is **bounded and non-invertible**, different past staes can be mapped to similar hidden representation, leading to information being compressed.\n",
        "\n",
        "Furthermore, during BPTT, the gradient of the loss wrt earlier hidden states, and since these factors are less than or equal to 1 (due to `tanh`) this will lead to vanishing gradient thus making the model unable to learn about long-range dependencies.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:18:23.856501Z",
          "start_time": "2022-11-04T12:18:23.764777Z"
        },
        "id": "ZiqdGy_5f9gt"
      },
      "source": [
        "##**Question 2:**\n",
        "Could this problem be solved by a different model? How does it accomplish this?   **(3 Points)**\n",
        "\n",
        "\n",
        "---\n",
        "This can be solved (though not for extremely long sequences) through LSTM (Long Short Term Memory).\n",
        "LSTM introduces a cell state $C_t$ which allows information to flow across many time steps with minimal modiciation. This is acheived using three gates:\n",
        "\n",
        "1. Forget Gate - Learning over time what aspects of the information collected during the cumulative past steps are relevant and which should be \"thrown away\".\n",
        "2. Input Gate - Learning based on the current input and the past information what current information should be stored and how much of this information should we keep\n",
        "\n",
        "3. Output Gate - Learning based on the current input and the past information what should be the current output.\n",
        "\n",
        "Using the results of the output Gate and the latest long term-memory we create our new updated hidden state $h_t$\n",
        "\n",
        "\n",
        "Using addition of the results from the Forget Gate and the Input gate we produce the current time step of the long term-memory $C_t$. Since this cell state is created via addition process it survives during periods of BPTT and thus enable better results on long sequences dependencies.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 3:**\n",
        "What does the hidden state represent intuitively?  **(4 Points)**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Intuitively the hidden state represents the model's internal context at a given time step. providing a compact way to summarize its past inputs and continously update based on current inputs when new data arrives and uses this for future predictions. It does this by encoding the most useful features of the sequence so far.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ATnX0yxt2Log"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZItvc-zf9gt"
      },
      "source": [
        "# Building Image Captioning in PyTorch (50 points)\n",
        "\n",
        "- The goal of image captioning is to describe a given image using natural language.\n",
        "- Using neural networks, we can partition the problem into 2 separate challenges.\n",
        "1. We need to extract meaningful features regarding the image that would help us describe it.\n",
        "2. We need to generate a sequence of words that best fit those features. Luckily, the flexability of neural networks allows us to take a CNN architecture and connect it directly to a LSTM network. We only need to provide proper labels to train the new network we created.\n",
        "\n",
        "- For this exercise, you will be provided with pretrained networks for both feature extraction and sentence generation, and you will connect the different components needed to make image captioning work.\n",
        "\n",
        "- First, we define the feature extractor and the recurrent model seperately. The feature extractor takes an image and produces a vector representation of the image features. As those features hold information about the image, we will use that vector as the input for our recurrent model. The RNN will produce the image captioning using an LSTM architecture."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please upload the folder to your drive and adjust the path if necessary.\n",
        "drive.mount('/content/drive')\n",
        "my_path = '/content/drive/MyDrive/deep_hw4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIve2yO_5ku-",
        "outputId": "6fb47e63-f73f-4aa5-a48c-b0f7d334b81d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rarfile.UNRAR_TOOL = \"/usr/bin/unrar\"  # Ensure the unrar tool is installed"
      ],
      "metadata": {
        "id": "3fzruDaQ8X4O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMU1LbSSf9gu",
        "outputId": "994f7bc0-6d17-4809-c4d4-8944da9308ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Missing parts: /content/drive/MyDrive/deep_hw4/models/models.part01.rar, /content/drive/MyDrive/deep_hw4/models/models.part02.rar, /content/drive/MyDrive/deep_hw4/models/models.part03.rar, /content/drive/MyDrive/deep_hw4/models/models.part04.rar, /content/drive/MyDrive/deep_hw4/models/models.part05.rar, /content/drive/MyDrive/deep_hw4/models/models.part06.rar, /content/drive/MyDrive/deep_hw4/models/models.part07.rar, /content/drive/MyDrive/deep_hw4/models/models.part08.rar, /content/drive/MyDrive/deep_hw4/models/models.part09.rar\n"
          ]
        }
      ],
      "source": [
        "# unzipping the pretrained models\n",
        "# with zipfile.ZipFile(os.path.join('models', 'pretrained_model.zip'), 'r') as zip_ref:\n",
        "#     zip_ref.extractall('models')\n",
        "\n",
        "# Define paths\n",
        "rar_parts = [f'{my_path}/models/models.part0{i}.rar' for i in range(1, 10)]  # Adjust range as needed\n",
        "output_dir = 'models'\n",
        "\n",
        "# Check if all parts exist\n",
        "missing_parts = [part for part in rar_parts if not os.path.exists(part)]\n",
        "if missing_parts:\n",
        "    print(f\"Error: Missing parts: {', '.join(missing_parts)}\")\n",
        "else:\n",
        "    try:\n",
        "        # Open the first part and extract all files\n",
        "        with rarfile.RarFile(rar_parts[0], 'r') as rf:\n",
        "            rf.extractall(output_dir)\n",
        "            print(f\"Extracted all files to '{output_dir}' successfully.\")\n",
        "    except rarfile.Error as e:\n",
        "        print(f\"An error occurred while extracting the RAR files: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5hc6UJif9gu"
      },
      "outputs": [],
      "source": [
        "# unzipping vocabulary\n",
        "with zipfile.ZipFile(os.path.join(my_path, 'data/vocab.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayFH8nMMf9gu"
      },
      "outputs": [],
      "source": [
        "conv_path = 'models/encoder-5-3000.pkl'\n",
        "lstm_path = 'models/decoder-5-3000.pkl'\n",
        "vocab_path   = 'data/vocab.pkl'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Use the following code to check if all the files are in place\n",
        "print(\"conv_path \", 'âœ“' if os.path.isfile(conv_path) == True else 'âœ—')\n",
        "print(\"lstm_path \", 'âœ“' if os.path.isfile(lstm_path) == True else 'âœ—')\n",
        "print(\"vocab_path \", 'âœ“' if os.path.isfile(vocab_path) == True else 'âœ—')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:24:56.821196Z",
          "start_time": "2022-11-04T12:24:56.758302Z"
        },
        "id": "QoKXGXBSf9gu"
      },
      "source": [
        "## Implementing image captioning model **(40 points)**.\n",
        "\n",
        "As training a multimodal classifier could take some time and resources, we spared you the training phase this time â³.\n",
        "\n",
        "In this exercise,Â we use a pretrained model to solve the image captioning task. Using pretrained models is a common practice in the deep learningÂ community and it's important to be aware of such techniques to save time and energy.\n",
        "\n",
        "\n",
        "In previous cells, we unzipped the necessaryÂ files, but in order to be able to load the models (and then use them) it is required to build the same PyTorch model as the pretrained model.\n",
        "\n",
        "- **ConvNet architecture:** resnet152 (without last fc layer) -> fc layer -> BatchNorm1d\n",
        "\n",
        "- **LSTM architecture:** LSTM -> linear -> embed\n",
        "\n",
        "We added more detailed instructions in the next cells, please make sure you follow them carefully.\n",
        "\n",
        "**Please make sure you construct your models based on the sizes we provided.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6fpHa6_f9gu"
      },
      "outputs": [],
      "source": [
        "embed_size   = 256      # dimension of word embedding vectors\n",
        "hidden_size  = 512      # dimension of lstm hidden states\n",
        "num_layers   = 1        # number of layers in lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPP5OCjzf9gu"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(ConvNet, self).__init__()\n",
        "        resnet = models.resnet152() # construct an nn.Sequential model without the last resnet152 layer\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        # make sure you define each of the None parameters\n",
        "        self.linear = None\n",
        "        self.bn = None\n",
        "        #############################################################################\n",
        "        # TO DO:                                                                    #\n",
        "        # create a new sequential model which includes the resnet and               #\n",
        "        # add a new fully connected layer that outputs a vector with the size of    #\n",
        "        # the wanted embedding. Next, you should add a batchnorm layer with         #\n",
        "        # momentum=0.01 (BatchNorm1d parameter).                                    #\n",
        "        # This function has no return value.                                        #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "    def forward(self, images):\n",
        "        #############################################################################\n",
        "        # TO DO:                                                                    #\n",
        "        # Define the forward propagation. You need to pass an image through the     #\n",
        "        # network and extract the feature vector. In this case, when using a        #\n",
        "        # perdefined network, you don't want to change it's weights.                #\n",
        "        # The rest of the layers you defined should accepts gradients for them to   #\n",
        "        # improve during training. Make sure you are inputing a correct shape       #\n",
        "        # to the batchnorm layer.                                                   #\n",
        "        # This function returns the features of the image.                          #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.max_seg_length = max_seq_length\n",
        "        self.embed = None\n",
        "        self.lstm = None\n",
        "        self.linear = None\n",
        "        #############################################################################\n",
        "        # TO DO:                                                                    #\n",
        "        # Define the hyper-parameters and the layers of the pretrained LSTM.        #\n",
        "        # Create an Embedding layer that accepts the output of the                  #\n",
        "        # feature extractor.  Next, the built-in LSTM architecture in PyTorch.nn    #\n",
        "        # with the proper inputs (use the built-in documentation tool in Jupyter    #\n",
        "        # or just look at the official documentation online).                       #\n",
        "        # Define an additional linear layer that comes after the LSTM and outputs   #\n",
        "        # a vector that will support the size of our vocabulary.                    #\n",
        "        # This function has no return value.                                        #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "    def sample(self, features, states=None):\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        #############################################################################\n",
        "        # TO DO:                                                                    #\n",
        "        # Generate captions for a given image features.                             #\n",
        "        # First, obtain the output of the LSTM network.        #\n",
        "        # Next, use the hidden states to obtain the most probable word and store    #\n",
        "        # all the word predictions in the sampled_ids list. Don't forget to update  #\n",
        "        # the inputs for each timestep to continue making predictions based on the  #\n",
        "        # words you are alreaedy predicted.                                         #\n",
        "        # Make sure you keep track of the dimensions of the inputs and outputs,     #\n",
        "        # since PyTorch expects tensors with a batch dimension. You can use the     #\n",
        "        # methods .squeeze() and .unsqueeze()                                       #\n",
        "        # This function returns the list of predicted words.                        #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "        return sampled_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha6wvjsgf9gu"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize([224, 224], Image.Resampling.LANCZOS)\n",
        "\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpjOoE3nf9gv"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sors9RUlf9gv"
      },
      "outputs": [],
      "source": [
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unleash your creativity! ðŸ’«\n",
        "\n",
        "Choose an image that really gets your imagination going! Let's try to wow Ron with some funny pictures that will make the homework checking process a whole lot more enjoyable! ðŸ¤©\n"
      ],
      "metadata": {
        "id": "_vkFOCpt3kJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mA3w15lf9gv"
      },
      "outputs": [],
      "source": [
        "# Build models\n",
        "conv = ConvNet(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
        "lstm = LSTM(embed_size, hidden_size, len(vocab), num_layers)\n",
        "conv = conv.to(device)\n",
        "lstm = lstm.to(device)\n",
        "\n",
        "# Load the trained model parameters\n",
        "conv.load_state_dict(torch.load(conv_path))\n",
        "lstm.load_state_dict(torch.load(lstm_path))\n",
        "\n",
        "# Prepare an image\n",
        "image_path = os.path.join(my_path, 'data/pic.jpg')\n",
        "image = load_image(image_path, transform)\n",
        "image_tensor = image.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCfOQS9Zf9gv"
      },
      "outputs": [],
      "source": [
        "# Generate an caption from the image\n",
        "# Important Note - this piece of code may not work for all implementations. You may need to adjust it a bit so it\n",
        "# will produce the desired output (image + caption)\n",
        "feature = conv(image_tensor)\n",
        "sampled_ids = lstm.sample(feature)\n",
        "sampled_ids = sampled_ids[0].cpu().numpy()\n",
        "\n",
        "# Convert word_ids to words\n",
        "sampled_caption = []\n",
        "for word_id in sampled_ids:\n",
        "    word = vocab.idx2word[word_id]\n",
        "    sampled_caption.append(word)\n",
        "    if word == '<end>':\n",
        "        break\n",
        "sentence = ' '.join(sampled_caption)\n",
        "\n",
        "# Print out the image and the generated caption\n",
        "print(sentence)\n",
        "image = Image.open(image_path)\n",
        "plt.imshow(np.asarray(image));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:21:49.196557Z",
          "start_time": "2022-11-04T12:21:49.127510Z"
        },
        "id": "UX1ZHdYbf9gv"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-04T12:21:49.196557Z",
          "start_time": "2022-11-04T12:21:49.127510Z"
        },
        "id": "cKfY3gUzf9gv"
      },
      "source": [
        "##**Question 4:**\n",
        "Could you think of scenarios where the model fails? Why would that happen? give at least 4 resones **(4 Points)**\n",
        "\n",
        "We encourageÂ youÂ to test it on different images (jpg).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "your answer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 5:**\n",
        "Provide one example of a good caption and one that the model didn't manage to work well with. **(1 Point)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "your answer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0YRYnw8M6sRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 6:**\n",
        "What happens if the forget gate is always close to zero? Close to one? **(2 Points)**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "your answer\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CwUosopl47Qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 7:**\n",
        "How does the LSTM mitigate the vanishing gradient problem? **(3 Points)**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "your answer\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DIx1Pdnf5KUA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPrYON6n6-ko"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}