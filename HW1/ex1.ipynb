{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise 1: Linear Image Classifier\n","\n","In this exercise you will implement a linear image classifier while getting familiar with `numpy` and the benefits of vectorized operations in Python.\n","\n","## Environment\n","- Platform: Google Colab, CPU runtime (no GPU).\n","- Set a global random seed for reproducibility (e.g., np.random.seed(42)).\n","\n","\n","## Read the following instructions carefully:\n","\n","1. This jupyter notebook contains all the step by step instructions needed for this exercise. Fill in the missing parts (TODO) in the linear_models.py file and int this notebook.\n","2. Write **efficient vectorized** code whenever instructed.\n","3. You are responsible for the correctness of your code and should add as many tests as you see fit (tests will not be graded nor checked).\n","4. Do not change the functions we provided you.\n","5. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n","6. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden.\n","6. Your code must run without errors.\n","7. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support).\n","\n","\n","## Submission guidelines:\n","- What to submit: only a zip named HW1_<YOUR_ID>.zip containing: (1) hw1.ipynb (2) linear_models.py\n","- Your submitted notebook should **run without problems**.\n","- Please submit your **executed** (fully run) notebook, including all outputs (plots, printed results, etc.). Make sure that all cells have been run in order from top to bottom before submission.\n","\n","## Academic integrity\n","\n","You may discuss ideas, but all submitted code must be your own. Cite any external snippets."],"metadata":{"id":"af95vouo-TzT"}},{"cell_type":"markdown","source":["Q: How do I make sure everything works before I submit?\n","\n","A: You should restart your kernel and rerun all cells. Make sure you get the desired output and that you meet exercise requirements. **This is an important step. You should include your desired outputs in the output cells to make your code easier to understand.**\n","\n","Q: Changes I do to the code in the linear_models.py file don't update in the ipynb file.\n","\n","A: Instead of doing:\n","\n","```\n","from linear_models import LinearClassification\n","...\n","linear_classification = LinearClassification(...)\n","```\n","In your code just use:\n","```\n","linear_classification = linear_models.LinearClassification(...)\n","```\n","This will cause the changes to load automatically into the notebook (if you ran the cell with the ```watch_module``` function)\n"],"metadata":{"id":"qSSqrYKCCiZO"}},{"cell_type":"markdown","source":["## Setting up the project folder\n"],"metadata":{"id":"3px0-S3JIClc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762968435524,"user_tz":-120,"elapsed":21581,"user":{"displayName":"Ron Tohar","userId":"10058177826211754471"}},"outputId":"752616ca-6e8e-403f-e139-f3d755b2b9a8","id":"jTJXcEXJICld"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","import os\n","\n","# TODO: Replace 'your_project_folder' with the actual path to your project folder in Google Drive\n","# project_folder = '/content/drive/MyDrive/your_project_folder'\n","sys.path.append(project_folder)\n","\n","print(f\"Added {project_folder} to system path.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762968469760,"user_tz":-120,"elapsed":54,"user":{"displayName":"Ron Tohar","userId":"10058177826211754471"}},"outputId":"edbfe4ff-0daf-4d26-b489-369e6f6742c5","id":"s3Z18i7oICle"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Added /content/drive/MyDrive/Reichman_CV/Ex1_final to system path.\n"]}]},{"cell_type":"code","source":["# Set up automatic reloading for linear_models.py\n","import importlib\n","import threading\n","import time\n","\n","# TODO: Ensure linear_models.py is in the project_folder specified above\n","module_name = 'linear_models'\n","module_path = os.path.join(project_folder, f'{module_name}.py')\n","\n","def watch_module(module, path, interval=2):\n","    last_mtime = os.path.getmtime(path)\n","    def watch():\n","        nonlocal last_mtime\n","        while True:\n","            try:\n","                mtime = os.path.getmtime(path)\n","                if mtime > last_mtime:\n","                    importlib.reload(module)\n","                    print(f\"[auto-reloaded {module.__name__}]\")\n","                    last_mtime = mtime\n","            except Exception as e:\n","                print(e)\n","            time.sleep(interval)\n","    threading.Thread(target=watch, daemon=True).start()\n","\n","try:\n","    # Import the module initially\n","    linear_models = importlib.import_module(module_name)\n","    print(f\"Imported {module_name}.\")\n","    # Start watching the module file for changes\n","    watch_module(linear_models, module_path)\n","    print(f\"✅ Watching {module_path} for changes.\")\n","except ImportError:\n","    print(f\"Error: Could not import module {module_name}. Make sure '{module_path}' exists and contains the module.\")\n","except FileNotFoundError:\n","     print(f\"Error: Module file not found at '{module_path}'. Make sure the path is correct.\")"],"metadata":{"id":"6jFw83pSICle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"ErHFbc3sIClf"}},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import urllib.request\n","import tarfile\n","import zipfile\n","from random import randrange\n","from functools import partial\n","import itertools\n","import time\n","# from linear_models import * # It is okay if you get \"import cannot be resolved\"\n","import linear_models\n","import importlib\n","importlib.reload(linear_models)\n","from google.colab import drive\n","import threading\n","from typing import Dict, Tuple, Iterable, Optional, Any\n","\n","\n","# specify the way plots behave in jupyter notebook\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 3.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n"],"metadata":{"id":"hHHK2v1cIClf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Blank cell for pip install\n","### EXAMPLE\n","# %pip install numpy (in some platforms or versions you will need to use '!' instead of '%'. like this - !pip install numpy)\n","\n","\n","### Make sure to restart the note book after the installs ###"],"metadata":{"id":"9MHi24etIClf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"THIOP02kEEb3"}},{"cell_type":"markdown","source":["## Data download and processing Helper Code"],"metadata":{"id":"-AJDYUj7EJoQ"}},{"cell_type":"code","source":["def download_and_extract(url, download_dir):\n","    \"\"\"\n","    Download and extract the CIFAR-10 dataset if it doesn't already exist.\n","\n","    Parameters\n","    ----------\n","    url : str\n","        Internet URL for the tar-file to download.\n","        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","    download_dir : str\n","        Directory where the downloaded file will be saved and extracted.\n","        Example: \"data/CIFAR-10/\"\n","\n","    Returns\n","    -------\n","    None\n","    \"\"\"\n","\n","    # Determine filename and full path where the file will be saved\n","    filename = url.split('/')[-1]\n","    file_path = os.path.join(download_dir, filename)\n","\n","    # Check if dataset is already downloaded (and extracted)\n","    if not os.path.exists(file_path):\n","       # Create the directory if it doesn’t exist\n","        if not os.path.exists(download_dir):\n","            os.makedirs(download_dir)\n","\n","        # Download the dataset\n","        print(\"Downloading, This might take several minutes.\")\n","        last_update_time = time.time()\n","        file_path, _ = urllib.request.urlretrieve(url=url, filename=file_path)\n","\n","        print()\n","        print(\"Download finished. Extracting files.\")\n","\n","        # Extract the dataset\n","        if file_path.endswith(\".zip\"):\n","            # Unpack the zip-file.\n","            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n","        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n","            # Unpack the tar-ball.\n","            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n","\n","        print(\"Done. Dataset is ready!\")\n","    else:\n","        print(\"Dataset already downloaded and unpacked.\")\n","        print(\"If something seems wrong, delete the folder and re-run.\")\n","\n","\n","def load_CIFAR_batch(filename):\n","    ''' Load a single batch of the CIFAR-10 dataset.'''\n","    with open(filename, 'rb') as f:\n","        datadict = pickle.load(f, encoding = 'latin1')\n","        X = datadict['data']\n","        Y = datadict['labels']\n","\n","        # Reshape and transpose: original shape (10000, 3072)\n","        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n","        Y = np.array(Y)\n","        return X, Y\n","\n","\n","def load(ROOT):\n","    ''' Load all training and test batches of CIFAR-10.'''\n","    xs = []\n","    ys = []\n","    for b in range(1, 6):\n","        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","        X, Y = load_CIFAR_batch(f)\n","        xs.append(X)\n","        ys.append(Y)\n","    Xtr = np.concatenate(xs)\n","    Ytr = np.concatenate(ys)\n","    del X, Y\n","    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","    return Xtr, Ytr, Xte, Yte"],"metadata":{"id":"vrPv3ZJhD0ee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Download\n","\n","In this section, we will download and extract the **CIFAR-10** dataset if it is not already available locally.\n","\n","- The dataset will be saved under: `datasets/cifar10/`\n","- If it’s already there, the script will **skip downloading**.\n","- This process may take a few minutes the first time you run it."],"metadata":{"id":"baWBBkAOEQnX"}},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# Data Download (Run once)\n","# ---------------------------------------------------------------------\n","URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","PATH = 'datasets/cifar10/'\n","download_and_extract(URL, PATH)"],"metadata":{"id":"3c7ndZ5BERBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# Load the full CIFAR-10 dataset from local path\n","# ---------------------------------------------------------------------\n","CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n","X_train, y_train, X_test, y_test = load(CIFAR10_PATH)  # load the entire data"],"metadata":{"id":"qobX3bN8ETmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ❀ Q1: Exploratory Data Analysis (EDA) ❀\n","\n","Use the uplodaed **CIFAR-10** dataset.\n","1. Print the **shapes** of the training and test sets.\n","2. Display the **number of classes** and their names.\n","3. Show the **class distribution** in the training set.\n","\n","**(5 points)**"],"metadata":{"id":"3ZBW7aMVEZgr"}},{"cell_type":"code","source":["# TODO - your answer\n","\n","\n","#(1)\n","\n","#(2)\n","\n","#(3)"],"metadata":{"id":"9IhyHvB0Ez7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing Part 1"],"metadata":{"id":"teVbrutIEg6H"}},{"cell_type":"markdown","source":["We have included several image processing functions. Notice the following in particular: we created an additional validation dataset you need to use for hyperparameter optimization.\n","\n","\n","\n"],"metadata":{"id":"V6yhdTzlEk98"}},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# Filter the dataset to include only the target classes\n","# ------------------------------------------/---------------------------\n","TARGET_CLASSES = [2, 3, 4]\n","classes = ['bird', 'cat', 'deer']\n","train_mask = np.isin(y_train, TARGET_CLASSES)\n","test_mask  = np.isin(y_test,  TARGET_CLASSES)\n","\n","X_train = X_train[train_mask]\n","y_train = y_train[train_mask]\n","X_test  = X_test[test_mask]\n","y_test  = y_test[test_mask]\n","\n","# ---------------------------------------------------------------------\n","# Relabel to {0,1,2} so predictions match labels\n","# ---------------------------------------------------------------------\n","label_map = {orig: i for i, orig in enumerate(TARGET_CLASSES)}\n","y_train = np.vectorize(label_map.get)(y_train)\n","y_test  = np.vectorize(label_map.get)(y_test)\n","\n","# ------------------------------------------------------------\n","# Define sizes for training / validation / test splits\n","# ------------------------------------------------------------\n","num_training   = 10000\n","num_validation = 1000\n","num_testing    = 1000\n","\n","# ---------------------------------------------------------------------\n","# Create subset\n","# ---------------------------------------------------------------------\n","\n","# Training subset\n","mask = range(num_training)\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","# Validation subset\n","mask = range(num_validation)\n","X_val = X_test[mask]\n","y_val = y_test[mask]\n","\n","# Test subset\n","mask = range(num_validation, num_validation + num_testing)\n","X_test = X_test[mask]\n","y_test = y_test[mask]\n","\n","# ---------------------------------------------------------------------\n","# Convert to float64 (optional)\n","# ---------------------------------------------------------------------\n","X_train = X_train.astype(np.float64)\n","X_val   = X_val.astype(np.float64)\n","X_test  = X_test.astype(np.float64)\n","\n","print(\"Shapes ->\",\n","      \"X_train\", X_train.shape, \"y_train\", y_train.shape,\n","      \"X_val\",   X_val.shape,   \"y_val\",   y_val.shape,\n","      \"X_test\",  X_test.shape,  \"y_test\",  y_test.shape)"],"metadata":{"id":"4_uffNpYEaBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch(X, y, n=1000):\n","    \"Randomly select a batch of samples from the dataset.\"\n","    rand_items = np.random.randint(0, X.shape[0], size=n)\n","    images = X[rand_items]\n","    labels = y[rand_items]\n","    return images, labels\n","\n","\n","def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n","    \"Create a grid of random images (or flattened vectors) from dataset.\"\n","    if random_flag:\n","        rand_items = np.random.randint(0, x.shape[0], size=n)\n","    else:\n","        rand_items = np.arange(0, x.shape[0])\n","    images = x[rand_items]\n","    labels = y[rand_items]\n","    if convert_to_image:\n","        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n","    else:\n","        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n","    print('\\t'.join('%9s' % classes[labels[j]] for j in range(n)))\n","    return grid\n","\n","\n","def vec_2_img(x):\n","    \"\"\" Convert a flattened CIFAR-10 image vector back to a (32, 32, 3) RGB image.\n","    Removes bias term if present.\"\"\"\n","    x = np.reshape(x[:-1], (32, 32, 3))\n","    return x"],"metadata":{"id":"qCOken3OEqDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quick sanity check preview\n","X_batch, y_batch = get_batch(X_test, y_test, 100)\n","plt.imshow(make_random_grid(X_batch, y_batch, n=4, convert_to_image=False))\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"OwSRLSMXVhhX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing Part 2\n","\n","In this section, we apply several image preprocessing steps to prepare the data for our linear models:\n","\n","1. Mean Subtraction - We subtract the mean image (computed over the training set) from every image.\n","\n","2. Flattening - Each image originally has the shape (32 × 32 × 3).\n","We flatten it into a single vector of 3072 features.\n","\n","3. Bias Trick - We append an additional constant feature (set to 1) to every image vector."],"metadata":{"id":"SwXAps6ZVscx"}},{"cell_type":"code","source":["# =========================\n","# Final data preprocessing\n","# =========================\n","\n","# 1) subtract the TRAIN mean image (feature-wise)\n","mean_image = np.mean(X_train, axis=0, keepdims=True)\n","X_train = X_train - mean_image\n","X_val   = X_val   - mean_image\n","X_test  = X_test  - mean_image\n","\n","# 2) flatten HxWxC -> D\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_val   = X_val.reshape(X_val.shape[0], -1)\n","X_test  = X_test.reshape(X_test.shape[0], -1)\n","\n","# 3) add a bias term (last feature = 1)\n","X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","X_val   = np.hstack([X_val,   np.ones((X_val.shape[0],   1))])\n","X_test  = np.hstack([X_test,  np.ones((X_test.shape[0],  1))])\n","\n","print(f\"Shape of Training Set: {X_train.shape}\")   # (N_train, D+1)\n","print(f\"Shape of Validation Set: {X_val.shape}\")   # (N_val,   D+1)\n","print(f\"Shape of Test Set: {X_test.shape}\")        # (N_test,  D+1)\n","\n","# Ensure labels are 0..C-1\n","num_classes = int(np.max(y_train)) + 1\n","assert set(np.unique(y_train)) <= set(range(num_classes)), \"y must be in 0..C-1\""],"metadata":{"id":"tSw6ciCCVvPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ❀ Q2: Understanding the Image Preprocessing ❀\n","\n","Before moving on, let’s review the preprocessing steps applied to our dataset.\n","\n","Answer the following questions:\n","\n","1. Mean Subtraction\n","\n","- Why do we subtract the mean image (computed from the training set) from every image?\n","\n","2. Flattening\n","\n","- Why do we need to flatten the images into 3072-dimensional vectors before training a linear model?\n","\n","\n","3. Bias Trick\n","\n","- Why do we add a constant 1 to every image vector (known as the bias trick)?\n","\n","**(5 points)**\n","\n"],"metadata":{"id":"430XCO3cV462"}},{"cell_type":"markdown","source":["TODO - your answer\n","\n","1.  \n","\n","2.\n","\n","3.\n"],"metadata":{"id":"Yq09NPlNjwzo"}},{"cell_type":"markdown","source":["# Linear classifier: mapping images to scores\n","\n","- During this exercise, we will maintain a python class with basic functionality (such as training the model).\n","\n","- The linear classifiers we will build will inherit some functionality from that class and will change several functions (such as the loss function, for example).\n","\n","- Read the code in the next cell and make sure you understand it.\n","\n","- Need a quick Python OOP refresher? This short tutorial on classes and objects can help [short classes in python tutorial](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/) .\n","\n","\n","**TODO:**  \n","Complete the missing code in the dedicated places in the linear_models.py file (Linear Classifier class).\n","- `train` **(10 points)**\n","- `calc_accuracy` **(5 points)**\n","\n","\n","\n"],"metadata":{"id":"nZmfdHynXz-_"}},{"cell_type":"markdown","source":["## Linear perceptron\n","\n","Our first linear classifier is the **Perceptron**, a foundational model in machine learning.\n","\n","It computes a **linear score** for each class:\n","\n","$$ f(x_i; W, b) = W \\cdot x_i + b $$\n","\n","- W : a weight matrix of shape *(num_features × num_classes)*  \n","- b : a bias term (included using the **bias trick**)\n","\n","Each column in \\( W \\) corresponds to one class.  \n","The **final prediction** is the class with the **highest score**.\n","\n","---\n","\n","### Multiclass vs. Binary\n","\n","- **Multiclass Case:**  \n","  `W` is a matrix. Each row corresponds to a feature, each column to a class.  \n","  Prediction is made via:\n","\n","  $$ \\hat{y_i} = \\arg\\max_j (W_j \\cdot x_i) $$\n","\n","- **Binary Case (special case of 2 classes):**  \n","  You can either:\n","  1. Treat it as a **2-class multiclass** problem (use 2 columns in `W`),  \n","     prediction = class with the highest score.  \n","  2. Or use a **single weight vector** `w`, where:\n","    $$ \\text{predict } 1 \\text{ if } w \\cdot x + b > 0, \\text{ else } 0 $$\n","\n","In this exercise, our implementation is **multiclass**, supporting more than 2 classes.\n","\n","\n","### Objective\n","\n","Your goal is to **learn** the parameters `W` and `b` such that the classifier correctly predicts the labels in the training set.  We’ll do this by **minimizing the Perceptron loss**, which penalizes misclassified examples.\n","\n","\n","**TODO:**  \n","We’ll now implement the `LinearPerceptron` class, which inherits from `LinearClassifier`.\n","Implement the `init` and the `predict` functions.\n","\n","\n","**(5 Points)**"],"metadata":{"id":"FTtAMTw3ZSLG"}},{"cell_type":"code","source":["classifier = linear_models.LinearPerceptron(X_train, y_train)\n","y_pred = classifier.predict(X_test)"],"metadata":{"id":"eoqQfVkGV2Xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# Visualization exmaple: Compare Predicted vs True Labels on Test Set\n","# ---------------------------------------------------------------------\n","\n","num_show = 8 # you can change the num of exmaples\n","idxs = np.random.choice(len(X_test), size=num_show, replace=False)\n","\n","fig, axes = plt.subplots(1, num_show, figsize=(16, 3))\n","\n","mean_flat = mean_image.reshape(-1)\n","if mean_flat.size > 3072:\n","    mean_flat = mean_flat[:3072]\n","mean_img_3d = mean_flat.reshape(32, 32, 3)\n","\n","for ax, i in zip(axes, idxs):\n","    vec = X_test[i].reshape(-1)\n","    if vec.size > 3072:\n","        vec = vec[:3072]   # strip bias\n","        img = vec[:3072].reshape(32, 32, 3) + mean_img_3d\n","        img = (img - img.min()) / (img.max() - img.min())  # scale to 0–1\n","    true_lbl = classes[y_test[i]]\n","    pred_lbl = classes[y_pred[i]]\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","    ax.set_title(f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\",\n","                 color=\"green\" if y_pred[i] == y_test[i] else \"red\")\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"JLlPAcXLeZ5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation\n","\n","Use the class method `calc_accuracy`"],"metadata":{"id":"OuwnMDaBemBy"}},{"cell_type":"code","source":["print(\"model accuracy: \", classifier.calc_accuracy(X_train, y_train))"],"metadata":{"id":"3BF0G2pTdRZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ❀ Q3 ❀\n","\n","\n","Explain why the accuracy on the training dataset is low\n","**(5 points)**\n"],"metadata":{"id":"ZzXtHEcHetcN"}},{"cell_type":"markdown","source":["TODO - your answer\n"],"metadata":{"id":"VRkLmzS0kEkN"}},{"cell_type":"markdown","source":["## Perceptron loss\n","\n","In this section, you’ll implement the **Perceptron loss**\n","- Notice the loss method for each class is just a call for the loss function written in the next cell.\n","- Once you are finished with implementation, everything should work.\n","\n","TODO:\n","\n","- First, complete the function `perceptron_loss_naive` in the linear_models.py file. This function takes as input the weights, data, labels and outputs the calculated loss as a single number and the gradients with respect to W.  \n","\n","- Test your function using the following cells\n","\n","**(15 points)**"],"metadata":{"id":"MZBaddI2e2Ln"}},{"cell_type":"code","source":["W = np.random.randn(3073, 3) * 1e-4"],"metadata":{"id":"oxyZMtoFw54U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Quick test\n","# =========================\n","%%time\n","loss_naive, grad_naive = linear_models.perceptron_loss_naive(W, X_val, y_val)\n","print(\"Loss:\", loss_naive)"],"metadata":{"id":"zmAUyLbDffqF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TODO:**\n","\n","once your code works, complete the function `softmax_cross_entropy` and compare the results of the two functions using the cell below.\n","\n","**(10 points)**"],"metadata":{"id":"8En8I7oif0P0"}},{"cell_type":"code","source":["# =========================\n","# Quick test\n","# =========================\n","%%time\n","loss, _ = linear_models.softmax_cross_entropy(W, X_val, y_val)\n","print ('loss: %f' % (loss))"],"metadata":{"id":"X8wLv9Y1gDQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note:\n","- You might not see big changes in time due to other computing factors. In big enough datasets it would be crucial to a the vectorized version, which you will implement soon.\n","\n","We have obtained an efficient function for loss and gradient calculation and we can now train our network.\n"],"metadata":{"id":"UNpVJwcdgJMI"}},{"cell_type":"code","source":["%%time\n","perceptron = linear_models.LinearPerceptron(X_train, y_train)\n","loss_history = perceptron.train(X_train, y_train, learning_rate=1e-7,\n","                                num_iters=1500, verbose=True)"],"metadata":{"id":"-7tB7EcPgSgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(loss_history, color='c')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"metadata":{"id":"Akvmdz5sg29a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training accuracy: \", perceptron.calc_accuracy(X_train, y_train))\n","print(\"Testing accuracy: \", perceptron.calc_accuracy(X_test, y_test))"],"metadata":{"id":"pcfO6K4Eg6Ph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter optimization\n","\n","\n","Now that your model achieves reasonable accuracy (around 50% → 75%), it’s time to tune your hyperparameters to get the best possible performance.\n","\n","Complete the `tune_parameters` function inside the `linear_models.py` file.\n","\n","TODO:\n","\n","1. Explore different hyperparameter combinations — specifically:\n","\n","- Learning rate (e.g., [1e-3, 1e-2, 1e-1])\n","\n","- Batch size (e.g., [32, 64, 128])\n","\n","2. Train a new model for each combination using the training set.\n","\n","3. Evaluate each model on the validation set.\n","\n","4. Record your results in a dictionary of the form:\n","\n","`results = {\n","    (learning_rate, batch_size): (train_acc, val_acc),\n","    ...\n","}`\n","\n","\n","5. After finishing your search, select the best-performing configuration (based on validation accuracy).\n","\n","6. Finally, retrain your model with this configuration and evaluate it on the test set.\n","\n","\\\n","Tip: Start with a small number of iterations to debug your code quickly. Once everything works, increase num_iters to get stable results.\n","\n","\\\n","Goal:\n","Identify the combination of learning rate and batch size that leads to the highest validation accuracy, then report its training, validation, and test performance.\n","\n","**(10 points)**\n"],"metadata":{"id":"-ubDQOZphNTy"}},{"cell_type":"code","source":["import linear_models\n","\n","learning_rates = [1e-3, 1e-2, 1e-1]\n","batch_sizes    = [32, 64, 128, 256]\n","\n","results, best_perc, best_val = linear_models.tune_perceptron(\n","    linear_models.LinearPerceptron,\n","    X_train, y_train, X_val, y_val,\n","    learning_rates, batch_sizes,\n","    num_iters=500,\n","    model_kwargs=None,\n","    verbose=True,  # notebook printing here, not inside autogradable code\n",")\n","\n","# Print / log results (outside the library function)\n","for (lr, bs), (tr, va) in sorted(results.items()):\n","    print(f\"lr={lr: .1e} bs={bs:4d}  train_acc={tr:.4f}  val_acc={va:.4f}\")\n","\n","print(f\"best validation accuracy: {best_val:.4f}\")\n","test_acc = best_perc.calc_accuracy(X_test, y_test)\n","print(f\"final test accuracy: {test_acc:.4f}\")\n"],"metadata":{"id":"GXeRzRIAkMF3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistic regression\n","\n","Logistic regression can be generalized from binary classification to **multiclass** by applying the **softmax** function to the linear scores:\n","\n","$$\n","\\text{scores}(x) = XW \\quad,\\quad\n","p(y=j \\mid x) = \\frac{\\exp(\\text{scores}_j)}{\\sum_k \\exp(\\text{scores}_k)}\n","$$\n","\n","\n","- `W` : weight matrix (one column per class).\n","- The model outputs a **probability distribution** over classes.\n","- We train by minimizing **categorical cross-entropy** (a.k.a. softmax loss).\n","\n","**Numerical stability:** Always subtract the row-wise max before exponentiating (log-sum-exp trick).\n","\n","### TODO:\n","1. Implement a **numerically stable** `softmax(x)` **(5 points)**\n","2. Complete `LogisticRegression.predict(X)` and `LogisticRegression.init` **(5 points)**\n"],"metadata":{"id":"G-qqptIEmEy5"}},{"cell_type":"code","source":["logistic = linear_models.LogisticRegression(X_train, y_train)\n","y_pred = logistic.predict(X_test)"],"metadata":{"id":"oUBsI94VmGbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# Visualization exmaple: Compare Predicted vs True Labels on Test Set\n","# ---------------------------------------------------------------------\n","num_show = 8\n","idxs = np.random.choice(len(X_test), size=num_show, replace=False)\n","\n","fig, axes = plt.subplots(1, num_show, figsize=(16, 3))\n","\n","# mean -> (32,32,3); safely drop any bias/extra tail\n","mean_flat = np.asarray(mean_image).reshape(-1)\n","mean_img_3d = mean_flat[:3072].reshape(32, 32, 3)\n","\n","for ax, i in zip(axes, idxs):\n","    # take ONLY the first 3072 features (ignore any duplicate bias etc.)\n","    vec = np.asarray(X_test[i]).reshape(-1)[:3072]\n","    img = vec.reshape(32, 32, 3) + mean_img_3d\n","\n","    vmin, vmax = img.min(), img.max()\n","    if vmax > vmin:  # avoid divide-by-zero\n","        img_disp = (img - vmin) / (vmax - vmin)\n","    else:\n","        img_disp = np.zeros_like(img)\n","\n","    true_lbl = classes[y_test[i]]\n","    pred_lbl = classes[y_pred[i]]\n","\n","    ax.imshow(img_disp)\n","    ax.axis(\"off\")\n","    ax.set_title(\n","        f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\",\n","        color=(\"green\" if y_pred[i] == y_test[i] else \"red\")\n","    )\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QxJ5QgnrneSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"model accuracy: \", logistic.calc_accuracy(X_train, y_train))"],"metadata":{"id":"NO8pWkJEnet4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multiclass Cross-Entropy Loss (Softmax)\n","\n","In this section, you’ll implement the **softmax cross-entropy loss**, also known as the **multiclass logistic loss**.\n","\n","This loss generalizes binary cross-entropy to multiple classes.  \n","Given scores $s = XW$ for each class, we first normalize them into probabilities using **softmax**:\n","\n","\n","$$ p(y=j|x_i) = \\frac{e^{s_j}}{\\sum_{k} e^{s_k}} $$\n","\n","The loss for a single example is the **negative log-likelihood** of the correct class:\n","\n","\n","$$ L_i = -\\log(p(y_i|x_i))$$\n","\n","The overall loss is the average across all examples.\n","\n","\n","TODO:\n","\n","Complete the function **`softmax_cross_entropy_vectorized`** using vectorized code.  \n","\n","\n","\n","**(10 points)**"],"metadata":{"id":"NDj21XStnh68"}},{"cell_type":"code","source":["W = np.random.randn(3073, 3) * 1e-4\n"],"metadata":{"id":"LSbCO5_Vn17G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","loss_val, grad_val = linear_models.softmax_cross_entropy_vectorized(W, X_val, y_val)\n","print(f\"loss: {loss_val:.6f}\")\n","print(\"grad shape:\", grad_val.shape)"],"metadata":{"id":"Ov0CcY2gniPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grad_check(f, x, analytic_grad, num_checks=10, h=1e-5):\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h # increment by h\n","        fxph = f(x) # evaluate f(x + h)\n","        x[ix] = oldval - h # increment by h\n","        fxmh = f(x) # evaluate f(x - h)\n","        x[ix] = oldval # reset\n","\n","        grad_numerical = (fxph - fxmh) / (2 * h)\n","        grad_analytic = analytic_grad[ix]\n","        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n","        print ('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))"],"metadata":{"id":"XZRlR5WVn5bq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, grad = linear_models.softmax_cross_entropy(W, X_val, y_val)\n","f = lambda w: linear_models.softmax_cross_entropy_vectorized(w, X_val, y_val)[0]\n","grad_numerical = grad_check(f, W, grad)"],"metadata":{"id":"30OckmLDojAq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","logistic = linear_models.LogisticRegression(X_train, y_train)\n","loss_history = logistic.train(X_train, y_train,\n","                         learning_rate=1e-7,\n","                         num_iters=1500,\n","                         verbose=True)"],"metadata":{"id":"zmuWMGXIoqRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(loss_history, color = 'pink')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"metadata":{"id":"B6We7UcJovdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training accuracy: \", logistic.calc_accuracy(X_train, y_train))\n","print(\"Testing accuracy: \", logistic.calc_accuracy(X_test, y_test))"],"metadata":{"id":"m1OyJl2eoxkz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter optimization\n","\n","\n","Now that your model achieves reasonable accuracy (around 50% → 75%), it’s time to tune your hyperparameters to get the best possible performance.\n","\n","Your task is to:\n","\n","1. Explore different hyperparameter combinations — specifically:\n","\n","- Learning rate (e.g., [1e-3, 1e-2, 1e-1])\n","\n","- Batch size (e.g., [32, 64, 128])\n","\n","2. Train a new model for each combination using the training set.\n","\n","3. Evaluate each model on the validation set.\n","\n","4. Record your results in a dictionary of the form:\n","\n","`results = {\n","    (learning_rate, batch_size): (train_acc, val_acc),\n","    ...\n","}`\n","\n","\n","5. After finishing your search, select the best-performing configuration (based on validation accuracy).\n","\n","6. Finally, retrain your model with this configuration and evaluate it on the test set.\n","\n","\\\n","Tip: Start with a small number of iterations to debug your code quickly. Once everything works, increase num_iters to get stable results.\n","\n","\\\n","Goal:\n","Identify the combination of learning rate and batch size that leads to the highest validation accuracy, then report its training, validation, and test performance.\n","\n"," **(10 points)**"],"metadata":{"id":"HlmHbRgPlpvJ"}},{"cell_type":"code","source":["# Perform Hyper-parameter tuning\n","learning_rates = [1e-3, 1e-2, 1e-1]\n","batch_sizes    = [32, 64, 128, 256]\n","\n","results, best_perc, best_val = linear_models.tune_perceptron(\n","    linear_models.LogisticRegression,\n","    X_train, y_train, X_val, y_val,\n","    learning_rates, batch_sizes,\n","    num_iters=500,\n","    model_kwargs=None,\n","    verbose=True,  # notebook printing here, not inside autogradable code\n",")\n","\n","# Print / log results (outside the library function)\n","for (lr, bs), (tr, va) in sorted(results.items()):\n","    print(f\"lr={lr: .1e} bs={bs:4d}  train_acc={tr:.4f}  val_acc={va:.4f}\")\n","\n","print(f\"best validation accuracy: {best_val:.4f}\")\n","test_acc = best_perc.calc_accuracy(X_test, y_test)\n","print(f\"final test accuracy: {test_acc:.4f}\")\n"],"metadata":{"id":"aMOQ659ho9-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The End!\n"],"metadata":{"id":"e3_KyhhLpU7w"}}]}