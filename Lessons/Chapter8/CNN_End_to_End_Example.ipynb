{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2dc358",
   "metadata": {},
   "source": [
    "# CNN End-to-End Example: Theoretical Walkthrough\n",
    "\n",
    "This notebook provides a comprehensive theoretical analysis of a Convolutional Neural Network, examining both the forward and backward passes with detailed calculations of:\n",
    "- Output dimensions\n",
    "- Memory requirements (32-bit floats)\n",
    "- Parameter counts\n",
    "- Computational cost (FLOPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fc014",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Architecture Overview](#architecture-overview)\n",
    "2. [Notation and Conventions](#notation-and-conventions)\n",
    "3. [Forward Pass](#forward-pass)\n",
    "   - [Input Layer](#input-layer)\n",
    "   - [Block 1: Conv1 → BatchNorm1 → ReLU1 → MaxPool1](#block-1)\n",
    "   - [Block 2: Conv2 → BatchNorm2 → ReLU2 → MaxPool2](#block-2)\n",
    "   - [Block 3: Conv3 → BatchNorm3 → ReLU3 → MaxPool3](#block-3)\n",
    "   - [Global Average Pooling](#global-average-pooling)\n",
    "   - [Fully Connected Output Layer](#fully-connected-output-layer)\n",
    "4. [Backward Pass](#backward-pass)\n",
    "   - [Fully Connected Layer Gradients](#fc-gradients)\n",
    "   - [Global Average Pooling Gradients](#gap-gradients)\n",
    "   - [Block 3 Gradients](#block-3-gradients)\n",
    "   - [Block 2 Gradients](#block-2-gradients)\n",
    "   - [Block 1 Gradients](#block-1-gradients)\n",
    "5. [Summary Tables](#summary-tables)\n",
    "   - [Forward Pass Summary](#forward-summary)\n",
    "   - [Backward Pass Summary](#backward-summary)\n",
    "   - [Total Network Statistics](#total-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1ef07",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/E2E1.png\" width=\"710\"/>\n",
    "<img src=\"../images/chap8/E2E2.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "We'll analyze a CNN designed for image classification with the following architecture:\n",
    "\n",
    "```\n",
    "Input (32×32×3 RGB images)\n",
    "    ↓\n",
    "Block 1: Conv(3→32, k=3, s=1, p=1) → BatchNorm → ReLU → MaxPool(k=2, s=2)\n",
    "    ↓\n",
    "Block 2: Conv(32→64, k=3, s=1, p=1) → BatchNorm → ReLU → MaxPool(k=2, s=2)\n",
    "    ↓\n",
    "Block 3: Conv(64→128, k=3, s=1, p=1) → BatchNorm → ReLU → MaxPool(k=2, s=2)\n",
    "    ↓\n",
    "Global Average Pooling\n",
    "    ↓\n",
    "Fully Connected (128 → 10 classes)\n",
    "```\n",
    "\n",
    "**Task**: Classify images into 10 categories\n",
    "\n",
    "**Batch Size**: $\\textcolor{magenta}{N = 32}$ for all calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e2928",
   "metadata": {},
   "source": [
    "## Notation and Conventions\n",
    "\n",
    "Throughout this notebook, we use the following color-coded notation:\n",
    "\n",
    "| Symbol | Meaning | Color |\n",
    "|--------|---------|-------|\n",
    "| $\\textcolor{magenta}{N}$ | Batch size | Magenta |\n",
    "| $\\textcolor{blue}{C_{in}}$ | Input channels | Blue |\n",
    "| $\\textcolor{orange}{H_{in}, W_{in}}$ | Input height and width | Orange |\n",
    "| $\\textcolor{green}{C_{out}}$ | Output channels | Green |\n",
    "| $\\textcolor{red}{H_{out}, W_{out}}$ | Output height and width | Red |\n",
    "| $\\textcolor{purple}{k, k_h, k_w}$ | Kernel size | Purple |\n",
    "| $s$ | Stride | Black |\n",
    "| $p$ | Padding | Black |\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Output Spatial Dimensions**:\n",
    "$$H_{out} = \\left\\lfloor \\frac{H_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "$$W_{out} = \\left\\lfloor \\frac{W_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "**Memory (32-bit floats)**: Number of elements × 4 bytes\n",
    "\n",
    "**Convolution FLOPs**: $N \\times H_{out} \\times W_{out} \\times C_{out} \\times (C_{in} \\times k_h \\times k_w \\times 2)$\n",
    "- The factor of 2 accounts for multiply-accumulate operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2abae3",
   "metadata": {},
   "source": [
    "---\n",
    "# Forward Pass\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510f812",
   "metadata": {},
   "source": [
    "## Input Layer\n",
    "\n",
    "### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{N}, \\textcolor{blue}{C_{in}}, \\textcolor{orange}{H_{in}}, \\textcolor{orange}{W_{in}}) = (32, 3, 32, 32)$\n",
    "- **Description**: RGB images of size 32×32 pixels\n",
    "\n",
    "### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{blue}{3}, \\textcolor{orange}{32}, \\textcolor{orange}{32})$\n",
    "- **Memory**: $32 \\times 3 \\times 32 \\times 32 = 98,304$ elements = **393,216 bytes** (384 KB)\n",
    "- **Parameters**: 0\n",
    "- **FLOPs**: 0\n",
    "\n",
    "### Analysis\n",
    "The input layer simply receives the data. No transformations occur here, so there are no parameters to learn or computations to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ed554",
   "metadata": {},
   "source": [
    "## Block 1: Conv1 → BatchNorm1 → ReLU1 → MaxPool1\n",
    "\n",
    "### Conv1: Convolutional Layer\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{blue}{3}, \\textcolor{orange}{32}, \\textcolor{orange}{32})$\n",
    "- **Filters**: $\\textcolor{green}{C_{out}} = 32$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k_h = k_w = 3}$\n",
    "- **Stride**: $s = 1$\n",
    "- **Padding**: $p = 1$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{32 + 2(1) - 3}{1} \\right\\rfloor + 1 = \\left\\lfloor \\frac{31}{1} \\right\\rfloor + 1 = 32$$\n",
    "\n",
    "$$\\textcolor{red}{W_{out}} = \\left\\lfloor \\frac{32 + 2(1) - 3}{1} \\right\\rfloor + 1 = 32$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$\n",
    "\n",
    "#### Memory (Forward)\n",
    "- **Activations**: $32 \\times 32 \\times 32 \\times 32 = 1,048,576$ elements = **4,194,304 bytes** (4 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Weights**: $\\textcolor{green}{C_{out}} \\times \\textcolor{blue}{C_{in}} \\times \\textcolor{purple}{k_h} \\times \\textcolor{purple}{k_w} = 32 \\times 3 \\times 3 \\times 3 = 864$\n",
    "- **Bias**: $\\textcolor{green}{C_{out}} = 32$\n",
    "- **Total**: $864 + 32 = 896$ parameters = **3,584 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "For each output position, we perform:\n",
    "- $C_{in} \\times k_h \\times k_w = 3 \\times 3 \\times 3 = 27$ multiplications\n",
    "- $27$ additions (accumulation)\n",
    "- $1$ bias addition\n",
    "\n",
    "Total multiply-adds per output element: $2 \\times 27 = 54$\n",
    "\n",
    "$$\\text{FLOPs} = N \\times H_{out} \\times W_{out} \\times C_{out} \\times (2 \\times C_{in} \\times k_h \\times k_w)$$\n",
    "$$= 32 \\times 32 \\times 32 \\times 32 \\times 54 = 56,623,104 \\approx \\textbf{56.6 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### BatchNorm1: Batch Normalization\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$\n",
    "- **Number of channels**: $\\textcolor{green}{C = 32}$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$ (unchanged)\n",
    "- **Memory**: $32 \\times 32 \\times 32 \\times 32 = 1,048,576$ elements = **4,194,304 bytes** (4 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Scale ($\\gamma$)**: $C = 32$\n",
    "- **Shift ($\\beta$)**: $C = 32$\n",
    "- **Running mean** (not learned, updated during training): $C = 32$\n",
    "- **Running variance** (not learned, updated during training): $C = 32$\n",
    "- **Learnable Total**: $32 + 32 = 64$ parameters = **256 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "For each channel, we compute:\n",
    "1. Mean: $N \\times H \\times W$ additions\n",
    "2. Variance: $N \\times H \\times W$ subtractions and squaring\n",
    "3. Normalization: $N \\times H \\times W$ operations (subtract mean, divide by std)\n",
    "4. Scale and shift: $N \\times H \\times W \\times 2$ operations\n",
    "\n",
    "Approximate FLOPs per channel: $5 \\times N \\times H \\times W = 5 \\times 32 \\times 32 \\times 32 = 163,840$\n",
    "\n",
    "$$\\text{FLOPs} = C \\times 163,840 = 32 \\times 163,840 = 5,242,880 \\approx \\textbf{5.2 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU1: Activation Function\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$\n",
    "- **Function**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$ (unchanged)\n",
    "- **Memory**: $32 \\times 32 \\times 32 \\times 32 = 1,048,576$ elements = **4,194,304 bytes** (4 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0 (ReLU has no learnable parameters)\n",
    "\n",
    "#### FLOPs\n",
    "One comparison per element:\n",
    "$$\\text{FLOPs} = N \\times C \\times H \\times W = 32 \\times 32 \\times 32 \\times 32 = 1,048,576 \\approx \\textbf{1.0 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### MaxPool1: Max Pooling\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{32}, \\textcolor{red}{32})$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k = 2}$\n",
    "- **Stride**: $s = 2$\n",
    "- **Padding**: $p = 0$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{32 + 2(0) - 2}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{30}{2} \\right\\rfloor + 1 = 16$$\n",
    "\n",
    "$$\\textcolor{red}{W_{out}} = \\left\\lfloor \\frac{32 + 2(0) - 2}{2} \\right\\rfloor + 1 = 16$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{32}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "\n",
    "#### Memory\n",
    "- **Activations**: $32 \\times 32 \\times 16 \\times 16 = 262,144$ elements = **1,048,576 bytes** (1 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0 (pooling has no learnable parameters)\n",
    "\n",
    "#### FLOPs\n",
    "For each output position, we compare $k \\times k = 4$ values:\n",
    "$$\\text{FLOPs} = N \\times C \\times H_{out} \\times W_{out} \\times k^2 = 32 \\times 32 \\times 16 \\times 16 \\times 4 = 1,048,576 \\approx \\textbf{1.0 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Block 1 Summary\n",
    "\n",
    "| Layer | Output Shape | Memory | Parameters | FLOPs |\n",
    "|-------|--------------|--------|------------|-------|\n",
    "| Conv1 | (32, 32, 32, 32) | 4.0 MB | 896 | 56.6 M |\n",
    "| BatchNorm1 | (32, 32, 32, 32) | 4.0 MB | 64 | 5.2 M |\n",
    "| ReLU1 | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| MaxPool1 | (32, 32, 16, 16) | 1.0 MB | 0 | 1.0 M |\n",
    "| **Block Total** | — | **13.0 MB** | **960** | **63.8 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69887ea",
   "metadata": {},
   "source": [
    "## Block 2: Conv2 → BatchNorm2 → ReLU2 → MaxPool2\n",
    "\n",
    "### Conv2: Convolutional Layer\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{blue}{32}, \\textcolor{orange}{16}, \\textcolor{orange}{16})$\n",
    "- **Filters**: $\\textcolor{green}{C_{out}} = 64$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k_h = k_w = 3}$\n",
    "- **Stride**: $s = 1$\n",
    "- **Padding**: $p = 1$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{16 + 2(1) - 3}{1} \\right\\rfloor + 1 = \\left\\lfloor \\frac{15}{1} \\right\\rfloor + 1 = 16$$\n",
    "\n",
    "$$\\textcolor{red}{W_{out}} = 16$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "\n",
    "#### Memory\n",
    "- **Activations**: $32 \\times 64 \\times 16 \\times 16 = 524,288$ elements = **2,097,152 bytes** (2 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Weights**: $64 \\times 32 \\times 3 \\times 3 = 18,432$\n",
    "- **Bias**: $64$\n",
    "- **Total**: $18,432 + 64 = 18,496$ parameters = **73,984 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 16 \\times 16 \\times 64 \\times (2 \\times 32 \\times 3 \\times 3)$$\n",
    "$$= 32 \\times 16 \\times 16 \\times 64 \\times 576 = 301,989,888 \\approx \\textbf{302.0 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### BatchNorm2: Batch Normalization\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "- **Number of channels**: $\\textcolor{green}{C = 64}$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "- **Memory**: $32 \\times 64 \\times 16 \\times 16 = 524,288$ elements = **2,097,152 bytes** (2 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Learnable**: $2 \\times C = 2 \\times 64 = 128$ parameters = **512 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 64 \\times (5 \\times 32 \\times 16 \\times 16) = 64 \\times 40,960 = 2,621,440 \\approx \\textbf{2.6 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU2: Activation Function\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "- **Memory**: **2,097,152 bytes** (2 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 64 \\times 16 \\times 16 = 524,288 \\approx \\textbf{0.5 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### MaxPool2: Max Pooling\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{16}, \\textcolor{red}{16})$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k = 2}$\n",
    "- **Stride**: $s = 2$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{16 - 2}{2} \\right\\rfloor + 1 = 8$$\n",
    "$$\\textcolor{red}{W_{out}} = 8$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{64}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "\n",
    "#### Memory\n",
    "- **Activations**: $32 \\times 64 \\times 8 \\times 8 = 131,072$ elements = **524,288 bytes** (0.5 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 64 \\times 8 \\times 8 \\times 4 = 524,288 \\approx \\textbf{0.5 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Block 2 Summary\n",
    "\n",
    "| Layer | Output Shape | Memory | Parameters | FLOPs |\n",
    "|-------|--------------|--------|------------|-------|\n",
    "| Conv2 | (32, 64, 16, 16) | 2.0 MB | 18,496 | 302.0 M |\n",
    "| BatchNorm2 | (32, 64, 16, 16) | 2.0 MB | 128 | 2.6 M |\n",
    "| ReLU2 | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| MaxPool2 | (32, 64, 8, 8) | 0.5 MB | 0 | 0.5 M |\n",
    "| **Block Total** | — | **6.5 MB** | **18,624** | **305.6 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19841e1e",
   "metadata": {},
   "source": [
    "## Block 3: Conv3 → BatchNorm3 → ReLU3 → MaxPool3\n",
    "\n",
    "### Conv3: Convolutional Layer\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{blue}{64}, \\textcolor{orange}{8}, \\textcolor{orange}{8})$\n",
    "- **Filters**: $\\textcolor{green}{C_{out}} = 128$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k_h = k_w = 3}$\n",
    "- **Stride**: $s = 1$\n",
    "- **Padding**: $p = 1$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{8 + 2(1) - 3}{1} \\right\\rfloor + 1 = 8$$\n",
    "$$\\textcolor{red}{W_{out}} = 8$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "\n",
    "#### Memory\n",
    "- **Activations**: $32 \\times 128 \\times 8 \\times 8 = 262,144$ elements = **1,048,576 bytes** (1 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Weights**: $128 \\times 64 \\times 3 \\times 3 = 73,728$\n",
    "- **Bias**: $128$\n",
    "- **Total**: $73,728 + 128 = 73,856$ parameters = **295,424 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 8 \\times 8 \\times 128 \\times (2 \\times 64 \\times 3 \\times 3)$$\n",
    "$$= 32 \\times 8 \\times 8 \\times 128 \\times 1,152 = 301,989,888 \\approx \\textbf{302.0 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### BatchNorm3: Batch Normalization\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "- **Number of channels**: $\\textcolor{green}{C = 128}$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "- **Memory**: **1,048,576 bytes** (1 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Learnable**: $2 \\times 128 = 256$ parameters = **1,024 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 128 \\times (5 \\times 32 \\times 8 \\times 8) = 128 \\times 10,240 = 1,310,720 \\approx \\textbf{1.3 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU3: Activation Function\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "\n",
    "#### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "- **Memory**: **1,048,576 bytes** (1 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 128 \\times 8 \\times 8 = 262,144 \\approx \\textbf{0.3 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### MaxPool3: Max Pooling\n",
    "\n",
    "#### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{8}, \\textcolor{red}{8})$\n",
    "- **Kernel Size**: $\\textcolor{purple}{k = 2}$\n",
    "- **Stride**: $s = 2$\n",
    "\n",
    "#### Output Dimensions\n",
    "$$\\textcolor{red}{H_{out}} = \\left\\lfloor \\frac{8 - 2}{2} \\right\\rfloor + 1 = 4$$\n",
    "$$\\textcolor{red}{W_{out}} = 4$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{4}, \\textcolor{red}{4})$\n",
    "\n",
    "#### Memory\n",
    "- **Activations**: $32 \\times 128 \\times 4 \\times 4 = 65,536$ elements = **262,144 bytes** (0.25 MB)\n",
    "\n",
    "#### Parameters\n",
    "- **Total**: 0\n",
    "\n",
    "#### FLOPs\n",
    "$$\\text{FLOPs} = 32 \\times 128 \\times 4 \\times 4 \\times 4 = 262,144 \\approx \\textbf{0.3 MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Block 3 Summary\n",
    "\n",
    "| Layer | Output Shape | Memory | Parameters | FLOPs |\n",
    "|-------|--------------|--------|------------|-------|\n",
    "| Conv3 | (32, 128, 8, 8) | 1.0 MB | 73,856 | 302.0 M |\n",
    "| BatchNorm3 | (32, 128, 8, 8) | 1.0 MB | 256 | 1.3 M |\n",
    "| ReLU3 | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| MaxPool3 | (32, 128, 4, 4) | 0.25 MB | 0 | 0.3 M |\n",
    "| **Block Total** | — | **3.25 MB** | **74,112** | **303.9 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a18bc3",
   "metadata": {},
   "source": [
    "## Global Average Pooling\n",
    "\n",
    "### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128}, \\textcolor{red}{4}, \\textcolor{red}{4})$\n",
    "- **Operation**: Average each channel across spatial dimensions\n",
    "\n",
    "### Output\n",
    "For each channel, compute:\n",
    "$$\\text{GAP}_c = \\frac{1}{H \\times W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{c,h,w}$$\n",
    "\n",
    "$$\\text{GAP}_c = \\frac{1}{4 \\times 4} \\sum_{h=1}^{4} \\sum_{w=1}^{4} x_{c,h,w} = \\frac{1}{16} \\sum_{h=1}^{4} \\sum_{w=1}^{4} x_{c,h,w}$$\n",
    "\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{128})$\n",
    "\n",
    "### Memory\n",
    "- **Activations**: $32 \\times 128 = 4,096$ elements = **16,384 bytes** (16 KB)\n",
    "\n",
    "### Parameters\n",
    "- **Total**: 0 (no learnable parameters)\n",
    "\n",
    "### FLOPs\n",
    "For each of 128 channels in each of 32 batches:\n",
    "- Add $4 \\times 4 = 16$ values: 15 additions\n",
    "- Divide by 16: 1 division\n",
    "\n",
    "$$\\text{FLOPs} = N \\times C \\times (H \\times W) = 32 \\times 128 \\times 16 = 65,536 \\approx \\textbf{0.07 MFLOPs}$$\n",
    "\n",
    "### Analysis\n",
    "Global Average Pooling (GAP) reduces spatial dimensions to 1×1 per channel, effectively converting spatial feature maps into a feature vector. This:\n",
    "1. Eliminates the need for large fully connected layers\n",
    "2. Reduces overfitting by having no parameters\n",
    "3. Makes the network more robust to spatial translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59cbc2",
   "metadata": {},
   "source": [
    "## Fully Connected Output Layer\n",
    "\n",
    "### Given\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, \\textcolor{blue}{128})$\n",
    "- **Output Classes**: $\\textcolor{green}{10}$\n",
    "- **Operation**: Linear transformation $y = xW + b$\n",
    "\n",
    "### Output\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, \\textcolor{green}{10})$\n",
    "- **Interpretation**: Logits (raw scores) for 10 classes\n",
    "\n",
    "### Memory\n",
    "- **Activations**: $32 \\times 10 = 320$ elements = **1,280 bytes** (1.25 KB)\n",
    "\n",
    "### Parameters\n",
    "- **Weights**: $\\textcolor{blue}{128} \\times \\textcolor{green}{10} = 1,280$\n",
    "- **Bias**: $\\textcolor{green}{10}$\n",
    "- **Total**: $1,280 + 10 = 1,290$ parameters = **5,160 bytes**\n",
    "\n",
    "### FLOPs\n",
    "For each output:\n",
    "- 128 multiplications and 128 additions (dot product)\n",
    "- 1 bias addition\n",
    "\n",
    "$$\\text{FLOPs} = N \\times \\text{output\\_dim} \\times (2 \\times \\text{input\\_dim}) = 32 \\times 10 \\times (2 \\times 128)$$\n",
    "$$= 32 \\times 10 \\times 256 = 81,920 \\approx \\textbf{0.08 MFLOPs}$$\n",
    "\n",
    "### Final Output\n",
    "The output logits are typically passed through a Softmax function during inference:\n",
    "$$\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{10} e^{z_j}}$$\n",
    "\n",
    "This converts logits to class probabilities. During training, we use Cross-Entropy Loss:\n",
    "$$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\text{Softmax}(z_i)_{y_i})$$\n",
    "\n",
    "where $y_i$ is the true class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9e241",
   "metadata": {},
   "source": [
    "---\n",
    "# Backward Pass\n",
    "---\n",
    "\n",
    "During backpropagation, we compute gradients with respect to:\n",
    "1. **Parameters** (weights and biases) - used to update the model\n",
    "2. **Activations** (layer inputs) - used to propagate gradients to previous layers\n",
    "\n",
    "We work backwards from the loss function through each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3e7a1",
   "metadata": {},
   "source": [
    "## Fully Connected Layer Gradients\n",
    "\n",
    "### Given\n",
    "- **Forward**: $y = Wx + b$\n",
    "- **Input Shape**: $(\\textcolor{magenta}{32}, 128)$\n",
    "- **Output Shape**: $(\\textcolor{magenta}{32}, 10)$\n",
    "- **Gradient from Loss**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 10)$\n",
    "\n",
    "### Gradients to Compute\n",
    "\n",
    "#### 1. Gradient w.r.t. Weights ($\\frac{\\partial \\mathcal{L}}{\\partial W}$)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = x^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial y}$$\n",
    "\n",
    "- **Shape**: $(128, 10)$ - same as $W$\n",
    "- **Memory**: $128 \\times 10 = 1,280$ elements = **5,120 bytes**\n",
    "\n",
    "#### 2. Gradient w.r.t. Bias ($\\frac{\\partial \\mathcal{L}}{\\partial b}$)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial \\mathcal{L}}{\\partial y_i}$$\n",
    "\n",
    "Sum across batch dimension.\n",
    "\n",
    "- **Shape**: $(10,)$ - same as $b$\n",
    "- **Memory**: $10$ elements = **40 bytes**\n",
    "\n",
    "#### 3. Gradient w.r.t. Input ($\\frac{\\partial \\mathcal{L}}{\\partial x}$)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot W^T$$\n",
    "\n",
    "- **Shape**: $(32, 128)$ - same as input $x$\n",
    "- **Memory**: $32 \\times 128 = 4,096$ elements = **16,384 bytes**\n",
    "\n",
    "### FLOPs\n",
    "\n",
    "1. **Weight gradient**: Matrix multiplication $(128, 32) \\times (32, 10)$\n",
    "   - FLOPs: $128 \\times 10 \\times 2 \\times 32 = 81,920$\n",
    "\n",
    "2. **Bias gradient**: Sum over batch\n",
    "   - FLOPs: $10 \\times 32 = 320$\n",
    "\n",
    "3. **Input gradient**: Matrix multiplication $(32, 10) \\times (10, 128)$\n",
    "   - FLOPs: $32 \\times 128 \\times 2 \\times 10 = 81,920$\n",
    "\n",
    "**Total FLOPs**: $81,920 + 320 + 81,920 = 164,160 \\approx \\textbf{0.16 MFLOPs}$\n",
    "\n",
    "### Memory Summary (Backward)\n",
    "- **Gradient Storage**: $5,120 + 40 + 16,384 = 21,544$ bytes ≈ **21 KB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf878e2e",
   "metadata": {},
   "source": [
    "## Global Average Pooling Gradients\n",
    "\n",
    "### Given\n",
    "- **Forward**: $y_c = \\frac{1}{H \\times W} \\sum_{h,w} x_{c,h,w}$\n",
    "- **Input Shape**: $(32, 128, 4, 4)$\n",
    "- **Output Shape**: $(32, 128)$\n",
    "- **Gradient from next layer**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 128)$\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Since each spatial position contributes equally to the average:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x_{c,h,w}} = \\frac{1}{H \\times W} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial y_c}$$\n",
    "\n",
    "The gradient is **broadcasted** from $(32, 128)$ to $(32, 128, 4, 4)$ and scaled by $\\frac{1}{16}$.\n",
    "\n",
    "### Memory\n",
    "- **Gradient**: $32 \\times 128 \\times 4 \\times 4 = 65,536$ elements = **262,144 bytes** (0.25 MB)\n",
    "\n",
    "### Parameters\n",
    "- **Gradients**: 0 (no learnable parameters)\n",
    "\n",
    "### FLOPs\n",
    "Broadcasting and scaling:\n",
    "$$\\text{FLOPs} = N \\times C \\times H \\times W = 32 \\times 128 \\times 4 \\times 4 = 65,536 \\approx \\textbf{0.07 MFLOPs}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc00e8d",
   "metadata": {},
   "source": [
    "## Block 3 Gradients: MaxPool3 ← ReLU3 ← BatchNorm3 ← Conv3\n",
    "\n",
    "### MaxPool3 Gradients\n",
    "\n",
    "#### Given\n",
    "- **Forward Input**: $(32, 128, 8, 8)$\n",
    "- **Forward Output**: $(32, 128, 4, 4)$\n",
    "- **Gradient from GAP**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 128, 4, 4)$\n",
    "\n",
    "#### Gradient Computation\n",
    "Max pooling passes gradient only to the position where the maximum occurred:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x_{h,w}} = \\begin{cases} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{h',w'}} & \\text{if } x_{h,w} = \\max(\\text{pool region}) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "- **Gradient Shape**: $(32, 128, 8, 8)$\n",
    "- **Memory**: $32 \\times 128 \\times 8 \\times 8 = 262,144$ elements = **1,048,576 bytes** (1 MB)\n",
    "- **FLOPs**: $\\approx 0.3$ MFLOPs (routing gradients)\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU3 Gradients\n",
    "\n",
    "#### Given\n",
    "- **Forward**: $y = \\max(0, x)$\n",
    "- **Gradient from MaxPool**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 128, 8, 8)$\n",
    "\n",
    "#### Gradient Computation\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\mathbb{1}_{x > 0}$$\n",
    "\n",
    "where $\\mathbb{1}_{x > 0}$ is an indicator function (1 if $x > 0$, else 0).\n",
    "\n",
    "- **Gradient Shape**: $(32, 128, 8, 8)$\n",
    "- **Memory**: **1,048,576 bytes** (1 MB)\n",
    "- **FLOPs**: $262,144 \\approx \\textbf{0.3 MFLOPs}$ (element-wise multiplication)\n",
    "\n",
    "---\n",
    "\n",
    "### BatchNorm3 Gradients\n",
    "\n",
    "#### Given\n",
    "- **Forward**: $y = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n",
    "- **Input Shape**: $(32, 128, 8, 8)$\n",
    "- **Gradient from ReLU**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 128, 8, 8)$\n",
    "\n",
    "#### Gradient Computation\n",
    "\n",
    "BatchNorm backward pass is complex. For each channel:\n",
    "\n",
    "1. **Gradient w.r.t. $\\gamma$**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{n,h,w} \\frac{\\partial \\mathcal{L}}{\\partial y_{n,c,h,w}} \\cdot \\frac{x_{n,c,h,w} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}$$\n",
    "\n",
    "2. **Gradient w.r.t. $\\beta$**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{n,h,w} \\frac{\\partial \\mathcal{L}}{\\partial y_{n,c,h,w}}$$\n",
    "\n",
    "3. **Gradient w.r.t. input $x$**: Involves gradients of mean and variance (complex chain rule)\n",
    "\n",
    "#### Memory\n",
    "- **Input gradient**: $(32, 128, 8, 8)$ = **1,048,576 bytes** (1 MB)\n",
    "- **Parameter gradients**: $\\gamma$ and $\\beta$ each have 128 elements = **1,024 bytes**\n",
    "\n",
    "#### FLOPs\n",
    "Similar to forward pass (computing statistics and applying chain rule):\n",
    "$$\\text{FLOPs} \\approx 1.3 \\text{ MFLOPs}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Conv3 Gradients\n",
    "\n",
    "#### Given\n",
    "- **Forward**: Convolution with input $(32, 64, 8, 8)$, output $(32, 128, 8, 8)$\n",
    "- **Filters**: $128 \\times 64 \\times 3 \\times 3$\n",
    "- **Gradient from BatchNorm**: $\\frac{\\partial \\mathcal{L}}{\\partial y}$ with shape $(32, 128, 8, 8)$\n",
    "\n",
    "#### Gradient Computation\n",
    "\n",
    "1. **Gradient w.r.t. Weights** ($\\frac{\\partial \\mathcal{L}}{\\partial W}$):\n",
    "   - Convolve input with gradient\n",
    "   - Shape: $(128, 64, 3, 3)$\n",
    "   - Memory: $73,728$ elements = **294,912 bytes**\n",
    "\n",
    "2. **Gradient w.r.t. Bias** ($\\frac{\\partial \\mathcal{L}}{\\partial b}$):\n",
    "   - Sum gradient across batch and spatial dimensions\n",
    "   - Shape: $(128,)$\n",
    "   - Memory: $128$ elements = **512 bytes**\n",
    "\n",
    "3. **Gradient w.r.t. Input** ($\\frac{\\partial \\mathcal{L}}{\\partial x}$):\n",
    "   - Convolve gradient with transposed filters (\"full\" convolution)\n",
    "   - Shape: $(32, 64, 8, 8)$\n",
    "   - Memory: $131,072$ elements = **524,288 bytes** (0.5 MB)\n",
    "\n",
    "#### FLOPs\n",
    "Backward convolution requires roughly **2× forward FLOPs**:\n",
    "- Weight gradient convolution: $\\approx 302$ MFLOPs\n",
    "- Input gradient convolution: $\\approx 302$ MFLOPs\n",
    "\n",
    "**Total**: $\\approx \\textbf{604 MFLOPs}$\n",
    "\n",
    "---\n",
    "\n",
    "### Block 3 Backward Summary\n",
    "\n",
    "| Layer | Gradient Shape | Memory | Param Gradients | FLOPs |\n",
    "|-------|----------------|--------|-----------------|-------|\n",
    "| MaxPool3 | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| ReLU3 | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| BatchNorm3 | (32, 128, 8, 8) | 1.0 MB | 1,024 B | 1.3 M |\n",
    "| Conv3 | (32, 64, 8, 8) | 0.5 MB | 295,424 B | 604.0 M |\n",
    "| **Block Total** | — | **3.5 MB** | **296,448 B** | **605.9 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8dfe",
   "metadata": {},
   "source": [
    "## Block 2 Gradients: MaxPool2 ← ReLU2 ← BatchNorm2 ← Conv2\n",
    "\n",
    "Following the same pattern as Block 3, but with different dimensions:\n",
    "\n",
    "### MaxPool2 Gradients\n",
    "- **Gradient Shape**: $(32, 64, 16, 16)$\n",
    "- **Memory**: $32 \\times 64 \\times 16 \\times 16 = 524,288$ elements = **2,097,152 bytes** (2 MB)\n",
    "- **FLOPs**: $\\approx \\textbf{0.5 MFLOPs}$\n",
    "\n",
    "### ReLU2 Gradients\n",
    "- **Gradient Shape**: $(32, 64, 16, 16)$\n",
    "- **Memory**: **2,097,152 bytes** (2 MB)\n",
    "- **FLOPs**: $524,288 \\approx \\textbf{0.5 MFLOPs}$\n",
    "\n",
    "### BatchNorm2 Gradients\n",
    "- **Input Gradient Shape**: $(32, 64, 16, 16)$\n",
    "- **Memory**: **2,097,152 bytes** (2 MB)\n",
    "- **Parameter Gradients**: $\\gamma$ and $\\beta$ (64 each) = **512 bytes**\n",
    "- **FLOPs**: $\\approx \\textbf{2.6 MFLOPs}$\n",
    "\n",
    "### Conv2 Gradients\n",
    "- **Input Gradient Shape**: $(32, 32, 16, 16)$\n",
    "- **Memory**: $32 \\times 32 \\times 16 \\times 16 = 262,144$ elements = **1,048,576 bytes** (1 MB)\n",
    "- **Weight Gradient**: $(64, 32, 3, 3)$ = $18,432$ elements = **73,728 bytes**\n",
    "- **Bias Gradient**: $(64,)$ = **256 bytes**\n",
    "- **FLOPs**: $\\approx \\textbf{604 MFLOPs}$\n",
    "\n",
    "### Block 2 Backward Summary\n",
    "\n",
    "| Layer | Gradient Shape | Memory | Param Gradients | FLOPs |\n",
    "|-------|----------------|--------|-----------------|-------|\n",
    "| MaxPool2 | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| ReLU2 | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| BatchNorm2 | (32, 64, 16, 16) | 2.0 MB | 512 B | 2.6 M |\n",
    "| Conv2 | (32, 32, 16, 16) | 1.0 MB | 73,984 B | 604.0 M |\n",
    "| **Block Total** | — | **7.0 MB** | **74,496 B** | **607.6 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d266ea1",
   "metadata": {},
   "source": [
    "## Block 1 Gradients: MaxPool1 ← ReLU1 ← BatchNorm1 ← Conv1\n",
    "\n",
    "### MaxPool1 Gradients\n",
    "- **Gradient Shape**: $(32, 32, 32, 32)$\n",
    "- **Memory**: $32 \\times 32 \\times 32 \\times 32 = 1,048,576$ elements = **4,194,304 bytes** (4 MB)\n",
    "- **FLOPs**: $\\approx \\textbf{1.0 MFLOPs}$\n",
    "\n",
    "### ReLU1 Gradients\n",
    "- **Gradient Shape**: $(32, 32, 32, 32)$\n",
    "- **Memory**: **4,194,304 bytes** (4 MB)\n",
    "- **FLOPs**: $1,048,576 \\approx \\textbf{1.0 MFLOPs}$\n",
    "\n",
    "### BatchNorm1 Gradients\n",
    "- **Input Gradient Shape**: $(32, 32, 32, 32)$\n",
    "- **Memory**: **4,194,304 bytes** (4 MB)\n",
    "- **Parameter Gradients**: $\\gamma$ and $\\beta$ (32 each) = **256 bytes**\n",
    "- **FLOPs**: $\\approx \\textbf{5.2 MFLOPs}$\n",
    "\n",
    "### Conv1 Gradients\n",
    "- **Input Gradient Shape**: $(32, 3, 32, 32)$ (back to original input)\n",
    "- **Memory**: $32 \\times 3 \\times 32 \\times 32 = 98,304$ elements = **393,216 bytes** (384 KB)\n",
    "- **Weight Gradient**: $(32, 3, 3, 3)$ = $864$ elements = **3,456 bytes**\n",
    "- **Bias Gradient**: $(32,)$ = **128 bytes**\n",
    "- **FLOPs**: $\\approx \\textbf{113 MFLOPs}$\n",
    "\n",
    "### Block 1 Backward Summary\n",
    "\n",
    "| Layer | Gradient Shape | Memory | Param Gradients | FLOPs |\n",
    "|-------|----------------|--------|-----------------|-------|\n",
    "| MaxPool1 | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| ReLU1 | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| BatchNorm1 | (32, 32, 32, 32) | 4.0 MB | 256 B | 5.2 M |\n",
    "| Conv1 | (32, 3, 32, 32) | 384 KB | 3,584 B | 113.0 M |\n",
    "| **Block Total** | — | **12.4 MB** | **3,840 B** | **120.2 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ebb387",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary Tables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15555d",
   "metadata": {},
   "source": [
    "## Forward Pass Summary\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Memory | Parameters | FLOPs |\n",
    "|-------|-------------|--------------|--------|------------|-------|\n",
    "| **Input** | — | (32, 3, 32, 32) | 384 KB | 0 | 0 |\n",
    "| Conv1 | (32, 3, 32, 32) | (32, 32, 32, 32) | 4.0 MB | 896 | 56.6 M |\n",
    "| BatchNorm1 | (32, 32, 32, 32) | (32, 32, 32, 32) | 4.0 MB | 64 | 5.2 M |\n",
    "| ReLU1 | (32, 32, 32, 32) | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| MaxPool1 | (32, 32, 32, 32) | (32, 32, 16, 16) | 1.0 MB | 0 | 1.0 M |\n",
    "| Conv2 | (32, 32, 16, 16) | (32, 64, 16, 16) | 2.0 MB | 18,496 | 302.0 M |\n",
    "| BatchNorm2 | (32, 64, 16, 16) | (32, 64, 16, 16) | 2.0 MB | 128 | 2.6 M |\n",
    "| ReLU2 | (32, 64, 16, 16) | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| MaxPool2 | (32, 64, 16, 16) | (32, 64, 8, 8) | 0.5 MB | 0 | 0.5 M |\n",
    "| Conv3 | (32, 64, 8, 8) | (32, 128, 8, 8) | 1.0 MB | 73,856 | 302.0 M |\n",
    "| BatchNorm3 | (32, 128, 8, 8) | (32, 128, 8, 8) | 1.0 MB | 256 | 1.3 M |\n",
    "| ReLU3 | (32, 128, 8, 8) | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| MaxPool3 | (32, 128, 8, 8) | (32, 128, 4, 4) | 0.25 MB | 0 | 0.3 M |\n",
    "| GAP | (32, 128, 4, 4) | (32, 128) | 16 KB | 0 | 0.07 M |\n",
    "| FC | (32, 128) | (32, 10) | 1.25 KB | 1,290 | 0.08 M |\n",
    "| **TOTAL** | — | — | **22.2 MB** | **94,986** | **673.4 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff1122",
   "metadata": {},
   "source": [
    "## Backward Pass Summary\n",
    "\n",
    "| Layer | Gradient Shape | Memory | Param Grad Memory | FLOPs |\n",
    "|-------|----------------|--------|-------------------|-------|\n",
    "| FC | (32, 128) | 16 KB | 5,160 B | 0.16 M |\n",
    "| GAP | (32, 128, 4, 4) | 0.25 MB | 0 | 0.07 M |\n",
    "| MaxPool3 | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| ReLU3 | (32, 128, 8, 8) | 1.0 MB | 0 | 0.3 M |\n",
    "| BatchNorm3 | (32, 128, 8, 8) | 1.0 MB | 1,024 B | 1.3 M |\n",
    "| Conv3 | (32, 64, 8, 8) | 0.5 MB | 295,424 B | 604.0 M |\n",
    "| MaxPool2 | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| ReLU2 | (32, 64, 16, 16) | 2.0 MB | 0 | 0.5 M |\n",
    "| BatchNorm2 | (32, 64, 16, 16) | 2.0 MB | 512 B | 2.6 M |\n",
    "| Conv2 | (32, 32, 16, 16) | 1.0 MB | 73,984 B | 604.0 M |\n",
    "| MaxPool1 | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| ReLU1 | (32, 32, 32, 32) | 4.0 MB | 0 | 1.0 M |\n",
    "| BatchNorm1 | (32, 32, 32, 32) | 4.0 MB | 256 B | 5.2 M |\n",
    "| Conv1 | (32, 3, 32, 32) | 384 KB | 3,584 B | 113.0 M |\n",
    "| **TOTAL** | — | **23.1 MB** | **379,944 B** | **1,334.0 M** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e16736",
   "metadata": {},
   "source": [
    "## Total Network Statistics\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Component | Count | Memory (Bytes) |\n",
    "|-----------|-------|----------------|\n",
    "| Convolutional layers | 93,184 | 372,736 |\n",
    "| Batch Normalization | 448 | 1,792 |\n",
    "| Fully Connected | 1,290 | 5,160 |\n",
    "| **Total Parameters** | **94,986** | **379,944** |\n",
    "\n",
    "### Memory Requirements (per batch of 32 images)\n",
    "\n",
    "| Type | Forward | Backward | Total |\n",
    "|------|---------|----------|-------|\n",
    "| Activations | 22.2 MB | 23.1 MB | 45.3 MB |\n",
    "| Parameters | 0.36 MB | 0.36 MB | 0.72 MB |\n",
    "| Gradients | — | 0.36 MB | 0.36 MB |\n",
    "| **Total** | **22.6 MB** | **23.8 MB** | **46.4 MB** |\n",
    "\n",
    "### Computational Cost\n",
    "\n",
    "| Pass | FLOPs | Percentage |\n",
    "|------|-------|------------|\n",
    "| Forward | 673.4 MFLOPs | 33.5% |\n",
    "| Backward | 1,334.0 MFLOPs | 66.5% |\n",
    "| **Total per iteration** | **2,007.4 MFLOPs** | **100%** |\n",
    "\n",
    "**Note**: Backward pass requires approximately **2× the FLOPs** of the forward pass due to:\n",
    "1. Computing gradients for both parameters and inputs\n",
    "2. Additional operations for weight updates\n",
    "\n",
    "### Computational Breakdown by Layer Type\n",
    "\n",
    "| Layer Type | Forward FLOPs | % of Total Forward |\n",
    "|------------|---------------|--------------------|\n",
    "| Convolution | 660.6 M | 98.1% |\n",
    "| Batch Normalization | 9.1 M | 1.4% |\n",
    "| Activation (ReLU) | 1.8 M | 0.3% |\n",
    "| Pooling | 1.8 M | 0.3% |\n",
    "| Fully Connected | 0.08 M | 0.01% |\n",
    "\n",
    "**Key Insight**: Convolutional layers dominate computation (~98%), making them the primary target for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb21f1",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "### 1. **Parameter Efficiency**\n",
    "- Total parameters: **94,986** (~95K)\n",
    "- This is extremely lightweight compared to fully-connected architectures\n",
    "- For comparison, a single FC layer from 32×32×3 flattened input to 128 features would require 393,344 parameters alone\n",
    "\n",
    "### 2. **Memory Requirements**\n",
    "- Forward pass: **22.6 MB** per batch\n",
    "- Backward pass: **23.8 MB** per batch\n",
    "- Training requires storing both, plus optimizer states (e.g., momentum)\n",
    "- For Adam optimizer, multiply memory by ~3× (parameters + first moment + second moment)\n",
    "\n",
    "### 3. **Computational Cost**\n",
    "- Dominated by convolutional operations (98% of FLOPs)\n",
    "- Conv2 and Conv3 are the bottlenecks despite smaller spatial dimensions\n",
    "- This is because FLOPs scale with $C_{in} \\times C_{out}$, not just spatial size\n",
    "\n",
    "### 4. **Design Trade-offs**\n",
    "- **Early layers** (Block 1): Large spatial dimensions but few channels → moderate compute\n",
    "- **Middle layers** (Block 2, 3): Smaller spatial dimensions but more channels → high compute\n",
    "- **Global Average Pooling**: Eliminates need for large FC layers, saving millions of parameters\n",
    "\n",
    "### 5. **Optimization Opportunities**\n",
    "- **Depthwise separable convolutions**: Can reduce FLOPs by 8-9× with minimal accuracy loss\n",
    "- **Mixed precision training**: Use FP16 instead of FP32 → 2× memory reduction\n",
    "- **Gradient checkpointing**: Trade computation for memory by recomputing activations during backward pass\n",
    "\n",
    "### 6. **Scaling Considerations**\n",
    "- Doubling input resolution (32×32 → 64×64): **4× memory**, **4× FLOPs**\n",
    "- Doubling channels at each layer: **4× parameters**, **4× FLOPs**\n",
    "- Adding one more conv block: **+~300M FLOPs**, **+~74K parameters**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
