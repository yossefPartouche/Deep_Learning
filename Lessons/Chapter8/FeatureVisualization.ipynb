{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6044a19e",
   "metadata": {},
   "source": [
    "# Feature Visualisation \n",
    "\n",
    "In this chapter we uncover:\n",
    "- What patterns are individual neurons actually detecting\n",
    "- Where in the image the network is focusing to make a descision\n",
    "- How to understand what features the model is extracting\n",
    "\n",
    "We'll be following 2 main approaches:\n",
    "1. Feature-based explanantions $\\to$ DeconvNet\n",
    "2. Descision-Based Explanations $\\to$ CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b67a75",
   "metadata": {},
   "source": [
    "## Visualizing The Weights \n",
    "\n",
    "The most basic inspection approach is to directly visualize the learned weights, primarily applicable to the **first convolutional layer**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What We See in First Layer Weights**\n",
    "\n",
    "First layer filters operate on raw RGB pixels, making them interpretable as visual templates:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/InitialLayer.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "| **Pattern** | **What It Detects** |\n",
    "|-------------|---------------------|\n",
    "| **Edge detectors** | Oriented gradients (horizontal, vertical, diagonal) |\n",
    "| **Color blobs** | Specific color combinations (red, green, blue) |\n",
    "| **Gabor-like patterns** | Textures at various orientations and frequencies |\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pretrained model\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# Extract first conv layer weights: [out_channels, in_channels, height, width]\n",
    "first_layer_weights = model.features[0].weight.data  # Shape: [64, 3, 11, 11]\n",
    "\n",
    "# Visualize first 64 filters\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Normalize to [0, 1] for display\n",
    "    weight = first_layer_weights[i].permute(1, 2, 0)  # [11, 11, 3]\n",
    "    weight = (weight - weight.min()) / (weight.max() - weight.min())\n",
    "    ax.imshow(weight.cpu().numpy())\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "| **Problem** | **Why It Fails** |\n",
    "|-------------|------------------|\n",
    "| **Only Layer 1** | Deeper layers have abstract, high-dimensional features (e.g., 3×3×256 tensors) — uninterpretable blobs |\n",
    "| **No context** | Shows *what* filters detect, not *where* they activate on real images |\n",
    "| **Static view** | Ignores how features compose hierarchically through the network |\n",
    "\n",
    "**Better approach:** Visualize **activations** and **gradients** instead → DeconvNet, CAM.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410b6dc",
   "metadata": {},
   "source": [
    "## Maximally Activating Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e21ea",
   "metadata": {},
   "source": [
    "**Key Question:** \"What kind of input patterns cause a specific neuron to activate most strongly?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **The Approach**\n",
    "\n",
    "Since a neuron's **receptive field** determines what it can \"see\" (and this grows with network depth), we can find image patches that maximally excite specific neurons:\n",
    "\n",
    "**Algorithm:**\n",
    "1. Select a target neuron/feature map at any layer\n",
    "2. Run thousands of images through the network\n",
    "3. Record activation values for that neuron\n",
    "4. Extract and visualize the top-K image patches with highest activations\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/Layer1.png\" width=\"270\"/>\n",
    "<img src=\"../images/chap8/Layer2.png\" width=\"370\"/>\n",
    "<img src=\"../images/chap8/Layer4.png\" width=\"350\"/>\n",
    "<img src=\"../images/chap8/Layer6.png\" width=\"350\"/>\n",
    "<p><i>Example: Layer 1, 2, 4 and 6 neuron activates </i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Hook to capture activations\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook on target layer (e.g., conv3_3)\n",
    "model.features[16].register_forward_hook(get_activation('conv3_3'))\n",
    "\n",
    "# Process images\n",
    "patches = []\n",
    "for img_path in image_dataset:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    \n",
    "    # Forward pass\n",
    "    model(img_tensor)\n",
    "    \n",
    "    # Extract activation for specific filter (e.g., filter 42)\n",
    "    activation_map = activations['conv3_3'][0, 42]  # [H, W]\n",
    "    max_row, max_col = np.unravel_index(activation_map.argmax(), activation_map.shape)\n",
    "    \n",
    "    # Extract corresponding receptive field patch from original image\n",
    "    patch = extract_receptive_field_patch(img, max_row, max_col, layer_depth=16)\n",
    "    patches.append((activation_map.max().item(), patch))\n",
    "\n",
    "# Display top-9 patches\n",
    "top_patches = sorted(patches, reverse=True)[:9]\n",
    "visualize_patches(top_patches)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages & Limitations**\n",
    "\n",
    "| **✅ Advantages** | **❌ Limitations** |\n",
    "|-------------------|-------------------|\n",
    "| Works for **any layer** (not just Layer 1) | Doesn't explain **which class** the features belong to |\n",
    "| Data-driven and accurate | Doesn't explain the **final decision** |\n",
    "| No gradients needed (only forward passes) | Limited by **dataset coverage** (only sees patterns in training data) |\n",
    "| Produces interpretable patterns at every depth | Doesn't work well for **fully connected layers** (no spatial structure) |\n",
    "\n",
    "**Key Insight:** Shows *what* activates neurons, but not *why* the network makes specific predictions. For decision explanations, we need **CAM** (Class Activation Mapping).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70231fc",
   "metadata": {},
   "source": [
    "## Visualizing the Representation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe18f6",
   "metadata": {},
   "source": [
    "**Key Question** What does the network represent as a whole?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd8548",
   "metadata": {},
   "source": [
    "## Visualizing the Representation Space\n",
    "\n",
    "**Key Question:** \"What does the network represent as a whole? How does it organize different images internally?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **The Concept**\n",
    "\n",
    "**At each layer**, an image is transformed into a high-dimensional vector:\n",
    "- **Early layers**: Low-level features (edges, textures) → vectors encode spatial patterns\n",
    "- **Deep layers**: High-level features (objects, concepts) → vectors encode semantic meaning\n",
    "- **Key insight**: Similar images produce similar vectors in representation space\n",
    "\n",
    "**The Problem:** \n",
    "Fully connected layers produce high-dimensional vectors (e.g., 4096-dimensional) with **no spatial structure**—we can't use patch-based visualization anymore.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution 1: Nearest Neighbors in Feature Space**\n",
    "\n",
    "**Idea:** Extract feature vectors from a chosen layer, then find images with the most similar vectors.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Choose a layer (typically the last FC layer before classification)\n",
    "2. Extract feature vectors for all images: $\\mathbf{x}_i \\in \\mathbb{R}^d$ where $d$ = 4096\n",
    "3. For a query image, compute distances: $\\text{distance}(\\mathbf{x}_{\\text{query}}, \\mathbf{x}_i) = \\|\\mathbf{x}_{\\text{query}} - \\mathbf{x}_i\\|_2$\n",
    "4. Retrieve top-K nearest neighbors (smallest distances)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/NNRespSpace.png\" width=\"570\"/>\n",
    "<p><i>Test image (left of red line), L2 nearest neighbors in feature space (right)</i></p>\n",
    "</div>\n",
    "\n",
    "**What This Reveals:**\n",
    "- **Learned similarity**: What the network considers \"similar\" (may differ from human perception)\n",
    "- **Category structure**: How well classes are separated in feature space\n",
    "- **Invariances**: Network ignores pose, lighting, background variations\n",
    "\n",
    "**Use Cases:**\n",
    "- **Debugging**: Find mislabeled images (distant from their class cluster)\n",
    "- **Class confusion**: Identify which classes have overlapping representations\n",
    "- **Dataset bias**: Detect spurious correlations (e.g., \"boats\" always near water)\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation: Nearest Neighbors**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet50(pretrained=True).eval()\n",
    "\n",
    "# Remove final classification layer to get features\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Output: [N, 2048, 1, 1]\n",
    "\n",
    "# Extract features for all images\n",
    "features = []\n",
    "for img_path in dataset:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature = feature_extractor(img_tensor).flatten()  # [2048]\n",
    "        features.append(feature.cpu().numpy())\n",
    "\n",
    "features = np.array(features)  # [N, 2048]\n",
    "\n",
    "# Find nearest neighbors for a query image\n",
    "nn_model = NearestNeighbors(n_neighbors=10, metric='euclidean')\n",
    "nn_model.fit(features)\n",
    "\n",
    "query_feature = features[query_idx].reshape(1, -1)\n",
    "distances, indices = nn_model.kneighbors(query_feature)\n",
    "\n",
    "# Visualize results\n",
    "visualize_nearest_neighbors(query_idx, indices[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution 2: Low-Dimensional Embeddings**\n",
    "\n",
    "**The Problem:** Feature vectors are 2048-4096 dimensional—impossible to visualize directly.\n",
    "\n",
    "**The Solution:** Project high-dimensional vectors into 2D/3D while **preserving local distances**.\n",
    "\n",
    "| **Method** | **How It Works** | **Best For** |\n",
    "|------------|------------------|--------------|\n",
    "| **t-SNE** | Non-linear; preserves local neighborhoods via probability distributions | Exploring clusters and local structure |\n",
    "| **UMAP** | Non-linear; faster than t-SNE, preserves global structure better | Large datasets, hierarchical structure |\n",
    "| **PCA** | Linear projection onto principal components | Quick overview, preserving variance |\n",
    "\n",
    "**Key Principle:** Points close in high-dimensional space should remain close in 2D.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/2dembed.png\" width=\"600\"/>\n",
    "<p><i>t-SNE visualization of ImageNet features: each color = different class</i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation: t-SNE Visualization**\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract features (same as above)\n",
    "features = np.array(features)  # [N, 2048]\n",
    "labels = np.array(labels)       # [N] class labels\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)  # [N, 2]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                      c=labels, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of Feature Space')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Embeddings Reveal**\n",
    "\n",
    "| **Observation** | **Interpretation** |\n",
    "|-----------------|-------------------|\n",
    "| **Tight clusters** | Class is well-learned, features are consistent |\n",
    "| **Overlapping clusters** | Model confuses these classes (e.g., \"husky\" vs \"wolf\") |\n",
    "| **Outliers** | Mislabeled images or unusual examples |\n",
    "| **Smooth transitions** | Network learns continuous representations (e.g., dog breeds form a continuum) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: When to Use Each**\n",
    "\n",
    "| **Method** | **When to Use** | **Limitation** |\n",
    "|------------|-----------------|----------------|\n",
    "| **Nearest Neighbors** | Find similar images, debug specific examples | Doesn't show overall structure |\n",
    "| **t-SNE/UMAP** | Visualize overall dataset structure, find clusters | Slow for large datasets; can distort global structure |\n",
    "| **PCA** | Quick linear projection, preserve variance | May miss non-linear relationships |\n",
    "\n",
    "**Key Insight:** Representation space visualization shows **how the network organizes its knowledge**—revealing both its strengths (semantic clustering) and weaknesses (class confusion).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
