{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6044a19e",
   "metadata": {},
   "source": [
    "# Feature Visualisation \n",
    "\n",
    "In this chapter we uncover:\n",
    "- What patterns are individual neurons actually detecting\n",
    "- Where in the image the network is focusing to make a descision\n",
    "- How to understand what features the model is extracting\n",
    "\n",
    "We'll be following 2 main approaches:\n",
    "1. Feature-based explanantions $\\to$ DeconvNet\n",
    "2. Descision-Based Explanations $\\to$ CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b67a75",
   "metadata": {},
   "source": [
    "## Visualizing The Weights \n",
    "\n",
    "The most basic inspection approach is to directly visualize the learned weights, primarily applicable to the **first convolutional layer**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8c51f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **What We See in First Layer Weights**\n",
    "\n",
    "First layer filters operate on raw RGB pixels, making them interpretable as visual templates:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/InitialLayer.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "| **Pattern** | **What It Detects** |\n",
    "|-------------|---------------------|\n",
    "| **Edge detectors** | Oriented gradients (horizontal, vertical, diagonal) |\n",
    "| **Color blobs** | Specific color combinations (red, green, blue) |\n",
    "| **Gabor-like patterns** | Textures at various orientations and frequencies |\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "Let the first convolution layer have wieghts \n",
    "$$\n",
    "\\mathbf{W} \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times K_h \\times K_w}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $C_{out}$ = Number of output channels\n",
    "- $C_{in}$ = Number of input channels (e.g. 3 for RGB)\n",
    "- $K_h, K_w$ = Kernel weights heigh and width (e.g. 11x11 AlexNet)\n",
    "\n",
    "Each filter $\\mathbf{W}_i$ is a tensor shape $[C_{in}, K_h, K_w]$\n",
    "\n",
    "To visualize filter $i$:\n",
    "1. **Extract $\\mathbf{W}_i \\in \\mathbb{R}^{C_{in} \\times K_h \\times K_w}$**\n",
    "2. **Permute** to $[K_h, K_w, C_in]$ for image display\n",
    "3. **Normalize** values to [0,1] for visualisation:\n",
    "\n",
    "$$\\mathbf{W}_i^{\\text{norm}} = \\frac{\\mathbf{W}_i - \\text{min}(\\mathbf{W}_i)}{\\text{max}(\\mathbf(W_i))\\text{min}(\\mathbf{W}_i)}$$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pretrained model\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# Extract first conv layer weights: [out_channels, in_channels, height, width]\n",
    "first_layer_weights = model.features[0].weight.data  # Shape: [64, 3, 11, 11]\n",
    "\n",
    "# Visualize first 64 filters\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Normalize to [0, 1] for display\n",
    "    weight = first_layer_weights[i].permute(1, 2, 0)  # [11, 11, 3]\n",
    "    weight = (weight - weight.min()) / (weight.max() - weight.min())\n",
    "    ax.imshow(weight.cpu().numpy())\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "| **Problem** | **Why It Fails** |\n",
    "|-------------|------------------|\n",
    "| **Only Layer 1** | Deeper layers have abstract, high-dimensional features (e.g., 3√ó3√ó256 tensors) ‚Äî uninterpretable blobs |\n",
    "| **No context** | Shows *what* filters detect, not *where* they activate on real images |\n",
    "| **Static view** | Ignores how features compose hierarchically through the network |\n",
    "\n",
    "**Better approach:** Visualize **activations** and **gradients** instead ‚Üí DeconvNet, CAM.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410b6dc",
   "metadata": {},
   "source": [
    "## Maximally Activating Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e21ea",
   "metadata": {},
   "source": [
    "**Key Question:** \"What kind of input patterns cause a specific neuron to activate most strongly?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **The Approach**\n",
    "\n",
    "Since a neuron's **receptive field** determines what it can \"see\" (and this grows with network depth), we can find image patches that maximally excite specific neurons:\n",
    "\n",
    "**Algorithm:**\n",
    "1. Select a target neuron/feature map at any layer\n",
    "2. Run thousands of images through the network\n",
    "3. Record activation values for that neuron\n",
    "4. Extract and visualize the top-K image patches with highest activations\n",
    "\n",
    "\n",
    "---\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "Given a convolutional layer's activations: $\\mathbf{A} \\in \\mathbb{R}^{C \\times H \\times W}$ be the activation tensor for a given image, where:\n",
    "\n",
    "- $C$ = The number of chnanels (filters)\n",
    "- $H, W$ = Spatial dimensions\n",
    "\n",
    "for a chosen filter $c^*$:\n",
    "1. **Extract the activation map** fpr filter $C^*$: $$a_{c^*} \\in \\mathbb{R}^{H \\times W}$$\n",
    "2. **Find the maximally activating location:** $$(i^*, j^*) = \\mathbf{argmax}_{(i,j)}\\mathbf{a}_{c^*}[i,j]$$\n",
    "3. **Extract the corresponding recepive feild patch** from the input image that led to this activation at $(i^*, j^*)$\n",
    "   - **Retreive Layer Parameters**, For each conv. and Pooling layer up to target layer collect {kernel size, stride, padding, dialiation (if used)}.\n",
    "   - **Compute the RF size**\n",
    "   - **Compute the input Coordinates cooresponding to $(i^*, j^*)$** for a single layer: \n",
    "     - $x_{center} = s \\cdot i^* - p + \\lfloor\\frac{k-1}{2}\\rfloor$\n",
    "     - $y_{center} = s \\cdot j^* - p + \\lfloor\\frac{k-1}{2}\\rfloor$\n",
    "   - **Detemine the Patch Bounds** from $(x_{center}, y_{center})$ which is the RF size.\n",
    "   - **Crop the patch from the input image**\n",
    "4. **Repeat for all images** in the dataset, and collect the top-$K$ patches with the highest activation for filter $c^*$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Hook to capture activations\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook on target layer (e.g., conv3_3)\n",
    "model.features[16].register_forward_hook(get_activation('conv3_3'))\n",
    "\n",
    "# Process images\n",
    "patches = []\n",
    "for img_path in image_dataset:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    \n",
    "    # Forward pass\n",
    "    model(img_tensor)\n",
    "    \n",
    "    # Extract activation for specific filter (e.g., filter 42)\n",
    "    activation_map = activations['conv3_3'][0, 42]  # [H, W]\n",
    "    max_row, max_col = np.unravel_index(activation_map.argmax(), activation_map.shape)\n",
    "    \n",
    "    # Extract corresponding receptive field patch from original image\n",
    "    patch = extract_receptive_field_patch(img, max_row, max_col, layer_depth=16)\n",
    "    patches.append((activation_map.max().item(), patch))\n",
    "\n",
    "# Display top-9 patches\n",
    "top_patches = sorted(patches, reverse=True)[:9]\n",
    "visualize_patches(top_patches)\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/Layer1.png\" width=\"270\"/>\n",
    "<img src=\"../images/chap8/Layer2.png\" width=\"370\"/>\n",
    "<img src=\"../images/chap8/Layer4.png\" width=\"350\"/>\n",
    "<img src=\"../images/chap8/Layer6.png\" width=\"350\"/>\n",
    "<p><i>Example: Layer 1, 2, 4 and 6 neuron activates </i></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "### **Advantages & Limitations**\n",
    "\n",
    "| **‚úÖ Advantages** | **‚ùå Limitations** |\n",
    "|-------------------|-------------------|\n",
    "| Works for **any layer** (not just Layer 1) | Doesn't explain **which class** the features belong to |\n",
    "| Data-driven and accurate | Doesn't explain the **final decision** |\n",
    "| No gradients needed (only forward passes) | Limited by **dataset coverage** (only sees patterns in training data) |\n",
    "| Produces interpretable patterns at every depth | Doesn't work well for **fully connected layers** (no spatial structure) |\n",
    "\n",
    "**Key Insight:** Shows *what* activates neurons, but not *why* the network makes specific predictions. For decision explanations, we need **CAM** (Class Activation Mapping).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd8548",
   "metadata": {},
   "source": [
    "## Visualizing the Representation Space\n",
    "\n",
    "**Key Question:** \"What does the network represent as a whole? How does it organize different images internally?\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22ca76",
   "metadata": {},
   "source": [
    "\n",
    "### **The Concept**\n",
    "\n",
    "**At each layer**, an image is transformed into a high-dimensional vector:\n",
    "- **Early layers**: Low-level features (edges, textures) ‚Üí vectors encode spatial patterns\n",
    "- **Deep layers**: High-level features (objects, concepts) ‚Üí vectors encode semantic meaning\n",
    "- **Key insight**: Similar images produce similar vectors in representation space\n",
    "\n",
    "**The Problem:** \n",
    "Fully connected layers produce high-dimensional vectors (e.g., 4096-dimensional) with **no spatial structure**‚Äîwe can't use patch-based visualization anymore.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution 1: Nearest Neighbors in Feature Space**\n",
    "\n",
    "**Idea:** Extract feature vectors from a chosen layer, then find images with the most similar vectors.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Choose a layer (typically the last FC layer before classification)\n",
    "2. Extract feature vectors for all images: $\\mathbf{x}_i \\in \\mathbb{R}^d$ where $d$ = 4096\n",
    "3. For a query image, compute distances: $\\text{distance}(\\mathbf{x}_{\\text{query}}, \\mathbf{x}_i) = \\|\\mathbf{x}_{\\text{query}} - \\mathbf{x}_i\\|_2$\n",
    "4. Retrieve top-K nearest neighbors (smallest distances)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/NNRespSpace.png\" width=\"570\"/>\n",
    "<p><i>Test image (left of red line), L2 nearest neighbors in feature space (right)</i></p>\n",
    "</div>\n",
    "\n",
    "**What This Reveals:**\n",
    "- **Learned similarity**: What the network considers \"similar\" (may differ from human perception)\n",
    "- **Category structure**: How well classes are separated in feature space\n",
    "- **Invariances**: Network ignores pose, lighting, background variations\n",
    "\n",
    "**Use Cases:**\n",
    "- **Debugging**: Find mislabeled images (distant from their class cluster)\n",
    "- **Class confusion**: Identify which classes have overlapping representations\n",
    "- **Dataset bias**: Detect spurious correlations (e.g., \"boats\" always near water)\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation: Nearest Neighbors**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet50(pretrained=True).eval()\n",
    "\n",
    "# Remove final classification layer to get features\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Output: [N, 2048, 1, 1]\n",
    "\n",
    "# Extract features for all images\n",
    "features = []\n",
    "for img_path in dataset:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature = feature_extractor(img_tensor).flatten()  # [2048]\n",
    "        features.append(feature.cpu().numpy())\n",
    "\n",
    "features = np.array(features)  # [N, 2048]\n",
    "\n",
    "# Find nearest neighbors for a query image\n",
    "nn_model = NearestNeighbors(n_neighbors=10, metric='euclidean')\n",
    "nn_model.fit(features)\n",
    "\n",
    "query_feature = features[query_idx].reshape(1, -1)\n",
    "distances, indices = nn_model.kneighbors(query_feature)\n",
    "\n",
    "# Visualize results\n",
    "visualize_nearest_neighbors(query_idx, indices[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution 2: Low-Dimensional Embeddings**\n",
    "\n",
    "**The Problem:** Feature vectors are 2048-4096 dimensional‚Äîimpossible to visualize directly.\n",
    "\n",
    "**The Solution:** Project high-dimensional vectors into 2D/3D while **preserving local distances**.\n",
    "\n",
    "| **Method** | **How It Works** | **Best For** |\n",
    "|------------|------------------|--------------|\n",
    "| **t-SNE** | Non-linear; preserves local neighborhoods via probability distributions | Exploring clusters and local structure |\n",
    "| **UMAP** | Non-linear; faster than t-SNE, preserves global structure better | Large datasets, hierarchical structure |\n",
    "| **PCA** | Linear projection onto principal components | Quick overview, preserving variance |\n",
    "\n",
    "**Key Principle:** Points close in high-dimensional space should remain close in 2D.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/2dembed.png\" width=\"600\"/>\n",
    "<p><i>t-SNE visualization of ImageNet features: each color = different class</i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation: t-SNE Visualization**\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract features (same as above)\n",
    "features = np.array(features)  # [N, 2048]\n",
    "labels = np.array(labels)       # [N] class labels\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)  # [N, 2]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                      c=labels, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of Feature Space')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Embeddings Reveal**\n",
    "\n",
    "| **Observation** | **Interpretation** |\n",
    "|-----------------|-------------------|\n",
    "| **Tight clusters** | Class is well-learned, features are consistent |\n",
    "| **Overlapping clusters** | Model confuses these classes (e.g., \"husky\" vs \"wolf\") |\n",
    "| **Outliers** | Mislabeled images or unusual examples |\n",
    "| **Smooth transitions** | Network learns continuous representations (e.g., dog breeds form a continuum) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: When to Use Each**\n",
    "\n",
    "| **Method** | **When to Use** | **Limitation** |\n",
    "|------------|-----------------|----------------|\n",
    "| **Nearest Neighbors** | Find similar images, debug specific examples | Doesn't show overall structure |\n",
    "| **t-SNE/UMAP** | Visualize overall dataset structure, find clusters | Slow for large datasets; can distort global structure |\n",
    "| **PCA** | Quick linear projection, preserve variance | May miss non-linear relationships |\n",
    "\n",
    "**Key Insight:** Representation space visualization shows **how the network organizes its knowledge**‚Äîrevealing both its strengths (semantic clustering) and weaknesses (class confusion).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d3432",
   "metadata": {},
   "source": [
    "## Model Inversion: DeconvNet & Guided Backpropagation\n",
    "\n",
    "**Key Question:** \"Which input pixels caused a specific neuron to activate strongly?\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a97dd",
   "metadata": {},
   "source": [
    "\n",
    "### **The Goal**\n",
    "\n",
    "We want to **reverse-engineer** what the network \"sees\" by:\n",
    "1. Identifying which neurons activate for a given image\n",
    "2. Tracing back through the network to find which input pixels caused those activations\n",
    "3. Creating a visualization that highlights the important image regions\n",
    "\n",
    "**Analogy:** If a neuron fires strongly, we want to ask: \"What part of the original image made you so excited?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **When Does This Happen?**\n",
    "\n",
    "**‚è∞ After Training (Inference/Analysis Phase)**\n",
    "\n",
    "- The network is **already trained** and frozen (weights are fixed)\n",
    "- We're not updating any parameters‚Äîjust analyzing what the network has learned\n",
    "- This is a **post-hoc analysis** tool for understanding trained models\n",
    "\n",
    "**Why after training?**\n",
    "- We want to see what patterns the network *has learned* to detect\n",
    "- If we did this during training, the weights would still be changing\n",
    "- Think of it as \"interrogating\" a trained expert about their decision-making process\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step: How Guided Backpropagation Works**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Forward Pass an Image**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "For a network with $L$ layers, compute forward activations:\n",
    "\n",
    "$$\\mathbf{a}^{(0)} = \\mathbf{x}_{\\text{input}}$$\n",
    "\n",
    "For each layer $l = 1, 2, \\ldots, L$:\n",
    "\n",
    "$$\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$$\n",
    "\n",
    "$$\\mathbf{a}^{(l)} = f(\\mathbf{z}^{(l)})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}_{\\text{input}} \\in \\mathbb{R}^{3 \\times H \\times W}$ (RGB image)\n",
    "- $\\mathbf{W}^{(l)}$ = weights (conv kernels or FC weights)\n",
    "- $f(\\cdot)$ = activation function (ReLU, etc.)\n",
    "- $\\mathbf{a}^{(l)} \\in \\mathbb{R}^{C_l \\times H_l \\times W_l}$ = activation at layer $l$\n",
    "\n",
    "**At layer $l$:** Output is a 3D tensor with $C_l$ channels (feature maps)\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained model (frozen)\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "img = Image.open('dog.jpg')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# x_input: [1, 3, 224, 224]\n",
    "img_tensor = preprocess(img).unsqueeze(0)\n",
    "img_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "\n",
    "# Forward pass: compute all activations a^(l)\n",
    "output = model(img_tensor)\n",
    "# output: [1, 1000] class probabilities\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- Input: $224 \\times 224 \\times 3$ RGB image\n",
    "- Activations at every layer stored internally\n",
    "- Model weights are frozen (no training)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Choose a Feature Map and a Specific Activation**\n",
    "\n",
    "**What does \"feature map\" mean?**\n",
    "- At any convolutional layer, the output is a 3D tensor: `[channels, height, width]`\n",
    "- Each **channel** is a 2D \"feature map\" (also called an activation map)\n",
    "- Each feature map represents one learned \"detector\" (e.g., edge detector, texture detector)\n",
    "\n",
    "**What does \"choose an activation\" mean?**\n",
    "- Within a chosen feature map (channel), pick a **specific spatial location** (one pixel/position)\n",
    "- This location had a strong activation‚Äîwe want to know why\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "Choose a target layer $l$ and extract activations:\n",
    "\n",
    "$$\\mathbf{A}^{(l)} = \\left[\\mathbf{a}_1^{(l)}, \\mathbf{a}_2^{(l)}, \\ldots, \\mathbf{a}_{C_l}^{(l)}\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{A}^{(l)} \\in \\mathbb{R}^{C_l \\times H_l \\times W_l}$ = all feature maps at layer $l$\n",
    "- $\\mathbf{a}_c^{(l)} \\in \\mathbb{R}^{H_l \\times W_l}$ = feature map for channel $c$\n",
    "\n",
    "**Choose a specific neuron:**\n",
    "- Channel: $c^* \\in \\{1, \\ldots, C_l\\}$\n",
    "- Position: $(i^*, j^*) \\in \\{1, \\ldots, H_l\\} \\times \\{1, \\ldots, W_l\\}$\n",
    "- Activation value: $a_{c^*, i^*, j^*}^{(l)}$\n",
    "\n",
    "**Find maximum activation:**\n",
    "\n",
    "$$c^*, i^*, j^* = \\arg\\max_{c, i, j} \\mathbf{A}^{(l)}[c, i, j]$$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Hook to capture activations at target layer\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        # output: a^(l) for layer l\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook at Conv5_3 (layer 28 in VGG16)\n",
    "# This captures A^(l) with shape [1, 512, 14, 14]\n",
    "model.features[28].register_forward_hook(\n",
    "    get_activation('conv5_3')\n",
    ")\n",
    "\n",
    "# Forward pass (already done in Step 1)\n",
    "model(img_tensor)\n",
    "\n",
    "# Extract: A^(l) with C_l=512 channels, H_l=W_l=14\n",
    "layer_activations = activations['conv5_3']\n",
    "# Shape: [1, 512, 14, 14]\n",
    "\n",
    "# Choose channel c* = 42\n",
    "chosen_channel = 42\n",
    "feature_map = layer_activations[0, chosen_channel]\n",
    "# Shape: [14, 14] - one feature map a_c^(l)\n",
    "\n",
    "# Find position (i*, j*) with max activation\n",
    "max_row, max_col = np.unravel_index(\n",
    "    feature_map.argmax(), \n",
    "    feature_map.shape\n",
    ")\n",
    "# Example: max_row=7, max_col=9\n",
    "```\n",
    "\n",
    "**What we've identified:**\n",
    "- $c^* = 42$ (this detector/filter)\n",
    "- $(i^*, j^*) = (7, 9)$ in $14 \\times 14$ grid\n",
    "- Activation: $a_{42,7,9}^{(l)} = 5.7$ (example value)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Zero Out All Values Except the One of Interest**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "Create a **mask** $\\mathbf{M}^{(l)}$ that isolates one neuron:\n",
    "\n",
    "$$\\mathbf{M}^{(l)}[c, i, j] = \\begin{cases} \n",
    "\\mathbf{A}^{(l)}[c^*, i^*, j^*] & \\text{if } c = c^*, i = i^*, j = j^* \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Properties:**\n",
    "- $\\mathbf{M}^{(l)} \\in \\mathbb{R}^{C_l \\times H_l \\times W_l}$ (same shape as $\\mathbf{A}^{(l)}$)\n",
    "- Only one non-zero entry: $\\mathbf{M}^{(l)}[c^*, i^*, j^*] = a_{c^*, i^*, j^*}^{(l)}$\n",
    "- All other entries: $\\mathbf{M}^{(l)}[c, i, j] = 0$ for $(c, i, j) \\neq (c^*, i^*, j^*)$\n",
    "\n",
    "**This mask represents:** \"Only the signal from neuron $(c^*, i^*, j^*)$\"\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Create mask M^(l) with same shape as A^(l)\n",
    "mask = torch.zeros_like(layer_activations)\n",
    "# Shape: [1, 512, 14, 14], all zeros\n",
    "\n",
    "# Set only the chosen position to its activation\n",
    "mask[0, chosen_channel, max_row, max_col] = \\\n",
    "    layer_activations[0, chosen_channel, max_row, max_col]\n",
    "\n",
    "# Now mask has:\n",
    "# M^(l)[42, 7, 9] = 5.7 (example)\n",
    "# M^(l)[c, i, j] = 0 for all other (c,i,j)\n",
    "```\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "Original A^(l) for channel 42:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 0.2  0.5  0.1  ‚îÇ\n",
    "‚îÇ 0.4  0.8  0.6  ‚îÇ\n",
    "‚îÇ ...  ...  ...  ‚îÇ\n",
    "‚îÇ 0.1  5.7  0.4  ‚îÇ ‚Üê position (7,9)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "After masking M^(l):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 0.0  0.0  0.0  ‚îÇ\n",
    "‚îÇ 0.0  0.0  0.0  ‚îÇ\n",
    "‚îÇ ...  ...  ...  ‚îÇ\n",
    "‚îÇ 0.0  5.7  0.0  ‚îÇ ‚Üê only this survives!\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Why?** Isolate contribution of this single neuron\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Propagate Back to Input (Guided Backpropagation)**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formulation**\n",
    "\n",
    "**Standard Backpropagation:**\n",
    "\n",
    "For layer $l$ with ReLU activation:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(l)}} \\odot \\mathbb{1}[\\mathbf{z}^{(l)} > 0]$$\n",
    "\n",
    "Where $\\mathbb{1}[\\cdot]$ is the indicator function.\n",
    "\n",
    "**Guided Backpropagation (Modified ReLU):**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(l)}} \\odot \\mathbb{1}[\\mathbf{z}^{(l)} > 0] \\odot \\mathbb{1}\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(l)}} > 0\\right]$$\n",
    "\n",
    "**Key Difference:** Two conditions must both be true:\n",
    "1. Forward activation was positive: $\\mathbf{z}^{(l)} > 0$\n",
    "2. Backward gradient is positive: $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(l)}} > 0$\n",
    "\n",
    "**Chain Rule (going backwards):**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_{\\text{input}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(L)}} \\cdot \\frac{\\partial \\mathbf{a}^{(L)}}{\\partial \\mathbf{z}^{(L)}} \\cdot \\ldots \\cdot \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{x}_{\\text{input}}}$$\n",
    "\n",
    "With $\\mathcal{L} = \\mathbf{M}^{(l)}$ (our mask), this gives us the gradient w.r.t. input.\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Custom ReLU implementing guided backprop\n",
    "class GuidedReLU(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Standard ReLU forward pass\n",
    "        # z^(l) -> a^(l) = ReLU(z^(l))\n",
    "        self.save_for_backward(x)\n",
    "        return torch.clamp(x, min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # grad_output = dL/da^(l)\n",
    "        x, = self.saved_tensors  # z^(l)\n",
    "        \n",
    "        grad_input = grad_output.clone()\n",
    "        \n",
    "        # Condition 1: forward was positive\n",
    "        grad_input[x <= 0] = 0\n",
    "        \n",
    "        # Condition 2: gradient is positive\n",
    "        grad_input[grad_output <= 0] = 0\n",
    "        \n",
    "        # Returns: dL/dz^(l)\n",
    "        return grad_input\n",
    "\n",
    "# Replace all ReLUs in model\n",
    "def replace_relu_with_guided_relu(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, torch.nn.ReLU):\n",
    "            setattr(model, child_name, GuidedReLU())\n",
    "        else:\n",
    "            # Recursive for nested modules\n",
    "            replace_relu_with_guided_relu(child)\n",
    "\n",
    "# Apply to model\n",
    "replace_relu_with_guided_relu(model)\n",
    "\n",
    "# Backpropagate from mask\n",
    "model.zero_grad()\n",
    "mask.backward(gradient=torch.ones_like(mask))\n",
    "\n",
    "# Extract gradient: dL/dx_input\n",
    "gradient = img_tensor.grad.data\n",
    "# Shape: [1, 3, 224, 224]\n",
    "```\n",
    "\n",
    "**What we computed:**\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} \\mathbf{M}^{(l)} = \\frac{\\partial \\mathbf{M}^{(l)}}{\\partial \\mathbf{x}_{\\text{input}}}$$\n",
    "\n",
    "This shows which input pixels contributed to neuron $(c^*, i^*, j^*)$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Guided Backpropagation Works Better**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Comparison**\n",
    "\n",
    "**Standard ReLU Backward:**\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Passes all gradients where forward was positive (including negative gradients).\n",
    "\n",
    "**Guided ReLU Backward:**\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\begin{cases}\n",
    "1 & \\text{if } x > 0 \\text{ AND } \\frac{\\partial \\mathcal{L}}{\\partial f(x)} > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Only passes positive gradients where forward was also positive.\n",
    "\n",
    "**Effect on Visualization:**\n",
    "- Standard: Negative gradients create noisy artifacts\n",
    "- Guided: Only positive contributions ‚Üí sharper, cleaner images\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Comparison Table**\n",
    "\n",
    "| **Method** | **ReLU Backward Rule** | **Output Quality** |\n",
    "|------------|------------------------|-------------------|\n",
    "| **Standard** | Pass if $x > 0$ | Blurry, noisy |\n",
    "| **Deconvolution** | Pass if $x > 0$ (uses transpose conv) | Somewhat sharp |\n",
    "| **Guided** | Pass if $x > 0$ AND $\\nabla > 0$ | Sharp, clean |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Standard backprop through ReLU\n",
    "# Forward: x = [-1, 2, -3, 4]\n",
    "# Forward output: [0, 2, 0, 4]\n",
    "# Backward gradient: [-0.5, 0.8, -0.2, 1.1]\n",
    "# Standard passes: [0, 0.8, 0, 1.1]\n",
    "#                   ‚Üë zeros from forward\n",
    "\n",
    "# Guided backprop\n",
    "# Guided passes: [0, 0.8, 0, 1.1]\n",
    "#                ‚Üë zeros from forward AND negative grad\n",
    "# (In this case same, but filters negatives too)\n",
    "```\n",
    "\n",
    "**Result:** Guided produces interpretable heatmaps!\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **What the Output Shows**\n",
    "\n",
    "**Interpretation of the Gradient Visualization:**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "**üìê Mathematical Meaning**\n",
    "\n",
    "The output $\\nabla_{\\mathbf{x}} \\mathbf{M}^{(l)}$ represents:\n",
    "\n",
    "$$\\frac{\\partial a_{c^*, i^*, j^*}^{(l)}}{\\partial \\mathbf{x}[p, q, r]}$$\n",
    "\n",
    "For each input pixel $(p, q)$ in channel $r$.\n",
    "\n",
    "**High magnitude = strong influence:**\n",
    "- Large positive: Increasing this pixel increases the activation\n",
    "- Values shown: How much each pixel \"contributed\" to neuron firing\n",
    "\n",
    "**Bright regions:** Pixels that caused strong activation\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "**üíª Visual Output**\n",
    "\n",
    "```\n",
    "Original image: Picture of a dog\n",
    "Chosen neuron: Channel 137, position (7,9)\n",
    "\n",
    "Heatmap shows:\n",
    "üî• Bright areas ‚Üí strong contribution\n",
    "üåë Dark areas ‚Üí little/no contribution\n",
    "\n",
    "Example:\n",
    "- Dog's ears: Bright (high gradient)\n",
    "- Dog's eyes: Bright (high gradient)\n",
    "- Background: Dark (low gradient)\n",
    "\n",
    "Interpretation:\n",
    "Neuron 137 learned to detect\n",
    "\"furry pointed structures\" like ears!\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e8415",
   "metadata": {},
   "source": [
    "## Class Activation Mapping (CAM)\n",
    "\n",
    "**Key Question:** \"Which regions of the image did the network look at to make its classification decision?\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074187a",
   "metadata": {},
   "source": [
    "\n",
    "### **The Problem CAM Solves**\n",
    "\n",
    "**Guided Backpropagation tells us:** Which pixels activated a specific neuron<br>\n",
    "**CAM tells us:** Which regions influenced the final **class prediction**\n",
    "\n",
    "**Example:**\n",
    "- Image of a dog\n",
    "- Network predicts: \"Golden Retriever\" (95% confidence)\n",
    "- **CAM shows:** Heatmap highlighting the dog's face and body (regions that made the network say \"Golden Retriever\")\n",
    "\n",
    "---\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/CAMsteps.png\" width=\"800\" />\n",
    "<img src=\"../images/chap8/whatCAm.png\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **How CAM Works: Step-by-Step**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Forward Pass and Extract Last Conv Features**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Setup**\n",
    "\n",
    "**Last convolutional layer output:**\n",
    "\n",
    "$$\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$$\n",
    "\n",
    "Where:\n",
    "- $C$ = number of channels (feature maps)\n",
    "- $H \\times W$ = spatial dimensions\n",
    "- $\\mathbf{F}_k \\in \\mathbb{R}^{H \\times W}$ = feature map for channel $k$\n",
    "\n",
    "**Example dimensions:**\n",
    "- $C = 512$ channels\n",
    "- $H = W = 7$ (7√ó7 spatial grid)\n",
    "- Each $\\mathbf{F}_k$ is a 7√ó7 feature map\n",
    "\n",
    "**Notation:**\n",
    "$$\\mathbf{F}_k[i, j] = \\text{activation at position } (i,j) \\text{ in channel } k$$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model with GAP architecture (e.g., ResNet)\n",
    "model = models.resnet50(pretrained=True).eval()\n",
    "\n",
    "# Hook to capture last conv layer\n",
    "last_conv_features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        last_conv_features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook at last conv layer (layer4 in ResNet)\n",
    "model.layer4.register_forward_hook(get_features('layer4'))\n",
    "\n",
    "# Forward pass\n",
    "img = Image.open('dog.jpg')\n",
    "img_tensor = preprocess(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "output = model(img_tensor)  # [1, 1000]\n",
    "\n",
    "# Extract features: F ‚àà R^{C√óH√óW}\n",
    "F = last_conv_features['layer4']  # Shape: [1, 512, 7, 7]\n",
    "# 512 channels, 7√ó7 spatial grid\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- $\\mathbf{F}$: [1, 512, 7, 7] tensor\n",
    "- 512 feature maps, each 7√ó7\n",
    "- Each position $(i,j)$ in each channel has a value\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Apply Global Average Pooling (GAP)**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Operation**\n",
    "\n",
    "**Global Average Pooling:** For each channel $k$, compute the **average** over all spatial positions:\n",
    "\n",
    "$$f_k = \\text{GAP}(\\mathbf{F}_k) = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\mathbf{F}_k[i, j]$$\n",
    "\n",
    "**Result:** A vector $\\mathbf{f} \\in \\mathbb{R}^C$ where:\n",
    "\n",
    "$$\\mathbf{f} = [f_1, f_2, \\ldots, f_C]$$\n",
    "\n",
    "**Intuition:**\n",
    "- Each $f_k$ = \"How much channel $k$ was activated overall\"\n",
    "- Collapses 7√ó7 grid into 1 number per channel\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Channel 1: 7√ó7 feature map\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 0.1  0.3  0.2  ‚îÇ\n",
    "‚îÇ 0.4  0.5  0.1  ‚îÇ\n",
    "‚îÇ ...  ...  ...  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì Average all values\n",
    "      f_1 = 0.28\n",
    "```\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Apply GAP manually\n",
    "# F shape: [1, 512, 7, 7]\n",
    "\n",
    "# Average over spatial dimensions (H, W)\n",
    "f = F.mean(dim=[2, 3])  # Shape: [1, 512]\n",
    "# f[k] = average of all 7√ó7 values in channel k\n",
    "\n",
    "# Alternatively, use PyTorch's built-in\n",
    "gap = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "f = gap(F).squeeze()  # Shape: [512]\n",
    "\n",
    "# Now f is a vector of 512 values\n",
    "# f[0] = average activation of channel 0\n",
    "# f[1] = average activation of channel 1\n",
    "# ...\n",
    "# f[511] = average activation of channel 511\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- $\\mathbf{f}$: [512] vector\n",
    "- Each value = average of one 7√ó7 feature map\n",
    "- This is the input to the final classification layer\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Final Classification Layer**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Operation**\n",
    "\n",
    "**Fully Connected Layer:** Compute class scores\n",
    "\n",
    "$$S_c = \\sum_{k=1}^{C} w_{c,k} \\cdot f_k$$\n",
    "\n",
    "Where:\n",
    "- $S_c$ = score for class $c$ (before softmax)\n",
    "- $w_{c,k}$ = weight connecting channel $k$ to class $c$\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{N \\times C}$ = weight matrix (N classes, C channels)\n",
    "\n",
    "**In matrix form:**\n",
    "\n",
    "$$\\mathbf{S} = \\mathbf{W} \\cdot \\mathbf{f}$$\n",
    "\n",
    "Where $\\mathbf{S} \\in \\mathbb{R}^N$ (N = 1000 classes for ImageNet)\n",
    "\n",
    "**Intuition:**\n",
    "- Each class has a weight vector $\\mathbf{w}_c = [w_{c,1}, w_{c,2}, \\ldots, w_{c,C}]$\n",
    "- $w_{c,k}$ = \"How much does channel $k$ contribute to class $c$?\"\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Extract weights from final FC layer\n",
    "fc_weights = model.fc.weight.data  # Shape: [1000, 512]\n",
    "# fc_weights[c, k] = weight from channel k to class c\n",
    "\n",
    "# Compute class scores\n",
    "# S = W ¬∑ f\n",
    "S = torch.matmul(fc_weights, f)  # Shape: [1000]\n",
    "# S[c] = score for class c\n",
    "\n",
    "# Find predicted class\n",
    "predicted_class = S.argmax().item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "# Example: 207 (Golden Retriever)\n",
    "\n",
    "# Extract weights for predicted class\n",
    "w_c = fc_weights[predicted_class]  # Shape: [512]\n",
    "# w_c[k] = importance of channel k for this class\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- $\\mathbf{w}_c$: [512] vector\n",
    "- $w_c[k]$ = weight from channel $k$ to predicted class $c$\n",
    "- These weights tell us which channels matter for this class!\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Generate Class Activation Map**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formula**\n",
    "\n",
    "**CAM for class $c$:** Weighted sum of feature maps\n",
    "\n",
    "$$\\text{CAM}_c(i, j) = \\sum_{k=1}^{C} w_{c,k} \\cdot \\mathbf{F}_k[i, j]$$\n",
    "\n",
    "Where:\n",
    "- $(i, j)$ = spatial position in the 7√ó7 grid\n",
    "- $w_{c,k}$ = weight from channel $k$ to class $c$\n",
    "- $\\mathbf{F}_k[i,j]$ = activation at position $(i,j)$ in channel $k$\n",
    "\n",
    "**In matrix form:**\n",
    "\n",
    "$$\\text{CAM}_c = \\sum_{k=1}^{C} w_{c,k} \\cdot \\mathbf{F}_k$$\n",
    "\n",
    "**Result:** $\\text{CAM}_c \\in \\mathbb{R}^{H \\times W}$ (a 7√ó7 heatmap)\n",
    "\n",
    "**Intuition:**\n",
    "- At each spatial location $(i,j)$:\n",
    "  - Look at all 512 channels\n",
    "  - Weight each by its importance to class $c$\n",
    "  - Sum them up\n",
    "- High value = \"this region strongly indicates class $c$\"\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# w_c shape: [512] (weights for predicted class)\n",
    "# F shape: [1, 512, 7, 7] (feature maps)\n",
    "\n",
    "# Compute weighted sum\n",
    "# CAM = Œ£ w_c[k] ¬∑ F[k]\n",
    "CAM = torch.zeros(7, 7)  # Initialize 7√ó7 heatmap\n",
    "\n",
    "for k in range(512):\n",
    "    # Add weighted feature map k\n",
    "    CAM += w_c[k] * F[0, k, :, :]\n",
    "\n",
    "# Alternatively, vectorized:\n",
    "CAM = torch.einsum('k,khw->hw', w_c, F[0])\n",
    "# 'k' = channels, 'h' = height, 'w' = width\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "CAM = torch.relu(CAM)  # Remove negative values\n",
    "CAM = (CAM - CAM.min()) / (CAM.max() - CAM.min())\n",
    "\n",
    "# Upsample to original image size (224√ó224)\n",
    "from torch.nn.functional import interpolate\n",
    "CAM_upsampled = interpolate(\n",
    "    CAM.unsqueeze(0).unsqueeze(0), \n",
    "    size=(224, 224), \n",
    "    mode='bilinear'\n",
    ").squeeze()\n",
    "\n",
    "# Shape: [224, 224] heatmap\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- $\\text{CAM}$: [224, 224] heatmap\n",
    "- High values = regions important for the prediction\n",
    "- Can overlay on original image!\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **Why CAM Works: The Math Behind It**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Proof**\n",
    "\n",
    "**Recall class score:**\n",
    "\n",
    "$$S_c = \\sum_{k=1}^{C} w_{c,k} \\cdot f_k$$\n",
    "\n",
    "**Substitute GAP definition:**\n",
    "\n",
    "$$S_c = \\sum_{k=1}^{C} w_{c,k} \\cdot \\left(\\frac{1}{H \\times W} \\sum_{i,j} \\mathbf{F}_k[i,j]\\right)$$\n",
    "\n",
    "**Rearrange:**\n",
    "\n",
    "$$S_c = \\frac{1}{H \\times W} \\sum_{i,j} \\underbrace{\\sum_{k=1}^{C} w_{c,k} \\cdot \\mathbf{F}_k[i,j]}_{\\text{CAM}_c(i,j)}$$\n",
    "\n",
    "**Result:**\n",
    "\n",
    "$$S_c = \\frac{1}{H \\times W} \\sum_{i,j} \\text{CAM}_c(i,j)$$\n",
    "\n",
    "**Interpretation:**\n",
    "- The class score is the **average** of the CAM\n",
    "- High CAM values = high contribution to class score\n",
    "- CAM directly shows spatial importance!\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Verification**\n",
    "\n",
    "```python\n",
    "# Verify the math\n",
    "# S_c should equal average of CAM\n",
    "\n",
    "# Method 1: Direct computation\n",
    "S_c_direct = (fc_weights[predicted_class] @ f).item()\n",
    "\n",
    "# Method 2: Average of CAM\n",
    "S_c_from_cam = CAM.mean().item() * (7 * 7)\n",
    "\n",
    "print(f\"Direct: {S_c_direct:.4f}\")\n",
    "print(f\"From CAM: {S_c_from_cam:.4f}\")\n",
    "# Should be approximately equal!\n",
    "\n",
    "# This proves CAM captures the spatial\n",
    "# contribution to the class score\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- CAM isn't arbitrary‚Äîit's mathematically grounded\n",
    "- Each pixel's value directly relates to its contribution to the prediction\n",
    "- This makes CAM interpretable and trustworthy\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Summary: CAM in 4 Steps**\n",
    "\n",
    "| **Step** | **Mathematical** | **Result** |\n",
    "|----------|------------------|------------|\n",
    "| **1. Extract features** | $\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$ | Last conv features (512√ó7√ó7) |\n",
    "| **2. Global Average Pooling** | $f_k = \\frac{1}{HW}\\sum_{i,j} \\mathbf{F}_k[i,j]$ | Feature vector (512,) |\n",
    "| **3. Get class weights** | $S_c = \\sum_k w_{c,k} f_k$ | Weights $\\mathbf{w}_c$ for class $c$ |\n",
    "| **4. Weighted sum** | $\\text{CAM}_c = \\sum_k w_{c,k} \\mathbf{F}_k$ | Heatmap (7√ó7 ‚Üí 224√ó224) |\n",
    "\n",
    "**Final Output:** A heatmap showing which regions of the image led to the class prediction! üî•\n",
    "\n",
    "|Advantages | Limitations | \n",
    "|-----------|-------------|\n",
    "| No backward Pass | Requires learning separate class-specific weights|\n",
    "| Uses spatial info alread in conv Layers | overhead grows with classes/filters |\n",
    "| Produces intuitive, class-specific heatmaps| Requires GAP $\\to$ restrict desgin | \n",
    "| Works without bounding-box supervision | Mostly Limited to classification problems |\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dccb2",
   "metadata": {},
   "source": [
    "## Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
    "\n",
    "**Key Question:** \"Can we obtain class-specific heatmaps **without modifying the model architecture** (no GAP requirement)?\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c29e6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **The Problem Grad-CAM Solves**\n",
    "\n",
    "**CAM's Limitation:**\n",
    "- Requires **Global Average Pooling (GAP)** + single FC layer\n",
    "- Must retrain model with this specific architecture\n",
    "- Doesn't work with existing pretrained models that use different architectures\n",
    "\n",
    "**Grad-CAM's Solution:**\n",
    "- Works with **any CNN architecture** (VGG, ResNet, Inception, etc.)\n",
    "- No retraining needed\n",
    "- Uses **gradients** instead of learned weights to measure feature map importance\n",
    "\n",
    "---\n",
    "\n",
    "### **CAM vs Grad-CAM: The Key Difference**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**CAM: Learned Weights**\n",
    "\n",
    "**How it determines importance:**\n",
    "\n",
    "$$\\alpha_k^c = w_{c,k}$$\n",
    "\n",
    "Where $w_{c,k}$ is the **learned weight** from the FC layer connecting:\n",
    "- Feature map $k$ \n",
    "- To class $c$\n",
    "\n",
    "**Problem:**\n",
    "- These weights only exist if you have GAP ‚Üí FC architecture\n",
    "- Can't use on arbitrary pretrained models\n",
    "\n",
    "**CAM Formula:**\n",
    "\n",
    "$$\\text{CAM}_c = \\sum_{k} w_{c,k} \\cdot \\mathbf{F}_k$$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**Grad-CAM: Gradient-derived Weights**\n",
    "\n",
    "**How it determines importance:**\n",
    "\n",
    "$$\\alpha_k^c = \\frac{1}{Z} \\sum_{i,j} \\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]}$$\n",
    "\n",
    "Where:\n",
    "- $S_c$ = score for class $c$ (before softmax)\n",
    "- $\\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]}$ = gradient of class score w.r.t. feature map $k$ at position $(i,j)$\n",
    "- We **average the gradients** to get the importance $\\alpha_k^c$\n",
    "\n",
    "**Advantage:**\n",
    "- Works with any architecture\n",
    "- Gradients exist for any differentiable model\n",
    "\n",
    "**Grad-CAM Formula:**\n",
    "\n",
    "$$\\text{Grad-CAM}_c = \\text{ReLU}\\left(\\sum_{k} \\alpha_k^c \\cdot \\mathbf{F}_k\\right)$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **The Intuition Behind Grad-CAM**\n",
    "\n",
    "**Question:** How much does feature map $k$ contribute to class $c$?\n",
    "\n",
    "**Answer:** Look at the gradient $\\frac{\\partial S_c}{\\partial \\mathbf{F}_k}$\n",
    "\n",
    "**Why gradients?**\n",
    "\n",
    "| **Gradient Value** | **Meaning** | **Interpretation** |\n",
    "|-------------------|-------------|-------------------|\n",
    "| **Large positive** | Increasing this feature map increases class score | This feature **supports** the class |\n",
    "| **Near zero** | Changes don't affect class score | This feature is **irrelevant** to this class |\n",
    "| **Large negative** | Increasing this feature map decreases class score | This feature **contradicts** the class |\n",
    "\n",
    "**By averaging gradients across spatial locations:**\n",
    "- We get a single importance weight $\\alpha_k^c$ per feature map $k$ for class $c$\n",
    "- High $\\alpha_k^c$ = \"feature map $k$ strongly supports class $c$\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Grad-CAM Step-by-Step**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Forward Pass and Extract Feature Maps**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Setup**\n",
    "\n",
    "**Choose any convolutional layer** (typically the last one):\n",
    "\n",
    "$$\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$$\n",
    "\n",
    "Where:\n",
    "- $C$ = number of feature maps (channels)\n",
    "- $H \\times W$ = spatial dimensions\n",
    "- $\\mathbf{F}_k[i, j]$ = activation at position $(i,j)$ in channel $k$\n",
    "\n",
    "**Forward pass to get class scores:**\n",
    "\n",
    "$$\\mathbf{S} = f_{\\text{model}}(\\mathbf{x}) \\in \\mathbb{R}^N$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input image\n",
    "- $N$ = number of classes\n",
    "- $S_c$ = score for class $c$ (logit, before softmax)\n",
    "\n",
    "**No architecture constraints!** Can be:\n",
    "- VGG with multiple FC layers\n",
    "- ResNet with adaptive pooling\n",
    "- Any CNN architecture\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ANY pretrained model (no GAP requirement!)\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "# Or: models.resnet50(), models.inception_v3(), etc.\n",
    "\n",
    "# Hook to capture feature maps\n",
    "feature_maps = {}\n",
    "gradients = {}\n",
    "\n",
    "def forward_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_maps[name] = output\n",
    "        # Enable gradient tracking\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "    return hook\n",
    "\n",
    "# Register hook at target layer\n",
    "# For VGG16: last conv layer (features.29)\n",
    "# For ResNet: layer4\n",
    "target_layer = model.features[29]\n",
    "target_layer.register_forward_hook(forward_hook('target'))\n",
    "\n",
    "# Forward pass\n",
    "img = Image.open('dog.jpg')\n",
    "img_tensor = preprocess(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "img_tensor.requires_grad_(True)\n",
    "\n",
    "output = model(img_tensor)  # [1, 1000]\n",
    "\n",
    "# Extract feature maps F\n",
    "F = feature_maps['target']  # Shape: [1, 512, 14, 14]\n",
    "\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- $\\mathbf{F}$: [1, 512, 14, 14] feature maps\n",
    "- $\\mathbf{S}$: [1, 1000] class scores\n",
    "- Model can have ANY architecture after conv layers!\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Compute Gradients of Class Score w.r.t. Feature Maps**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Operation**\n",
    "\n",
    "**For target class $c$, compute:**\n",
    "\n",
    "$$\\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i, j]}$$\n",
    "\n",
    "For all:\n",
    "- Channels $k = 1, 2, \\ldots, C$\n",
    "- Spatial positions $(i, j)$\n",
    "\n",
    "**Result:** A gradient tensor\n",
    "\n",
    "$$\\nabla_{\\mathbf{F}} S_c \\in \\mathbb{R}^{C \\times H \\times W}$$\n",
    "\n",
    "**Intuition at each position $(i,j)$ in channel $k$:**\n",
    "- \"If I increase $\\mathbf{F}_k[i,j]$ by a small amount, how much does $S_c$ increase?\"\n",
    "- Positive gradient = this activation helps predict class $c$\n",
    "- Negative gradient = this activation hurts prediction of class $c$\n",
    "\n",
    "**This works via backpropagation:**\n",
    "- From output class score $S_c$\n",
    "- Back through all layers\n",
    "- To the chosen feature map layer\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Choose target class\n",
    "# Option 1: Use predicted class\n",
    "predicted_class = output.argmax(dim=1).item()\n",
    "# Option 2: Use ground truth class\n",
    "# predicted_class = true_label\n",
    "\n",
    "# Extract score for class c\n",
    "S_c = output[0, predicted_class]\n",
    "\n",
    "# Backward pass: compute ‚àÇS_c/‚àÇF\n",
    "model.zero_grad()\n",
    "S_c.backward(retain_graph=True)\n",
    "\n",
    "# Extract gradients: ‚àÇS_c/‚àÇF_k[i,j]\n",
    "gradients = F.grad  # Shape: [1, 512, 14, 14]\n",
    "\n",
    "# gradients[0, k, i, j] = ‚àÇS_c/‚àÇF_k[i,j]\n",
    "```\n",
    "\n",
    "**What we computed:**\n",
    "\n",
    "$$\\nabla_{\\mathbf{F}} S_c = \\begin{bmatrix}\n",
    "\\frac{\\partial S_c}{\\partial \\mathbf{F}_1[1,1]} & \\cdots & \\frac{\\partial S_c}{\\partial \\mathbf{F}_1[H,W]} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial S_c}{\\partial \\mathbf{F}_C[1,1]} & \\cdots & \\frac{\\partial S_c}{\\partial \\mathbf{F}_C[H,W]}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "Feature Map k=42 (14√ó14):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 0.2  0.5  0.1  ‚îÇ\n",
    "‚îÇ 0.4  0.8  0.6  ‚îÇ\n",
    "‚îÇ ...  ...  ...  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Gradient ‚àÇS_c/‚àÇF_42 (14√ó14):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 0.1  0.3 -0.1  ‚îÇ ‚Üê gradients show\n",
    "‚îÇ 0.5  0.9  0.2  ‚îÇ   which positions\n",
    "‚îÇ ...  ...  ...  ‚îÇ   matter for class c\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Global Average Pooling of Gradients to Get Weights**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Operation**\n",
    "\n",
    "**For each feature map $k$, compute importance weight:**\n",
    "\n",
    "$$\\alpha_k^c = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i, j]}$$\n",
    "\n",
    "**Result:** A vector of importance weights\n",
    "\n",
    "$$\\boldsymbol{\\alpha}^c = [\\alpha_1^c, \\alpha_2^c, \\ldots, \\alpha_C^c] \\in \\mathbb{R}^C$$\n",
    "\n",
    "**Intuition:**\n",
    "- $\\alpha_k^c$ = \"On average, how much does feature map $k$ contribute to class $c$?\"\n",
    "- We average across spatial locations because:\n",
    "  - Some positions may have high positive gradients\n",
    "  - Some may have negative gradients\n",
    "  - We want the **overall** importance of the entire feature map\n",
    "\n",
    "**Why GAP (averaging)?**\n",
    "- Reduces spatial dimensions: $(C \\times H \\times W) \\to (C)$\n",
    "- Gives us one weight per channel\n",
    "- Same operation as CAM, but applied to **gradients** instead of activations\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# Compute Œ±_k^c for each channel k\n",
    "# gradients shape: [1, 512, 14, 14]\n",
    "\n",
    "# Average over spatial dimensions (H, W)\n",
    "alpha = gradients.mean(dim=[2, 3])  # Shape: [1, 512]\n",
    "# alpha[0, k] = Œ±_k^c\n",
    "\n",
    "# Remove batch dimension\n",
    "alpha = alpha.squeeze(0)  # Shape: [512]\n",
    "\n",
    "# Now we have importance weights:\n",
    "# alpha[0] = Œ±_1^c (importance of channel 1 for class c)\n",
    "# alpha[1] = Œ±_2^c (importance of channel 2 for class c)\n",
    "# ...\n",
    "# alpha[511] = Œ±_512^c (importance of channel 512 for class c)\n",
    "```\n",
    "\n",
    "**Example values:**\n",
    "```python\n",
    "alpha[0] = 0.42   # Channel 0: moderately important\n",
    "alpha[1] = -0.15  # Channel 1: slightly suppresses class c\n",
    "alpha[2] = 0.89   # Channel 2: very important!\n",
    "alpha[3] = 0.01   # Channel 3: nearly irrelevant\n",
    "...\n",
    "```\n",
    "\n",
    "**Comparison with CAM:**\n",
    "\n",
    "| **Method** | **Weight Source** | **Formula** |\n",
    "|------------|------------------|-------------|\n",
    "| **CAM** | Learned FC weights | $\\alpha_k^c = w_{c,k}$ |\n",
    "| **Grad-CAM** | Gradient averages | $\\alpha_k^c = \\frac{1}{HW}\\sum_{i,j} \\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]}$ |\n",
    "\n",
    "Both give us: \"importance of channel $k$ for class $c$\"\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Compute Grad-CAM (Weighted Sum + ReLU)**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Formula**\n",
    "\n",
    "**Grad-CAM for class $c$:**\n",
    "\n",
    "$$\\text{Grad-CAM}_c = \\text{ReLU}\\left(\\sum_{k=1}^{C} \\alpha_k^c \\cdot \\mathbf{F}_k\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_k^c$ = importance weight for channel $k$\n",
    "- $\\mathbf{F}_k \\in \\mathbb{R}^{H \\times W}$ = feature map for channel $k$\n",
    "- ReLU = $\\max(0, x)$ (zero out negative values)\n",
    "\n",
    "**Result:** $\\text{Grad-CAM}_c \\in \\mathbb{R}^{H \\times W}$ (heatmap)\n",
    "\n",
    "**Breakdown:**\n",
    "\n",
    "1. **Weighted sum:** $\\sum_k \\alpha_k^c \\cdot \\mathbf{F}_k$\n",
    "   - At each spatial location $(i,j)$\n",
    "   - Sum the weighted feature maps\n",
    "   - High value = many important features activate here\n",
    "\n",
    "2. **ReLU:** Keep only positive values\n",
    "   - Positive = regions that **support** the class\n",
    "   - Negative = regions that **contradict** the class\n",
    "   - We only visualize supporting regions\n",
    "\n",
    "**Why ReLU?**\n",
    "- Negative values mean \"this region suppresses the class\"\n",
    "- We want to show \"what made the network confident\"\n",
    "- Not \"what made it less confident\"\n",
    "- ReLU removes distracting negative influences\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Implementation**\n",
    "\n",
    "```python\n",
    "# alpha shape: [512] (importance weights)\n",
    "# F shape: [1, 512, 14, 14] (feature maps)\n",
    "\n",
    "# Remove batch dimension from F\n",
    "F = F.squeeze(0)  # Shape: [512, 14, 14]\n",
    "\n",
    "# Compute weighted sum: Œ£ Œ±_k^c ¬∑ F_k\n",
    "# Method 1: Loop\n",
    "Grad_CAM = torch.zeros(14, 14)\n",
    "for k in range(512):\n",
    "    Grad_CAM += alpha[k] * F[k, :, :]\n",
    "\n",
    "# Method 2: Vectorized (faster)\n",
    "Grad_CAM = torch.einsum('k,khw->hw', alpha, F)\n",
    "# 'k'=channels, 'h'=height, 'w'=width\n",
    "\n",
    "# Shape: [14, 14] raw heatmap\n",
    "\n",
    "# Apply ReLU (remove negative values)\n",
    "Grad_CAM = torch.relu(Grad_CAM)\n",
    "\n",
    "# Normalize to [0, 1] for visualization\n",
    "Grad_CAM = Grad_CAM - Grad_CAM.min()\n",
    "Grad_CAM = Grad_CAM / Grad_CAM.max()\n",
    "\n",
    "# Upsample to original image size (224√ó224)\n",
    "from torch.nn.functional import interpolate\n",
    "Grad_CAM = interpolate(\n",
    "    Grad_CAM.unsqueeze(0).unsqueeze(0),\n",
    "    size=(224, 224),\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ").squeeze()\n",
    "\n",
    "# Final shape: [224, 224] heatmap\n",
    "```\n",
    "\n",
    "**What we have:**\n",
    "- Grad-CAM heatmap: [224, 224]\n",
    "- Values range [0, 1]\n",
    "- High values = important regions for class prediction\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **Why ReLU? The Importance of Positive vs Negative Gradients**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üìê Mathematical Interpretation**\n",
    "\n",
    "**Gradient sign tells us:**\n",
    "\n",
    "$$\\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]} \\begin{cases}\n",
    "> 0 & \\text{Region supports class } c \\\\\n",
    "< 0 & \\text{Region suppresses class } c \\\\\n",
    "\\approx 0 & \\text{Region irrelevant to class } c\n",
    "\\end{cases}$$\n",
    "\n",
    "**After weighted sum, before ReLU:**\n",
    "\n",
    "$$\\text{Raw value} = \\sum_{k} \\alpha_k^c \\cdot \\mathbf{F}_k[i,j]$$\n",
    "\n",
    "Can be negative if:\n",
    "- Important channels ($\\alpha_k^c$ large)\n",
    "- Have low activation ($\\mathbf{F}_k[i,j]$ small)\n",
    "- Or: negative-weight channels are highly active\n",
    "\n",
    "**ReLU decision:**\n",
    "\n",
    "$$\\text{Grad-CAM}[i,j] = \\max\\left(0, \\sum_{k} \\alpha_k^c \\cdot \\mathbf{F}_k[i,j]\\right)$$\n",
    "\n",
    "**Result:**\n",
    "- Keep regions that **positively** contribute to class $c$\n",
    "- Remove regions that **negatively** contribute to class $c$\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "**üíª Visual Example**\n",
    "\n",
    "**Example: Dog vs Cat Classification**\n",
    "\n",
    "```python\n",
    "# For class c = \"dog\"\n",
    "# Before ReLU:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  0.8   0.9   0.7  ‚îÇ ‚Üê Dog's face (high positive)\n",
    "‚îÇ  0.6   0.5   0.4  ‚îÇ\n",
    "‚îÇ -0.2  -0.3  -0.1  ‚îÇ ‚Üê Cat toy in corner (negative)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "# After ReLU:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  0.8   0.9   0.7  ‚îÇ ‚Üê Dog features highlighted\n",
    "‚îÇ  0.6   0.5   0.4  ‚îÇ\n",
    "‚îÇ  0.0   0.0   0.0  ‚îÇ ‚Üê Confusing regions removed\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Bright regions:** \"This is why I said dog\"\n",
    "- **Dark regions:** Either:\n",
    "  - Irrelevant (gradient ‚âà 0)\n",
    "  - Contradictory (gradient < 0, removed by ReLU)\n",
    "\n",
    "**Why this is useful:**\n",
    "- Shows **positive evidence** for the prediction\n",
    "- Ignores **negative evidence** (what it's not)\n",
    "- Creates intuitive \"what made the network confident\" visualizations\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### **Grad-CAM: Mathematical Summary**\n",
    "\n",
    "| **Step** | **Mathematical Operation** | **Result** |\n",
    "|----------|---------------------------|------------|\n",
    "| **1. Forward** | $\\mathbf{F}, \\mathbf{S} = f_{\\text{model}}(\\mathbf{x})$ | Feature maps $\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$, scores $\\mathbf{S} \\in \\mathbb{R}^N$ |\n",
    "| **2. Backward** | $\\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]}$ | Gradients $\\nabla_{\\mathbf{F}} S_c \\in \\mathbb{R}^{C \\times H \\times W}$ |\n",
    "| **3. GAP gradients** | $\\alpha_k^c = \\frac{1}{HW} \\sum_{i,j} \\frac{\\partial S_c}{\\partial \\mathbf{F}_k[i,j]}$ | Importance weights $\\boldsymbol{\\alpha}^c \\in \\mathbb{R}^C$ |\n",
    "| **4. Weighted sum + ReLU** | $\\text{Grad-CAM}_c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c \\cdot \\mathbf{F}_k\\right)$ | Heatmap $\\in \\mathbb{R}^{H \\times W}$ |\n",
    "\n",
    "**Final Output:** A class-specific heatmap showing which regions contributed to the prediction! üî•\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages & Limitations**\n",
    "\n",
    "| **‚úÖ Advantages** | **‚ùå Limitations** |\n",
    "|-------------------|-------------------|\n",
    "| Works with **any CNN** (VGG, ResNet, Inception, etc.) | Requires **backward pass** (slower than CAM) |\n",
    "| **No retraining** required | Gradients can be **noisy** |\n",
    "| Can visualize **any layer** (not just last conv) | Lower resolution than input (inherits from feature map size) |\n",
    "| **Class-discriminative** (specific to predicted class) | Doesn't show pixel-level details (use Guided Grad-CAM for that) |\n",
    "| Mathematically grounded (gradient-based importance) | ReLU removes negative evidence (both good and limiting) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. **Grad-CAM uses gradients** instead of learned weights to measure feature importance\n",
    "2. **Works with any CNN** ‚Äî no architectural constraints\n",
    "3. **Gradients show sensitivity:** $\\frac{\\partial S_c}{\\partial \\mathbf{F}_k}$ = \"how much does feature $k$ affect class $c$?\"\n",
    "4. **GAP of gradients** gives per-channel importance: $\\alpha_k^c$\n",
    "5. **ReLU keeps positive contributions** (regions that support the prediction)\n",
    "6. **Class-discriminative visualizations** without modifying the model!\n",
    "\n",
    "---\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
