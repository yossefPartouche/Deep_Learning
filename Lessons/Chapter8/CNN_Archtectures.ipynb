{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e479e5b",
   "metadata": {},
   "source": [
    "# CNN Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34abc5",
   "metadata": {},
   "source": [
    "In this section investigate the CNN Architectures that were created for image classification specifically: \n",
    "- AlexNet\n",
    "- VGGNet \n",
    "- GoogleNet\n",
    "- ResNet\n",
    "- Wide ResNet, Dense Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2605a",
   "metadata": {},
   "source": [
    "## The ImageNet Classification Challenge\n",
    "\n",
    "Given 1,431,167 Images with human labels, with 1000 Object Classes, develop a model that's able to produce that highest accuracy rate of classification with top 5 Error evaluation. \n",
    "  \n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/ImageNet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "### Pre-AlexNet\n",
    "\n",
    "The main models that were used were Shallow NN, or Classical ML models where feature extraction was manually done. \n",
    "The best models during this period were obtaining 28-25% Error Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d42aaa",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "Suddenly AlexNet (developed by Ilya Sutskever, Geoffrey Hinton and Alex Krizhevsky) was the first model that was able to significantly reduce the error rates by roughly 10%, using Deep Neural network.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/AlexNet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "| **Innovation** | **Description** | **Impact** |\n",
    "|----------------|-----------------|------------|\n",
    "| **Deep Architecture** | 8 learned layers (5 conv + 3 FC) | First successful very deep CNN |\n",
    "| **ReLU Activation** | Used ReLU instead of tanh/sigmoid | 6× faster training, no vanishing gradients |\n",
    "| **Dropout** | Applied 0.5 dropout in FC layers | Reduced overfitting significantly |\n",
    "| **Data Augmentation** | Random crops, flips, color jittering | Increased training data diversity |\n",
    "| **GPU Training** | Trained on 2 GTX 580 GPUs | Made large-scale training feasible |\n",
    "| **Local Response Normalization** | Normalization across channels | Later replaced by Batch Normalization |\n",
    "| **Overlapping Pooling** | Pool size 3×3, stride 2 | Slight accuracy improvement |\n",
    "\n",
    "### Complete Architecture\n",
    "\n",
    "**Input**: $227 \\times 227 \\times 3$ RGB image (originally stated as 224×224, but 227×227 is correct for the dimensions to work out)\n",
    "\n",
    "**Assumptions**: \n",
    "- Batch size = 1 (single image)\n",
    "- 32-bit floating point (4 bytes per value)\n",
    "- FLOPs calculated as multiply-add operations\n",
    "\n",
    "| **Layer** | **Type** | **Input Shape** | **Kernel Size** | **Filters/Units** | **Stride** | **Padding** | **Output Shape** | **Parameters** | **Memory (MB)** | **FLOPs** | **Activation** |\n",
    "|-----------|----------|-----------------|-----------------|-------------------|------------|-------------|------------------|----------------|-----------------|-----------|----------------|\n",
    "| **Input** | Input | - | - | - | - | - | $227 \\times 227 \\times 3$ | 0 | $227 \\times 227 \\times 3 \\times 4 = 0.59$ | 0 | - |\n",
    "| **Conv1** | Convolution | $227 \\times 227 \\times 3$ | $11 \\times 11$ | 96 | 4 | 0 (valid) | $55 \\times 55 \\times 96$ | $34{,}944$ | $55 \\times 55 \\times 96 \\times 4 = 1.11$ | $55 \\times 55 \\times 96 \\times (11 \\times 11 \\times 3) = 105.4M$ | ReLU |\n",
    "| **Pool1** | Max Pooling | $55 \\times 55 \\times 96$ | $3 \\times 3$ | - | 2 | 0 | $27 \\times 27 \\times 96$ | 0 | $27 \\times 27 \\times 96 \\times 4 = 0.27$ | $27 \\times 27 \\times 96 \\times 9 = 6.3M$ | - |\n",
    "| **LRN1** | Local Response Norm | $27 \\times 27 \\times 96$ | - | - | - | - | $27 \\times 27 \\times 96$ | 0 | $0.27$ | $27 \\times 27 \\times 96 \\times 10 = 7.0M$ | - |\n",
    "| **Conv2** | Convolution | $27 \\times 27 \\times 96$ | $5 \\times 5$ | 256 | 1 | 2 (same) | $27 \\times 27 \\times 256$ | $614{,}656$ | $27 \\times 27 \\times 256 \\times 4 = 0.72$ | $27 \\times 27 \\times 256 \\times (5 \\times 5 \\times 96) = 448.1M$ | ReLU |\n",
    "| **Pool2** | Max Pooling | $27 \\times 27 \\times 256$ | $3 \\times 3$ | - | 2 | 0 | $13 \\times 13 \\times 256$ | 0 | $13 \\times 13 \\times 256 \\times 4 = 0.17$ | $13 \\times 13 \\times 256 \\times 9 = 3.9M$ | - |\n",
    "| **LRN2** | Local Response Norm | $13 \\times 13 \\times 256$ | - | - | - | - | $13 \\times 13 \\times 256$ | 0 | $0.17$ | $13 \\times 13 \\times 256 \\times 10 = 4.3M$ | - |\n",
    "| **Conv3** | Convolution | $13 \\times 13 \\times 256$ | $3 \\times 3$ | 384 | 1 | 1 (same) | $13 \\times 13 \\times 384$ | $885{,}120$ | $13 \\times 13 \\times 384 \\times 4 = 0.25$ | $13 \\times 13 \\times 384 \\times (3 \\times 3 \\times 256) = 149.5M$ | ReLU |\n",
    "| **Conv4** | Convolution | $13 \\times 13 \\times 384$ | $3 \\times 3$ | 384 | 1 | 1 (same) | $13 \\times 13 \\times 384$ | $1{,}327{,}488$ | $0.25$ | $13 \\times 13 \\times 384 \\times (3 \\times 3 \\times 384) = 224.2M$ | ReLU |\n",
    "| **Conv5** | Convolution | $13 \\times 13 \\times 384$ | $3 \\times 3$ | 256 | 1 | 1 (same) | $13 \\times 13 \\times 256$ | $884{,}992$ | $0.17$ | $13 \\times 13 \\times 256 \\times (3 \\times 3 \\times 384) = 149.5M$ | ReLU |\n",
    "| **Pool3** | Max Pooling | $13 \\times 13 \\times 256$ | $3 \\times 3$ | - | 2 | 0 | $6 \\times 6 \\times 256$ | 0 | $6 \\times 6 \\times 256 \\times 4 = 0.04$ | $6 \\times 6 \\times 256 \\times 9 = 0.8M$ | - |\n",
    "| **Flatten** | Flatten | $6 \\times 6 \\times 256$ | - | - | - | - | $9{,}216$ | 0 | $9{,}216 \\times 4 = 0.04$ | 0 | - |\n",
    "| **FC1** | Fully Connected | $9{,}216$ | - | 4096 | - | - | $4{,}096$ | $37{,}752{,}832$ | $4{,}096 \\times 4 = 0.016$ | $2 \\times 9{,}216 \\times 4{,}096 = 75.5M$ | ReLU + Dropout (0.5) |\n",
    "| **FC2** | Fully Connected | $4{,}096$ | - | 4096 | - | - | $4{,}096$ | $16{,}781{,}312$ | $0.016$ | $2 \\times 4{,}096 \\times 4{,}096 = 33.6M$ | ReLU + Dropout (0.5) |\n",
    "| **FC3** | Fully Connected | $4{,}096$ | - | 1000 | - | - | $1{,}000$ | $4{,}097{,}000$ | $1{,}000 \\times 4 = 0.004$ | $2 \\times 4{,}096 \\times 1{,}000 = 8.2M$ | Softmax |\n",
    "| **Output** | Softmax | $1{,}000$ | - | - | - | - | $1{,}000$ | 0 | $0.004$ | $1{,}000 \\times 5 = 0.005M$ | - |\n",
    "| | | | | | | | **TOTAL** | **61,378,344** | **~3.6 MB** | **~1.22 GFLOPS** | |\n",
    "\n",
    "\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "| **Choice** | **Rationale** | **Impact** |\n",
    "|------------|---------------|------------|\n",
    "| **Large first kernel (11×11)** | Capture large receptive field early | Extract diverse low-level features |\n",
    "| **Decreasing kernel sizes** | 11×11 → 5×5 → 3×3 | Balance computation and feature extraction |\n",
    "| **Overlapping pooling** | Pool 3×3, stride 2 (not 2×2, stride 2) | Slight reduction in overfitting |\n",
    "| **Deep FC layers (4096 units)** | High capacity for classification | Enables complex decision boundaries |\n",
    "| **Dropout in FC only** | Conv layers have fewer parameters | Regularization where needed most |\n",
    "| **No dropout in conv layers** | Conv layers less prone to overfitting | Retain feature extraction capacity |\n",
    "\n",
    "### Limitations and Modern Improvements\n",
    "\n",
    "| **AlexNet Feature** | **Limitation** | **Modern Solution** |\n",
    "|---------------------|----------------|---------------------|\n",
    "| **Local Response Normalization (LRN)** | Expensive, marginal benefit | Batch Normalization (more effective) |\n",
    "| **Large FC layers** | 95% of parameters, overfitting | Global Average Pooling (GAP) |\n",
    "| **Manual learning rate schedule** | Requires monitoring | Adaptive optimizers (Adam, AdamW) |\n",
    "| **Fixed input size (227×227)** | Less flexible | Fully convolutional networks |\n",
    "| **Two-GPU split** | Complex implementation | Better multi-GPU frameworks |\n",
    "| **Large initial kernels (11×11)** | Expensive computation | Smaller kernels (3×3) stacked |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3a1ed",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1f799",
   "metadata": {},
   "source": [
    "The Primary goal of this network was **to show how depth affects performance**\n",
    "\n",
    "**Design Rules**\n",
    "1. **Simplicity** \n",
    "   - Convolution filters: 3x3, s=1, p=1\n",
    "   - Max pools are: 2x2, s=2\n",
    "   - After pooling, double number of channels\n",
    "2. **Homogeneity** - Consistent design pattern.\n",
    "3. **Small Kernels** - Remove large Kernels with stacks of 3x3 Kernels\n",
    "4. **Systematic Depth** - By consisten design pattern depth is simple to increase and measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a6295",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/VGG.png\" width=\"195\"/>\n",
    "<img src=\"../images/chap8/VGGStruct.png\" width=\"710\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b216383",
   "metadata": {},
   "source": [
    "### Measuring stacked Kernels vs. Large Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1419ea",
   "metadata": {},
   "source": [
    "**Receptive Field**\n",
    "\n",
    "|Configuration | Receptive Field | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | 7x7 | $(7-1)\\cdot1 + 1$ |\n",
    "|One 5x5 Conv  | 5x5 | $(5-1)\\cdot1 + 1$ | \n",
    "|Two 3x3 Conv  | 5x5 | $((3-1)\\cdot 1 + (3-1)\\cdot 1) +1$ | \n",
    "|Three 3x3 Conv| 7x7 | $((3-1)\\cdot 1 + (3-1)\\cdot 1 + (3-1)\\cdot 1) + 1$\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "Given $C \\to C$ Channels\n",
    "\n",
    "|Configuration | Parameters | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | $49C^2$ | $C \\times C \\times 7 \\times 7$ |\n",
    "|One 5x5 Conv  | $25C^2$ | $C \\times C \\times 5 \\times 5$ | \n",
    "|Two 3x3 Conv  | $18C^2$| $2 \\times (C \\times C \\times 3 \\times 3)$ | \n",
    "|Three 3x3 Conv| $27C^2$ | $3 \\times (C \\times C \\times 3 \\times 3)$ | \n",
    "\n",
    "**Non-Linearity (Expressiveness)**\n",
    "\n",
    "|Configuration | Parameters |\n",
    "|--------------|-----------------|\n",
    "|One 7x7 Conv  | 1 ReLU |\n",
    "|One 5x5 Conv  | 1 ReLU |\n",
    "|Two 3x3 Conv  | 2 ReLUs|\n",
    "|Three 3x3 Conv| 13 ReLUs |\n",
    "\n",
    "**Computational Costs (LFOPs)**\n",
    "\n",
    "Given spatial Dimensions $H \\times W$ and $C \\to C$ Channels\n",
    "\n",
    "|Configuration | FLOPs | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | $98C^2HW$ | $H \\times W \\times C \\times (7 \\times 7 \\times C \\times 2)$ |\n",
    "|One 5x5 Conv  | $50C^2HW$ | $H \\times W \\times C \\times (5 \\times 5 \\times C \\times 2)$ | \n",
    "|Two 3x3 Conv  | $36C^2HW$ | $2 \\times (H \\times W \\times C \\times (3 \\times 3 \\times C \\times 2))$ | \n",
    "|Three 3x3 Conv| $54C^2HW$ | $3 \\times (H \\times W \\times C \\times (3 \\times 3 \\times C \\times 2))$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1c5a",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/VGGstats.png\" width=\"595\"/>\n",
    "</div>\n",
    "\n",
    "**VGG's Advantages: Stacked Small Kernels Win**\n",
    "\n",
    "Compared to large kernels (5×5, 7×7), VGG's stacked 3×3 convolutions provide:\n",
    "\n",
    "| **Metric** | **Advantage** | **Impact** |\n",
    "|------------|---------------|------------|\n",
    "| **Receptive Field** | Same coverage (3×3×3 = 7×7) | Equivalent spatial context |\n",
    "| **Parameters** | 45% fewer (27C² vs 49C²) | More efficient learning |\n",
    "| **Expressiveness** | 3× more ReLU activations | Better feature representations |\n",
    "| **Computation** | 45% fewer FLOPs | Faster training and inference |\n",
    "| **Accuracy** | 7.3% top-5 error | State-of-the-art in 2014 |\n",
    "\n",
    "**VGG proved that depth + simplicity beats complexity.**\n",
    "\n",
    "---\n",
    "\n",
    "**VGG's Critical Weakness: Computational Inefficiency**\n",
    "\n",
    "Despite its success, VGG has a major flaw:\n",
    "\n",
    "| **Problem** | **Numbers** | **Issue** |\n",
    "|-------------|-------------|-----------|\n",
    "| **Memory consumption** | ~140M parameters (VGG-16) | Huge model size |\n",
    "| **FLOPs per image** | ~15.5 billion operations | Very slow inference |\n",
    "| **FC layer dominance** | 90% of parameters in FC layers | Inefficient parameter use |\n",
    "| **Limited depth scaling** | Difficult to go beyond 19 layers | Diminishing returns |\n",
    "\n",
    "**The Question:** Can we build deeper, more accurate networks **without** exploding computation?\n",
    "\n",
    "**The Answer:** GoogleNet (Inception) introduces **multi-scale feature extraction** and **1×1 convolutions** to dramatically reduce parameters while maintaining (or improving) accuracy—achieving similar performance with **12× fewer parameters** than VGG.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
