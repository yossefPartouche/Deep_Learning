{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e479e5b",
   "metadata": {},
   "source": [
    "# CNN Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34abc5",
   "metadata": {},
   "source": [
    "In this section investigate the CNN Architectures that were created for image classification specifically: \n",
    "- AlexNet\n",
    "- VGGNet \n",
    "- GoogleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2605a",
   "metadata": {},
   "source": [
    "## The ImageNet Classification Challenge\n",
    "\n",
    "Given 1,431,167 Images with human labels, with 1000 Object Classes, develop a model that's able to produce that highest accuracy rate of classification with top 5 Error evaluation. \n",
    "  \n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/ImageNet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "### Pre-AlexNet\n",
    "\n",
    "The main models that were used were Shallow NN, or Classical ML models where feature extraction was manually done. \n",
    "The best models during this period were obtaining 28-25% Error Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d42aaa",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "Suddenly AlexNet (developed by Ilya Sutskever, Geoffrey Hinton and Alex Krizhevsky) was the first model that was able to significantly reduce the error rates by roughly 10%, using Deep Neural network.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/AlexNet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "| **Innovation** | **Description** | **Impact** |\n",
    "|----------------|-----------------|------------|\n",
    "| **Deep Architecture** | 8 learned layers (5 conv + 3 FC) | First successful very deep CNN |\n",
    "| **ReLU Activation** | Used ReLU instead of tanh/sigmoid | 6Ã— faster training, no vanishing gradients |\n",
    "| **Dropout** | Applied 0.5 dropout in FC layers | Reduced overfitting significantly |\n",
    "| **Data Augmentation** | Random crops, flips, color jittering | Increased training data diversity |\n",
    "| **GPU Training** | Trained on 2 GTX 580 GPUs | Made large-scale training feasible |\n",
    "| **Local Response Normalization** | Normalization across channels | Later replaced by Batch Normalization |\n",
    "| **Overlapping Pooling** | Pool size 3Ã—3, stride 2 | Slight accuracy improvement |\n",
    "\n",
    "### Complete Architecture\n",
    "\n",
    "**Input**: $227 \\times 227 \\times 3$ RGB image (originally stated as 224Ã—224, but 227Ã—227 is correct for the dimensions to work out)\n",
    "\n",
    "**Assumptions**: \n",
    "- Batch size = 1 (single image)\n",
    "- 32-bit floating point (4 bytes per value)\n",
    "- FLOPs calculated as multiply-add operations\n",
    "\n",
    "| **Layer** | **Type** | **Input Shape** | **Kernel Size** | **Filters/Units** | **Stride** | **Padding** | **Output Shape** | **Parameters** | **Memory (MB)** | **FLOPs** | **Activation** |\n",
    "|-----------|----------|-----------------|-----------------|-------------------|------------|-------------|------------------|----------------|-----------------|-----------|----------------|\n",
    "| **Input** | Input | - | - | - | - | - | $227 \\times 227 \\times 3$ | 0 | $227 \\times 227 \\times 3 \\times 4 = 0.59$ | 0 | - |\n",
    "| **Conv1** | Convolution | $227 \\times 227 \\times 3$ | $11 \\times 11$ | 96 | 4 | 0 (valid) | $55 \\times 55 \\times 96$ | $34{,}944$ | $55 \\times 55 \\times 96 \\times 4 = 1.11$ | $55 \\times 55 \\times 96 \\times (11 \\times 11 \\times 3) = 105.4M$ | ReLU |\n",
    "| **Pool1** | Max Pooling | $55 \\times 55 \\times 96$ | $3 \\times 3$ | - | 2 | 0 | $27 \\times 27 \\times 96$ | 0 | $27 \\times 27 \\times 96 \\times 4 = 0.27$ | $27 \\times 27 \\times 96 \\times 9 = 6.3M$ | - |\n",
    "| **LRN1** | Local Response Norm | $27 \\times 27 \\times 96$ | - | - | - | - | $27 \\times 27 \\times 96$ | 0 | $0.27$ | $27 \\times 27 \\times 96 \\times 10 = 7.0M$ | - |\n",
    "| **Conv2** | Convolution | $27 \\times 27 \\times 96$ | $5 \\times 5$ | 256 | 1 | 2 (same) | $27 \\times 27 \\times 256$ | $614{,}656$ | $27 \\times 27 \\times 256 \\times 4 = 0.72$ | $27 \\times 27 \\times 256 \\times (5 \\times 5 \\times 96) = 448.1M$ | ReLU |\n",
    "| **Pool2** | Max Pooling | $27 \\times 27 \\times 256$ | $3 \\times 3$ | - | 2 | 0 | $13 \\times 13 \\times 256$ | 0 | $13 \\times 13 \\times 256 \\times 4 = 0.17$ | $13 \\times 13 \\times 256 \\times 9 = 3.9M$ | - |\n",
    "| **LRN2** | Local Response Norm | $13 \\times 13 \\times 256$ | - | - | - | - | $13 \\times 13 \\times 256$ | 0 | $0.17$ | $13 \\times 13 \\times 256 \\times 10 = 4.3M$ | - |\n",
    "| **Conv3** | Convolution | $13 \\times 13 \\times 256$ | $3 \\times 3$ | 384 | 1 | 1 (same) | $13 \\times 13 \\times 384$ | $885{,}120$ | $13 \\times 13 \\times 384 \\times 4 = 0.25$ | $13 \\times 13 \\times 384 \\times (3 \\times 3 \\times 256) = 149.5M$ | ReLU |\n",
    "| **Conv4** | Convolution | $13 \\times 13 \\times 384$ | $3 \\times 3$ | 384 | 1 | 1 (same) | $13 \\times 13 \\times 384$ | $1{,}327{,}488$ | $0.25$ | $13 \\times 13 \\times 384 \\times (3 \\times 3 \\times 384) = 224.2M$ | ReLU |\n",
    "| **Conv5** | Convolution | $13 \\times 13 \\times 384$ | $3 \\times 3$ | 256 | 1 | 1 (same) | $13 \\times 13 \\times 256$ | $884{,}992$ | $0.17$ | $13 \\times 13 \\times 256 \\times (3 \\times 3 \\times 384) = 149.5M$ | ReLU |\n",
    "| **Pool3** | Max Pooling | $13 \\times 13 \\times 256$ | $3 \\times 3$ | - | 2 | 0 | $6 \\times 6 \\times 256$ | 0 | $6 \\times 6 \\times 256 \\times 4 = 0.04$ | $6 \\times 6 \\times 256 \\times 9 = 0.8M$ | - |\n",
    "| **Flatten** | Flatten | $6 \\times 6 \\times 256$ | - | - | - | - | $9{,}216$ | 0 | $9{,}216 \\times 4 = 0.04$ | 0 | - |\n",
    "| **FC1** | Fully Connected | $9{,}216$ | - | 4096 | - | - | $4{,}096$ | $37{,}752{,}832$ | $4{,}096 \\times 4 = 0.016$ | $2 \\times 9{,}216 \\times 4{,}096 = 75.5M$ | ReLU + Dropout (0.5) |\n",
    "| **FC2** | Fully Connected | $4{,}096$ | - | 4096 | - | - | $4{,}096$ | $16{,}781{,}312$ | $0.016$ | $2 \\times 4{,}096 \\times 4{,}096 = 33.6M$ | ReLU + Dropout (0.5) |\n",
    "| **FC3** | Fully Connected | $4{,}096$ | - | 1000 | - | - | $1{,}000$ | $4{,}097{,}000$ | $1{,}000 \\times 4 = 0.004$ | $2 \\times 4{,}096 \\times 1{,}000 = 8.2M$ | Softmax |\n",
    "| **Output** | Softmax | $1{,}000$ | - | - | - | - | $1{,}000$ | 0 | $0.004$ | $1{,}000 \\times 5 = 0.005M$ | - |\n",
    "| | | | | | | | **TOTAL** | **61,378,344** | **~3.6 MB** | **~1.22 GFLOPS** | |\n",
    "\n",
    "\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "| **Choice** | **Rationale** | **Impact** |\n",
    "|------------|---------------|------------|\n",
    "| **Large first kernel (11Ã—11)** | Capture large receptive field early | Extract diverse low-level features |\n",
    "| **Decreasing kernel sizes** | 11Ã—11 â†’ 5Ã—5 â†’ 3Ã—3 | Balance computation and feature extraction |\n",
    "| **Overlapping pooling** | Pool 3Ã—3, stride 2 (not 2Ã—2, stride 2) | Slight reduction in overfitting |\n",
    "| **Deep FC layers (4096 units)** | High capacity for classification | Enables complex decision boundaries |\n",
    "| **Dropout in FC only** | Conv layers have fewer parameters | Regularization where needed most |\n",
    "| **No dropout in conv layers** | Conv layers less prone to overfitting | Retain feature extraction capacity |\n",
    "\n",
    "### Limitations and Modern Improvements\n",
    "\n",
    "| **AlexNet Feature** | **Limitation** | **Modern Solution** |\n",
    "|---------------------|----------------|---------------------|\n",
    "| **Local Response Normalization (LRN)** | Expensive, marginal benefit | Batch Normalization (more effective) |\n",
    "| **Large FC layers** | 95% of parameters, overfitting | Global Average Pooling (GAP) |\n",
    "| **Manual learning rate schedule** | Requires monitoring | Adaptive optimizers (Adam, AdamW) |\n",
    "| **Fixed input size (227Ã—227)** | Less flexible | Fully convolutional networks |\n",
    "| **Two-GPU split** | Complex implementation | Better multi-GPU frameworks |\n",
    "| **Large initial kernels (11Ã—11)** | Expensive computation | Smaller kernels (3Ã—3) stacked |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3a1ed",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1f799",
   "metadata": {},
   "source": [
    "The Primary goal of this network was **to show how depth affects performance**\n",
    "\n",
    "**Design Rules**\n",
    "1. **Simplicity** \n",
    "   - Convolution filters: 3x3, s=1, p=1\n",
    "   - Max pools are: 2x2, s=2\n",
    "   - After pooling, double number of channels\n",
    "2. **Homogeneity** - Consistent design pattern.\n",
    "3. **Small Kernels** - Remove large Kernels with stacks of 3x3 Kernels\n",
    "4. **Systematic Depth** - By consisten design pattern depth is simple to increase and measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a6295",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/VGG.png\" width=\"195\"/>\n",
    "<img src=\"../images/chap8/VGGStruct.png\" width=\"710\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b216383",
   "metadata": {},
   "source": [
    "### Measuring stacked Kernels vs. Large Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1419ea",
   "metadata": {},
   "source": [
    "**Receptive Field**\n",
    "\n",
    "|Configuration | Receptive Field | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | 7x7 | $(7-1)\\cdot1 + 1$ |\n",
    "|One 5x5 Conv  | 5x5 | $(5-1)\\cdot1 + 1$ | \n",
    "|Two 3x3 Conv  | 5x5 | $((3-1)\\cdot 1 + (3-1)\\cdot 1) +1$ | \n",
    "|Three 3x3 Conv| 7x7 | $((3-1)\\cdot 1 + (3-1)\\cdot 1 + (3-1)\\cdot 1) + 1$\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "Given $C \\to C$ Channels\n",
    "\n",
    "|Configuration | Parameters | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | $49C^2$ | $C \\times C \\times 7 \\times 7$ |\n",
    "|One 5x5 Conv  | $25C^2$ | $C \\times C \\times 5 \\times 5$ | \n",
    "|Two 3x3 Conv  | $18C^2$| $2 \\times (C \\times C \\times 3 \\times 3)$ | \n",
    "|Three 3x3 Conv| $27C^2$ | $3 \\times (C \\times C \\times 3 \\times 3)$ | \n",
    "\n",
    "**Non-Linearity (Expressiveness)**\n",
    "\n",
    "|Configuration | Parameters |\n",
    "|--------------|-----------------|\n",
    "|One 7x7 Conv  | 1 ReLU |\n",
    "|One 5x5 Conv  | 1 ReLU |\n",
    "|Two 3x3 Conv  | 2 ReLUs|\n",
    "|Three 3x3 Conv| 13 ReLUs |\n",
    "\n",
    "**Computational Costs (LFOPs)**\n",
    "\n",
    "Given spatial Dimensions $H \\times W$ and $C \\to C$ Channels\n",
    "\n",
    "|Configuration | FLOPs | Calculation | \n",
    "|--------------|-----------------|-------------|\n",
    "|One 7x7 Conv  | $98C^2HW$ | $H \\times W \\times C \\times (7 \\times 7 \\times C \\times 2)$ |\n",
    "|One 5x5 Conv  | $50C^2HW$ | $H \\times W \\times C \\times (5 \\times 5 \\times C \\times 2)$ | \n",
    "|Two 3x3 Conv  | $36C^2HW$ | $2 \\times (H \\times W \\times C \\times (3 \\times 3 \\times C \\times 2))$ | \n",
    "|Three 3x3 Conv| $54C^2HW$ | $3 \\times (H \\times W \\times C \\times (3 \\times 3 \\times C \\times 2))$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1c5a",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/VGGstats.png\" width=\"795\"/>\n",
    "</div>\n",
    "\n",
    "**VGG's Advantages: Stacked Small Kernels Win**\n",
    "\n",
    "Compared to large kernels (5Ã—5, 7Ã—7), VGG's stacked 3Ã—3 convolutions provide:\n",
    "\n",
    "| **Metric** | **Advantage** | **Impact** |\n",
    "|------------|---------------|------------|\n",
    "| **Receptive Field** | Same coverage (3Ã—3Ã—3 = 7Ã—7) | Equivalent spatial context |\n",
    "| **Parameters** | 45% fewer (27CÂ² vs 49CÂ²) | More efficient learning |\n",
    "| **Expressiveness** | 3Ã— more ReLU activations | Better feature representations |\n",
    "| **Computation** | 45% fewer FLOPs | Faster training and inference |\n",
    "| **Accuracy** | 7.3% top-5 error | State-of-the-art in 2014 |\n",
    "\n",
    "**VGG proved that depth + simplicity beats complexity.**\n",
    "\n",
    "---\n",
    "\n",
    "**VGG's Critical Weakness: Computational Inefficiency**\n",
    "\n",
    "Despite its success, VGG has a major flaw:\n",
    "\n",
    "| **Problem** | **Numbers** | **Issue** |\n",
    "|-------------|-------------|-----------|\n",
    "| **Memory consumption** | ~140M parameters (VGG-16) | Huge model size |\n",
    "| **FLOPs per image** | ~15.5 billion operations | Very slow inference |\n",
    "| **FC layer dominance** | 90% of parameters in FC layers | Inefficient parameter use |\n",
    "| **Limited depth scaling** | Difficult to go beyond 19 layers | Diminishing returns |\n",
    "\n",
    "**The Question:** Can we build deeper, more accurate networks **without** exploding computation?\n",
    "\n",
    "**The Answer:** GoogleNet (Inception) introduces **multi-scale feature extraction** and **1Ã—1 convolutions** to dramatically reduce parameters while maintaining (or improving) accuracyâ€”achieving similar performance with **12Ã— fewer parameters** than VGG.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606f902",
   "metadata": {},
   "source": [
    "## GoogleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81481687",
   "metadata": {},
   "source": [
    "Inception v1, 2014 was designed to address VGG's inefficiency with three core objectives: \n",
    "1. **Reduce Computational Cost**\n",
    "    - VGG-16 has ~140 parameters and 15.5B FLOPs\n",
    "    - GoogleNet has ~7M parameters\n",
    "2. **Multi-Scale Feature Extraction**\n",
    "    - Single Kernel size captures only one scale\n",
    "    - Process features at multiple scales **Simulatneously**\n",
    "3. **Deeper Networks without Exploding Parameters**\n",
    "    - VGG's depth limited by parameters growth\n",
    "    - GooglesNet Bottleneck architecture enable deeper network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afc1d5",
   "metadata": {},
   "source": [
    "### The Inception Module: Core Building Block\n",
    "\n",
    "Before examining the full GoogleNet architecture, let's understand the **Inception module** â€” the fundamental innovation that makes GoogleNet efficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### **The Problem: Which Kernel Size to Choose?**\n",
    "\n",
    "Traditional CNNs force you to choose **one** kernel size per layer:\n",
    "- **Large kernels (5Ã—5, 7Ã—7)**: Capture global patterns but expensive\n",
    "- **Small kernels (3Ã—3)**: Efficient but may miss larger patterns\n",
    "- **1Ã—1 kernels**: Fast but limited receptive field\n",
    "\n",
    "**Question:** Why choose when you can use **all of them simultaneously**?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Naive Inception Module**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/NaiveIncept.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Idea:** Apply multiple operations in parallel, then concatenate:\n",
    "\n",
    "| **Operation** | **Purpose** | **Output Channels** |\n",
    "|---------------|-------------|---------------------|\n",
    "| **1Ã—1 conv** | Capture point-wise features | 64 |\n",
    "| **3Ã—3 conv** | Capture local patterns | 128 |\n",
    "| **5Ã—5 conv** | Capture larger patterns | 32 |\n",
    "| **3Ã—3 max pool** | Preserve spatial information | (same as input) |\n",
    "\n",
    "**Problem:** This is **computationally expensive**!\n",
    "\n",
    "For input $28 \\times 28 \\times 256$:\n",
    "- 5Ã—5 conv alone: $28 \\times 28 \\times 32 \\times (5 \\times 5 \\times 256 \\times 2) = 406M$ FLOPs\n",
    "\n",
    "---\n",
    "\n",
    "#### **Inception Module with Dimensionality Reduction**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/Inception.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Key Innovation:** Use **1Ã—1 convolutions as bottlenecks** before expensive operations.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why 1Ã—1 Convolutions Work**\n",
    "\n",
    "**1Ã—1 convolutions** perform **dimensionality reduction**:\n",
    "\n",
    "| **Step** | **Dimensions** | **Purpose** |\n",
    "|----------|---------------|-------------|\n",
    "| Input | $28 \\times 28 \\times 256$ | High-dimensional feature map |\n",
    "| 1Ã—1 conv | $28 \\times 28 \\times 16$ | **Compress** channels (256 â†’ 16) |\n",
    "| 5Ã—5 conv | $28 \\times 28 \\times 32$ | Process with reduced input depth |\n",
    "\n",
    "**Computational Savings:**\n",
    "\n",
    "| **Method** | **FLOPs** | **Calculation** |\n",
    "|------------|-----------|-----------------|\n",
    "| **Direct 5Ã—5 conv** | 406M | $28 \\times 28 \\times 32 \\times (5 \\times 5 \\times 256 \\times 2)$ |\n",
    "| **With 1Ã—1 bottleneck** | 14M | $(28^2 \\times 16 \\times 256 \\times 2) + (28^2 \\times 32 \\times 25 \\times 16 \\times 2)$ |\n",
    "| **Reduction** | **96.6% fewer FLOPs** | 29Ã— more efficient! |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Multi-Scale Feature Extraction**\n",
    "\n",
    "Each branch captures **different scales**:\n",
    "\n",
    "| **Branch** | **Receptive Field** | **What It Captures** |\n",
    "|------------|---------------------|----------------------|\n",
    "| 1Ã—1 conv | 1Ã—1 | Point-wise cross-channel patterns |\n",
    "| 3Ã—3 conv | 3Ã—3 | Local spatial patterns |\n",
    "| 5Ã—5 conv | 5Ã—5 | Larger spatial patterns |\n",
    "| Max pool + 1Ã—1 | Preserves spatial info | Maintains strong activations |\n",
    "\n",
    "**Result:** The network learns **which scale is most useful** for each task through training.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Advantages**\n",
    "\n",
    "| **Advantage** | **How It's Achieved** |\n",
    "|---------------|----------------------|\n",
    "| **Multi-scale processing** | Parallel branches with different kernel sizes |\n",
    "| **Computational efficiency** | 1Ã—1 bottleneck convolutions (96%+ reduction) |\n",
    "| **Feature diversity** | Concatenate outputs from all branches |\n",
    "| **Network depth** | Can stack many modules without exploding cost |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** See how GoogleNet stacks 9 of these Inception modules to create a 22-layer network with only 7M parameters! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb59d6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/FullGNet.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05353b",
   "metadata": {},
   "source": [
    "### GoogleNet Full Architecture\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/FullGNet.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **ðŸ”µ Stem: Initial Layers (Blue)**\n",
    "\n",
    "**Purpose:** Aggressive downsampling and feature extraction from raw images.\n",
    "\n",
    "**Structure:**\n",
    "- Conv 7Ã—7, stride=2 â†’ Pool 3Ã—3, stride=2\n",
    "- Conv 1Ã—1 â†’ Conv 3Ã—3 â†’ Pool 3Ã—3, stride=2\n",
    "- Reduces $224 \\times 224 \\times 3$ â†’ $28 \\times 28 \\times 192$\n",
    "\n",
    "**Why?**\n",
    "- **Large kernels (7Ã—7)**: Capture low-level features (edges, textures)\n",
    "- **Aggressive pooling**: Reduce spatial dimensions quickly (224â†’28)\n",
    "- **Traditional design**: Similar to AlexNet/VGG before Inception modules begin\n",
    "\n",
    "---\n",
    "\n",
    "#### **ðŸ”´ Inception Body: 9 Inception Modules (Maroon/Red)**\n",
    "\n",
    "**Purpose:** Multi-scale feature extraction with computational efficiency.\n",
    "\n",
    "**Structure:**\n",
    "- **9 stacked Inception modules** organized in groups\n",
    "- Spatial dimensions: $28 \\times 28 \\to 14 \\times 14 \\to 7 \\times 7$\n",
    "- Max pooling between module groups for downsampling\n",
    "\n",
    "**Why?**\n",
    "- **Multi-scale processing**: Each module captures 1Ã—1, 3Ã—3, 5Ã—5 features simultaneously\n",
    "- **Bottleneck architecture**: 1Ã—1 convs reduce computation by ~85%\n",
    "- **Deep without explosion**: 22 layers total, only 7M parameters\n",
    "\n",
    "---\n",
    "\n",
    "#### **ðŸŸ£ Auxiliary Classifiers (Purple)**\n",
    "\n",
    "**Purpose:** Combat vanishing gradients during training in deep networks.\n",
    "\n",
    "**Structure:**\n",
    "- Two auxiliary branches attached to **intermediate layers**\n",
    "- Each contains: AvgPool â†’ 1Ã—1 Conv â†’ FC â†’ Softmax\n",
    "- Weighted loss (0.3Ã—) added to main loss during training\n",
    "\n",
    "**Why?**\n",
    "- **Gradient injection**: Provide direct gradient signal to middle layers\n",
    "- **Regularization**: Act as implicit regularization (similar to dropout)\n",
    "- **Discarded at inference**: Only used during training, removed for deployment\n",
    "\n",
    "---\n",
    "\n",
    "#### **ðŸŸ¢ Final Classifier (Green)**\n",
    "\n",
    "**Purpose:** Global aggregation and classification.\n",
    "\n",
    "**Structure:**\n",
    "- Global Average Pooling (GAP): $7 \\times 7 \\times 1024 \\to 1 \\times 1 \\times 1024$\n",
    "- Dropout (0.4)\n",
    "- Fully Connected: $1024 \\to 1000$ classes\n",
    "- Softmax activation\n",
    "\n",
    "**Why?**\n",
    "- **GAP replaces large FC layers**: Reduces parameters dramatically (no 4096-unit layers like VGG)\n",
    "- **Translation invariance**: GAP makes network robust to input shifts\n",
    "- **Dropout**: Prevents overfitting in final classification layer\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Architectural Innovations**\n",
    "\n",
    "| **Section** | **Innovation** | **Benefit** |\n",
    "|-------------|---------------|-------------|\n",
    "| **Stem** | Traditional conv layers | Efficient initial downsampling |\n",
    "| **Inception Body** | Parallel multi-scale convolutions | Rich features, low cost |\n",
    "| **Auxiliary Classifiers** | Mid-network supervision | Better gradient flow |\n",
    "| **Final Classifier** | Global Average Pooling | Minimal parameters, strong generalization |\n",
    "\n",
    "### **Key Results**\n",
    "\n",
    "- **22 layers deep** with only **7M parameters** (VGG-16: 138M)\n",
    "- **~1.5B FLOPs** per image (VGG-16: 15.5B)\n",
    "- **6.7% top-5 error** on ImageNet (better than VGG's 7.3%)\n",
    "- **12Ã— fewer parameters** than VGG with superior accuracy\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
