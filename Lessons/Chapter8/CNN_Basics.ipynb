{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31999b93",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Limitations of MLP](#limitations-of-mlp)\n",
    "2. [Invariance and Equivariance](#invariance-and-equivariance)\n",
    "3. [Convolution Operation](#convolution-operation)\n",
    "4. [Padding Strategies](#padding-strategies)\n",
    "5. [Convolution Hyperparameters: Stride, Kernel Size, and Dilation](#convolution-hyperparameters-stride-kernel-size-and-dilation)\n",
    "6. [Receptive Field](#receptive-field)\n",
    "7. [Channels in Convolutional Layers](#channels-in-convolutional-layers)\n",
    "8. [Activation Functions and Bias in CNNs](#activation-functions-and-bias-in-cnns)\n",
    "9. [Pooling Layers](#pooling-layers)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923beb8",
   "metadata": {},
   "source": [
    "## Limitations of MLP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a940b",
   "metadata": {},
   "source": [
    "\n",
    "**Problem 1** \n",
    "\n",
    "Suppose we train a network using $(3 \\times 200 \\times 200)$ an RGB image. <br>\n",
    "We'd convert this to a single vector of size $120, 000$, now let's consider passing it through a single $\\text{Fully connected Layer}$ where we have $1000$ hidden units.<br> We'd need $120 \\text{ million parameters!!}$ This isn't feasible.\n",
    "\n",
    "**Problem 2**\n",
    "\n",
    "Note that an image contain values which describes: \n",
    "1. Small Scale - Color pixels and brightness\n",
    "2. Intermidiate Scale - Basic shapes (line/strokes of colors)\n",
    "3. Mid Scale - Basic Image context (face/parts of object) \n",
    "4. Large Scale - Larger collection of objects\n",
    "   \n",
    "The linear operations applied in a Dense NN will distort/corrupt this information before learning anything valuable, so we need operations that don't have these effects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9d2f5",
   "metadata": {},
   "source": [
    "## Invariance and equivariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac180ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We first address the second problem mathematically. \n",
    "\n",
    "$\\text{Let } f \\text{ be a function } f: (3 \\times m \\times n) \\to (3 \\times m \\times n) \\text{ which takes an image and outputs an image, this is said to be } invariant \\text{ to a transformation } t[x] \\text{ if:}$ $$\\boxed{f[t[x]] = f[x]}$$\n",
    "\n",
    "In terms of images this means: The network $f[x]$ should identify an image as containing the same object if it's been translated, rotated flipped or warped.\n",
    "\n",
    "$\\text{We say that the function } f \\text{ is also } equivariant \\text{ or } covariant \\text{ to a transformation } t[x] \\text{ if } t[x] \\text{ if:}$ $$\\boxed{f[t[x]] = t[f[x]]}$$\n",
    "\n",
    "In terms of images this means: If the image is translated, rotated or flipped, then the network $f[x]$ should return a segmentation that has been transformed in the same way\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7bce0",
   "metadata": {},
   "source": [
    "There a 2 main Layers invovled in a Convolution Network: \n",
    "\n",
    "1. $\\textbf{Convolutional Layers}$\n",
    "   - This invovles convolution operation which is $equivariant$ to $\\text{Translation}$\n",
    "2. $\\textbf{Pooling Layers}$\n",
    "   - This involves selecting/pooling layers of the transformed image which is $partially \\ invariant \\ to \\ translation$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e66bf",
   "metadata": {},
   "source": [
    "## Convolution Operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f84f10",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Aspect** | **1D Convolution** | **2D Convolution** |\n",
    "|------------|--------------------|--------------------|\n",
    "| **Simple Explanation** | A convolution is a **weighted sum** of neighboring input values, where the weights form a **kernel** (or filter). The kernel slides across the input, computing the weighted sum at each position. The number of weights in the kernel is called the **kernel size**. | A 2D convolution is a **weighted sum** of neighboring pixel values in a local region, where the weights form a 2D **kernel** (or filter). The kernel slides across the image (both horizontally and vertically), computing the weighted sum at each position. The kernel dimensions (e.g., $3 \\times 3$, $5 \\times 5$) define the **kernel size**. |\n",
    "| **Mathematical Formulation** | For a 1D input $\\mathbf{x} = [x_1, x_2, ..., x_n]$ and kernel $\\boldsymbol{\\omega} = [\\omega_1, \\omega_2, ..., \\omega_k]$ of size $k$, the convolution output at position $i$ is: $$z_i = \\sum_{j=0}^{k-1} \\omega_{j+1} \\cdot x_{i+j}$$ For kernel size 3 centered at position $i$: $$z_i = \\omega_1 x_{i-1} + \\omega_2 x_i + \\omega_3 x_{i+1}$$ | For a 2D input $\\mathbf{X} \\in \\mathbb{R}^{H \\times W}$ and kernel $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{k_h \\times k_w}$, the convolution output at position $(i, j)$ is: $$z_{i,j} = \\sum_{m=0}^{k_h-1} \\sum_{n=0}^{k_w-1} \\omega_{m,n} \\cdot x_{i+m, j+n}$$ For a $3 \\times 3$ kernel centered at $(i,j)$: $$z_{i,j} = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} \\omega_{m,n} \\cdot x_{i+m, j+n}$$ |\n",
    "| **Example** | **Input vector**: $\\mathbf{x} = [2, 5, 3, 7]$ <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (kernel size = 3) <br><br> **Computation** (with zero padding): <br> $z_1 = 1(0) + 0(2) + (-1)(5) = -5$ <br> $z_2 = 1(2) + 0(5) + (-1)(3) = -1$ <br> $z_3 = 1(5) + 0(3) + (-1)(7) = -2$ <br> $z_4 = 1(3) + 0(7) + (-1)(0) = 3$ <br><br> **Output vector**: $\\mathbf{z} = [-5, -1, -2, 3]$ | **Input matrix**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ <br> **Kernel**: $\\boldsymbol{\\Omega} = \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}$ (size $3 \\times 3$) <br><br> **Computation** (center position): <br> $z_{2,2} = 1(1) + 0(2) + (-1)(3) +$ <br> $\\quad\\quad\\quad 1(4) + 0(5) + (-1)(6) +$ <br> $\\quad\\quad\\quad 1(7) + 0(8) + (-1)(9)$ <br> $z_{2,2} = -6 - 6 - 6 = -12$ <br><br> **Output** (with valid padding): single value $z = -12$ |\n",
    "| **Visualization** | <img src=\"../images/chap8/conv1.png\" width=\"200\" /> | <img src=\"../images/chap8/convol2D1.png\" width=\"400\" /> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c166d6",
   "metadata": {},
   "source": [
    "## Padding Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d5fec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When applying convolutions, we need to handle boundaries where the kernel extends beyond the input. \n",
    "<br> Different **padding strategies** determine the output size and boundary behavior.\n",
    "\n",
    "| **Padding Type** | **Description** | **1D Example** | **2D Example** | **Visualization**|\n",
    "|------------------|-----------------|----------------|----------------|------------------|\n",
    "| **Valid Padding** | **No padding** is added.<br>The kernel only slides over valid positions where it fully overlaps the input.<br> This **reduces output size**. <br><br> **Output size**: $n_{out} = n_{in} - k + 1$ <br> where $n_{in}$ = input size, $k$ = kernel size | **Input**: $\\mathbf{x} = [2, 5, 3, 7]$ (size 4) <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (size 3) <br><br> **Valid positions**: 2 positions <br> $z_1 = 1(2) + 0(5) + (-1)(3) = -1$ <br> $z_2 = 1(5) + 0(3) + (-1)(7) = -2$ <br><br> **Output**: $\\mathbf{z} = [-1, -2]$ (size 2) | **Input**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ (size $3 \\times 3$) <br> **Kernel**: $3 \\times 3$ <br><br> **Valid position**: only center <br> $z_{1,1} = 1(1) + ... + (-1)(9)$ <br><br> **Output**: single value (size $1 \\times 1$) | <img src=\"../images/chap8/nopad.png\" width=\"300\" />|\n",
    "| **Same / Half Padding** | Add **zeros** around the input boundary so that **output size equals input size** (when stride = 1).<br> Most common in deep CNNs. <br><br> **Padding amount**: $p = \\lfloor k/2 \\rfloor$ <br> **Output size**: $n_{out} = n_{in}$ (same as input) | **Input**: $\\mathbf{x} = [2, 5, 3, 7]$ (size 4) <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (size 3) <br> **Padding**: $p = \\lfloor 3/2 \\rfloor = 1$ <br> **Padded**: $[0, 2, 5, 3, 7, 0]$ <br><br> $z_1 = 1(0) + 0(2) + (-1)(5) = -5$ <br> $z_2 = 1(2) + 0(5) + (-1)(3) = -1$ <br> $z_3 = 1(5) + 0(3) + (-1)(7) = -2$ <br> $z_4 = 1(3) + 0(7) + (-1)(0) = 3$ <br><br> **Output**: $\\mathbf{z} = [-5, -1, -2, 3]$ (size 4) | **Input**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ (size $3 \\times 3$) <br> **Kernel**: $3 \\times 3$ <br> **Padding**: $p = 1$ on all sides <br> **Padded**: $\\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 2 & 3 & 0 \\\\ 0 & 4 & 5 & 6 & 0 \\\\ 0 & 7 & 8 & 9 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$ <br><br> **Output**: $3 \\times 3$ matrix (same size) |<img src=\"../images/chap8/halfpad.png\" width=\"300\" />|\n",
    "| **Full Padding** | Add **maximum padding** with zeros so that every input element is visited by the kernel at least once.<br> The kernel can extend completely beyond the input on both sides.<br> This **increases output size**. <br><br> **Padding amount**: $p = k - 1$ <br> **Output size**: $n_{out} = n_{in} + k - 1$ | **Input**: $\\mathbf{x} = [2, 5, 3, 7]$ (size 4) <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (size 3) <br> **Padding**: $p = 3 - 1 = 2$ <br> **Padded**: $[0, 0, 2, 5, 3, 7, 0, 0]$ <br><br> $z_1 = 1(0) + 0(0) + (-1)(2) = -2$ <br> $z_2 = 1(0) + 0(2) + (-1)(5) = -5$ <br> $z_3 = 1(2) + 0(5) + (-1)(3) = -1$ <br> $z_4 = 1(5) + 0(3) + (-1)(7) = -2$ <br> $z_5 = 1(3) + 0(7) + (-1)(0) = 3$ <br> $z_6 = 1(7) + 0(0) + (-1)(0) = 7$ <br><br> **Output**: $\\mathbf{z} = [-2, -5, -1, -2, 3, 7]$ (size 6) | **Input**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ (size $3 \\times 3$) <br> **Kernel**: $3 \\times 3$ <br> **Padding**: $p = 2$ on all sides <br> **Padded**: $\\begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 2 & 3 & 0 & 0 \\\\ 0 & 0 & 4 & 5 & 6 & 0 & 0 \\\\ 0 & 0 & 7 & 8 & 9 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$ <br><br> **Output**: $5 \\times 5$ matrix |<img src=\"../images/chap8/fullpad.png\" width=\"300\" />|\n",
    "\n",
    "**Key Insights:**\n",
    "- **Valid**: No boundary artifacts, but loses spatial resolution → shrinking output\n",
    "- **Same/Half**: Maintains spatial dimensions → most common in CNNs (ResNet, VGG, etc.)\n",
    "- **Full**: Increases spatial dimensions → useful in transposed convolutions (upsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7a1de",
   "metadata": {},
   "source": [
    "## Convolution Hyperparameters: Stride, Kernel Size, and Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a03d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "These three hyperparameters control the **receptive field**, **output size**, and **computational efficiency** of convolutional layers.\n",
    "\n",
    "| **Parameter** | **Description** | **1D Example** | **2D Example** |\n",
    "|---------------|-----------------|----------------|----------------|\n",
    "| **Kernel Size** | The **spatial extent** of the filter/kernel. Determines how many neighboring values contribute to each output. <br>Can be determined from a single computation sequence <br> Common sizes: $3 \\times 3$, $5 \\times 5$, $7 \\times 7$ <br> Larger kernels = larger receptive field <br> **Output size** (valid padding): $n_{out} = n_{in} - k + 1$ | **Input**: $\\mathbf{x} = [1, 2, 3, 4, 5, 6]$ (size 6) <br><br> **Kernel size 3**: $\\boldsymbol{\\omega} = [1, 2, 1]$ <br> $z_1 = 1(1) + 2(2) + 1(3) = 8$ <br> $z_2 = 1(2) + 2(3) + 1(4) = 12$ <br> $z_3 = 1(3) + 2(4) + 1(5) = 16$ <br> $z_4 = 1(4) + 2(5) + 1(6) = 20$ <br> **Output**: $[8, 12, 16, 20]$ (size 4) <br><br> **Kernel size 5**: $\\boldsymbol{\\omega} = [1, 1, 1, 1, 1]$ <br> $z_1 = 1 + 2 + 3 + 4 + 5 = 15$ <br> $z_2 = 2 + 3 + 4 + 5 + 6 = 20$ <br> **Output**: $[15, 20]$ (size 2) | **Input**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 5 & 6 & 7 & 8 \\\\ 9 & 10 & 11 & 12 \\\\ 13 & 14 & 15 & 16 \\end{bmatrix}$ (size $4 \\times 4$) <br><br> **Kernel $2 \\times 2$**: <br> Output size: $(4-2+1) \\times (4-2+1) = 3 \\times 3$ <br><br> **Kernel $3 \\times 3$**: <br> Output size: $(4-3+1) \\times (4-3+1) = 2 \\times 2$ <br><br> **Kernel $4 \\times 4$**: <br> Output size: $(4-4+1) \\times (4-4+1) = 1 \\times 1$ <br><br> Larger kernel = smaller output, bigger receptive field |\n",
    "| **Stride** | The **step size** by which the kernel moves across the input. Controls downsampling. <br>Can only be determined from at least two computation sequences or be provided<br> **Stride = 1**: Kernel moves one position at a time (default) <br> **Stride > 1**: Skip positions, reduce output size <br> **Output size**: $n_{out} = \\lfloor \\frac{n_{in} - k}{s} \\rfloor + 1$ where $s$ = stride | **Input**: $\\mathbf{x} = [1, 2, 3, 4, 5, 6, 7, 8]$ (size 8) <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (size 3) <br><br> **Stride = 1** (every position): <br> $z_1 = 1(1) + 0(2) - 1(3) = -2$ <br> $z_2 = 1(2) + 0(3) - 1(4) = -2$ <br> $z_3 = 1(3) + 0(4) - 1(5) = -2$ <br> ... <br> **Output**: 6 values <br><br> **Stride = 2** (every other position): <br> $z_1 = 1(1) + 0(2) - 1(3) = -2$ <br> $z_2 = 1(3) + 0(4) - 1(5) = -2$ <br> $z_3 = 1(5) + 0(6) - 1(7) = -2$ <br> **Output**: $[-2, -2, -2]$ (size 3) <br><br> **Stride = 3**: <br> **Output**: 2 values | **Input**: $4 \\times 4$ matrix <br> **Kernel**: $3 \\times 3$ <br><br> **Stride = 1**: <br> Output: $\\lfloor \\frac{4-3}{1} \\rfloor + 1 = 2$ <br> Output size: $2 \\times 2$ <br> Positions: $(0,0), (0,1), (1,0), (1,1)$ <br><br> **Stride = 2**: <br> Output: $\\lfloor \\frac{4-3}{2} \\rfloor + 1 = 1$ <br> Output size: $1 \\times 1$ <br> Position: only $(0,0)$ <br><br> **Stride = (1, 2)** (different per axis): <br> Vertical stride = 1, Horizontal stride = 2 <br> Output size: $2 \\times 1$ <br><br> Common: stride=2 for downsampling by 2× |\n",
    "| **Dilation** | Spacing between kernel elements.<br> Can be determined from a single computation sequence <br> Creates an **expanded receptive field** without adding parameters.<br> Also called \"atrous convolution\". <br><br> **Dilation = 1**: Standard convolution (no gaps) <br> **Dilation = $d$**: Insert $d-1$ zeros between kernel weights <br> **Effective kernel size**: $k_{eff} = k + (k-1)(d-1)$ | **Input**: $\\mathbf{x} = [1, 2, 3, 4, 5, 6, 7, 8]$ (size 8) <br> **Kernel**: $\\boldsymbol{\\omega} = [1, 0, -1]$ (size 3) <br><br> **Dilation = 1** (standard): <br> $z_1 = 1(1) + 0(2) - 1(3) = -2$ <br> Uses positions: $i, i+1, i+2$ <br> Effective size: 3 <br><br> **Dilation = 2** (1 gap between): <br> $z_1 = 1(1) + 0(3) - 1(5) = -4$ <br> Uses positions: $i, i+2, i+4$ <br> Effective size: $3 + (3-1)(2-1) = 5$ <br> **Output**: $[-4, -4, -4, -4]$ <br><br> **Dilation = 3** (2 gaps): <br> $z_1 = 1(1) + 0(4) - 1(7) = -6$ <br> Uses positions: $i, i+3, i+6$ <br> Effective size: $3 + (3-1)(3-1) = 7$ | **Input**: $8 \\times 8$ matrix <br> **Kernel**: $3 \\times 3$ <br><br> **Dilation = 1**: <br> Standard $3 \\times 3$ convolution <br> Receptive field: $3 \\times 3 = 9$ pixels <br><br> **Dilation = 2**: <br> Kernel pattern: <br> $\\begin{bmatrix} \\omega_{0,0} & 0 & \\omega_{0,2} \\\\ 0 & 0 & 0 \\\\ \\omega_{2,0} & 0 & \\omega_{2,2} \\end{bmatrix}$ <br> Effective size: $5 \\times 5$ <br> Receptive field: 25 pixels <br> Only 9 parameters! <br><br> **Dilation = 4**: <br> Effective size: $9 \\times 9$ <br> Covers 81 pixels with 9 parameters <br><br> Used in: DeepLab, WaveNet |\n",
    "\n",
    "**General Output Size Formula:**\n",
    "\n",
    "For input size $n_{in}$, kernel size $k$, padding $p$, stride $s$, and dilation $d$:\n",
    "\n",
    "$$\\boxed{n_{out} = \\left\\lfloor \\frac{n_{in} + 2p - d(k-1) - 1}{s} \\right\\rfloor + 1}$$\n",
    "\n",
    "**Key Tradeoffs:**\n",
    "- **Large kernel size**: More parameters, more computation, bigger receptive field\n",
    "- **Large stride**: Faster computation, aggressive downsampling, may lose information\n",
    "- **Large dilation**: Exponentially growing receptive field, no extra parameters, but gaps in coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710e688",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Receptive Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6638b57",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### What is the Receptive Field?\n",
    "\n",
    "The **receptive field** of a neuron in a CNN is the **region of the input image** that influences the output of that neuron.<br> In other words, it's the \"field of view\" that a particular output pixel can \"see\" in the original input.\n",
    "\n",
    "- For a **single convolutional layer**: The receptive field is simply the kernel size\n",
    "- For **deep networks**: Each layer's receptive field grows, allowing deeper neurons to see larger portions of the input\n",
    "- The receptive field grows **multiplicatively** as we stack more layers\n",
    "\n",
    "#### What Does It Tell Us?\n",
    "\n",
    "The receptive field is crucial because it determines:\n",
    "\n",
    "1. **Context Understanding**: Larger receptive fields allow the network to capture more global context\n",
    "   - Small receptive field: Good for detecting **local features** (edges, textures)\n",
    "   - Large receptive field: Good for understanding **spatial relationships** and **semantic content**\n",
    "\n",
    "2. **Network Depth Requirements**: To recognize large objects, we need neurons with receptive fields large enough to cover them\n",
    "   - Face detection: Need receptive field ≥ face size\n",
    "   - Scene understanding: Need receptive field covering significant portion of image\n",
    "\n",
    "3. **Design Choices**: \n",
    "   - Shallow networks with large kernels vs. deep networks with small kernels\n",
    "   - Trade-off: $3 \\times (3 \\times 3)$ convolutions have same receptive field as $1 \\times (7 \\times 7)$ but with **fewer parameters** and **more non-linearity**\n",
    "\n",
    "#### How is it Computed?\n",
    "\n",
    "| **Case** | **Formula** | **Description** |\n",
    "|----------|-------------|-----------------|\n",
    "| **Single Layer** | $$\\boxed{r = k}$$ | For one convolutional layer, the receptive field size $r$ equals the kernel size $k$. |\n",
    "| **Multiple Layers (Iterative)** | $$\\boxed{r_l = r_{l-1} + (k_l - 1) \\cdot \\left(\\prod_{i=1}^{l-1} s_i \\right)\\cdot d_l}$$ | For layer $l$ with kernel size $k_l$, stride $s_l$, and dilation $d_l$. <br> Starting with $r_0 = 1$ (the input pixel itself). |\n",
    "| **Simplified (stride=1, dilation=1)** | $$\\boxed{r_L = 1 + L \\cdot (k - 1)}$$ <br> Or equivalently: <br> $$\\boxed{r_L = k + (L-1)(k-1)}$$ | For $L$ layers, each with kernel size $k$ and stride 1. <br> Linear growth with depth. <br> Each layer adds $(k-1)$ to receptive field. |\n",
    "| **General Closed Form** | $$\\boxed{r_L = \\sum_{i=1}^{L} \\left[(k_i - 1) \\prod_{j=1}^{i-1} s_j \\right] + 1}$$ | For $L$ layers with kernel sizes $k_1, k_2, ..., k_L$ <br> and strides $s_1, s_2, ..., s_L$ |\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "| **Example 1** | **Example 2** | **Example 3** | **Example 4** |\n",
    "|---------------|---------------|---------------|---------------|\n",
    "| **Three layers, kernel=3, stride=1** | **Three layers, kernel=3, stride=2** | **Mix of kernel sizes** | **With dilation** |\n",
    "| Layer 1: $r_1 = 3$ <br> Layer 2: $r_2 = 3 + (3-1) \\cdot 1 = 5$ <br> Layer 3: $r_3 = 5 + (3-1) \\cdot 1 = 7$ <br><br> **Key insight**: Each additional layer adds $(k-1) = 2$ to the receptive field. | Layer 1: $r_1 = 3$ <br> Layer 2: $r_2 = 3 + (3-1) \\cdot 2 = 7$ <br> Layer 3: $r_3 = 7 + (3-1) \\cdot (2 \\cdot 2) = 15$ <br><br> **Key insight**: Strides **amplify** receptive field growth! | Layers: $k_1=7$, $k_2=3$, $k_3=3$ <br> (all stride=1) <br><br> Layer 1: $r_1 = 7$ <br> Layer 2: $r_2 = 7 + (3-1) = 9$ <br> Layer 3: $r_3 = 9 + (3-1) = 11$ <br><br> **Key insight**: Larger initial kernel gives head start | Layer with kernel=3, dilation=2, stride=1: <br><br> Effective kernel size: <br> $k_{eff} = 3 + (3-1)(2-1) = 5$ <br><br> Receptive field increases by $(5-1) = 4$ <br><br> **Key insight**: Dilation expands receptive field without extra parameters |\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f6bd1",
   "metadata": {},
   "source": [
    "## Channels in Convolutional Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4417d5c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### What are Channels?\n",
    "\n",
    "**Channels** represent the **depth dimension** of the input/output, separate from spatial dimensions (height and width).\n",
    "\n",
    "- **Input channels**: Number of \"layers\" or \"features\" in the input\n",
    "  - Grayscale image: 1 channel (intensity)\n",
    "  - RGB image: 3 channels (Red, Green, Blue)\n",
    "  - Intermediate layers: arbitrary number of channels (learned features)\n",
    "\n",
    "- **Output channels**: Number of different filters/feature maps we want to produce\n",
    "  - Controlled by the number of filters in the layer\n",
    "  - Each filter produces one output channel\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Do We Use Multiple Channels (Filters)?\n",
    "\n",
    "Adding multiple output channels means applying **multiple different filters** to the same input. Each filter learns to detect **different features**:\n",
    "\n",
    "**1. Feature Diversity** \n",
    "- Different filters learn to detect different patterns:\n",
    "  - Filter 1 might detect **horizontal edges**\n",
    "  - Filter 2 might detect **vertical edges**\n",
    "  - Filter 3 might detect **diagonal lines**\n",
    "  - Filter 4 might detect **color gradients**\n",
    "  - Filter 5 might detect **textures**\n",
    "  \n",
    "**2. Hierarchical Feature Learning**\n",
    "- **Early layers** (few channels → many channels):\n",
    "  - Input: 3 channels (RGB)\n",
    "  - Output: 64 channels\n",
    "  - Learn **low-level features**: edges, corners, simple textures\n",
    "  \n",
    "- **Middle layers** (many channels → more channels):\n",
    "  - Input: 64 channels\n",
    "  - Output: 128/256 channels\n",
    "  - Learn **mid-level features**: combinations of edges (shapes, parts of objects)\n",
    "  \n",
    "- **Deep layers** (many channels → many channels):\n",
    "  - Input: 256 channels\n",
    "  - Output: 512 channels\n",
    "  - Learn **high-level features**: object parts, semantic concepts\n",
    "\n",
    "**3. Increased Representational Power**\n",
    "- More filters = more capacity to learn complex patterns\n",
    "- Each filter adds a new \"perspective\" or \"detector\" for analyzing the input\n",
    "- Network can combine information from multiple channels to make decisions\n",
    "\n",
    "**4. Why More Weights?**\n",
    "- **Trade-off**: More parameters vs. better representation\n",
    "- Example: 64 filters on RGB image\n",
    "  - Adds: $64 \\times 3 \\times 3 \\times 3 = 1,728$ parameters\n",
    "  - Gain: 64 different feature detectors instead of 1\n",
    "  - Result: Network can detect many patterns **simultaneously**\n",
    "\n",
    "**Analogy**: \n",
    "- **1 channel** = Looking at the world through 1 detector (e.g., only detecting vertical edges)\n",
    "- **64 channels** = Looking at the world through 64 different detectors simultaneously (edges, textures, colors, patterns)\n",
    "- The network **learns** what each filter should detect through training\n",
    "\n",
    "**Key Insight**: More channels ≠ redundancy. Each filter specializes in detecting different features, allowing the network to build a rich, diverse representation of the input.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dimension Notation with Batches\n",
    "\n",
    "**Input dimensions**: $\\textcolor{magenta}{N} \\times \\textcolor{blue}{C_{in}} \\times \\textcolor{orange}{H_{in}} \\times \\textcolor{orange}{W_{in}}$\n",
    "- $\\textcolor{magenta}{N}$ = **Batch size** (number of samples processed together)\n",
    "- $\\textcolor{blue}{C_{in}}$ = Number of **input channels** (depth)\n",
    "- $\\textcolor{orange}{H_{in}}$ = Input **height** (spatial)\n",
    "- $\\textcolor{orange}{W_{in}}$ = Input **width** (spatial)\n",
    "\n",
    "**Kernel/Filter dimensions**: $\\textcolor{green}{C_{out}} \\times \\textcolor{blue}{C_{in}} \\times \\textcolor{purple}{k_h} \\times \\textcolor{purple}{k_w}$\n",
    "- $\\textcolor{green}{C_{out}}$ = Number of **output channels** (how many filters)\n",
    "- $\\textcolor{blue}{C_{in}}$ = Number of **input channels** (must match input depth)\n",
    "- $\\textcolor{purple}{k_h}, \\textcolor{purple}{k_w}$ = **Kernel size** (height, width)\n",
    "\n",
    "**Output dimensions**: $\\textcolor{magenta}{N} \\times \\textcolor{green}{C_{out}} \\times \\textcolor{red}{H_{out}} \\times \\textcolor{red}{W_{out}}$\n",
    "- $\\textcolor{magenta}{N}$ = **Batch size** (unchanged through convolution)\n",
    "- $\\textcolor{green}{C_{out}}$ = Number of **output channels** (feature maps)\n",
    "- $\\textcolor{red}{H_{out}}$ = Output **height** (spatial)\n",
    "- $\\textcolor{red}{W_{out}}$ = Output **width** (spatial)\n",
    "\n",
    "**Visual Representation:**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/batchConv.png\" width=\"700\" />\n",
    "</div>\n",
    "\n",
    "**Key Notes:**\n",
    "- The **batch dimension** $\\textcolor{magenta}{N}$ remains constant through convolution\n",
    "- Each sample in the batch is processed **independently** with the **same filters**\n",
    "- Batching enables parallel processing and efficient GPU utilization\n",
    "\n",
    "---\n",
    "\n",
    "#### How Convolution Works with Channels\n",
    "\n",
    "**Single Filter (produces 1 output channel):**\n",
    "\n",
    "1. A single filter has dimensions: $\\textcolor{blue}{C_{in}} \\times \\textcolor{purple}{k_h} \\times \\textcolor{purple}{k_w}$\n",
    "2. It convolves across **all input channels** simultaneously\n",
    "3. Results from all input channels are **summed** to produce one output value\n",
    "4. This produces **one output channel** (feature map)\n",
    "\n",
    "**Formula for one output pixel**:\n",
    "$$z_{h,w} = \\sum_{c=1}^{\\textcolor{blue}{C_{in}}} \\sum_{i=0}^{\\textcolor{purple}{k_h}-1} \\sum_{j=0}^{\\textcolor{purple}{k_w}-1} \\omega_{c,i,j} \\cdot x_{c, h+i, w+j}$$\n",
    "\n",
    "**Multiple Filters (produces multiple output channels):**\n",
    "\n",
    "- Use $\\textcolor{green}{C_{out}}$ different filters\n",
    "- Each filter produces 1 output channel\n",
    "- Total output: $\\textcolor{green}{C_{out}}$ feature maps\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2e502",
   "metadata": {},
   "source": [
    "\n",
    "#### Detailed Examples\n",
    "\n",
    "| **Example** | **Input** | **Filter** | **Computation/Process** | **Output** |\n",
    "|-------------|-----------|------------|-------------------------|------------|\n",
    "| **Example 1: RGB → Single Feature Map** | $\\textcolor{blue}{3} \\times \\textcolor{orange}{5} \\times \\textcolor{orange}{5}$ (RGB image) <br><br> $\\textcolor{blue}{C_{in} = 3}$ (R, G, B) <br> $\\textcolor{orange}{H_{in} = 5, W_{in} = 5}$ | $\\textcolor{blue}{3} \\times \\textcolor{purple}{3} \\times \\textcolor{purple}{3}$ <br><br> $\\textcolor{blue}{C_{in} = 3}$ (matches input) <br> $\\textcolor{purple}{k_h = 3, k_w = 3}$ <br><br> **Parameters**: $3 \\times 3 \\times 3 = 27$ weights | **At position (1,1)**: <br> $$\\begin{aligned} z_{1,1} &= \\underbrace{\\sum_{i=0}^{2}\\sum_{j=0}^{2} \\omega_{\\text{R},i,j} \\cdot x_{\\text{R}, 1+i, 1+j}}_{\\text{Red}} \\\\ &+ \\underbrace{\\sum_{i=0}^{2}\\sum_{j=0}^{2} \\omega_{\\text{G},i,j} \\cdot x_{\\text{G}, 1+i, 1+j}}_{\\text{Green}} \\\\ &+ \\underbrace{\\sum_{i=0}^{2}\\sum_{j=0}^{2} \\omega_{\\text{B},i,j} \\cdot x_{\\text{B}, 1+i, 1+j}}_{\\text{Blue}} \\end{aligned}$$ <br> Sum across all 3 input channels → 1 value | $\\textcolor{green}{1} \\times \\textcolor{red}{3} \\times \\textcolor{red}{3}$ <br> (valid padding) <br><br> $\\textcolor{green}{C_{out} = 1}$ (single feature map) <br> $\\textcolor{red}{H_{out} = 5 - 3 + 1 = 3}$ <br> $\\textcolor{red}{W_{out} = 3}$ |\n",
    "| **Example 2: RGB → Multiple Feature Maps** | $\\textcolor{blue}{3} \\times \\textcolor{orange}{32} \\times \\textcolor{orange}{32}$ (RGB image) <br><br> $\\textcolor{blue}{C_{in} = 3}$ <br> $\\textcolor{orange}{H_{in} = 32, W_{in} = 32}$ | $\\textcolor{green}{64} \\times \\textcolor{blue}{3} \\times \\textcolor{purple}{5} \\times \\textcolor{purple}{5}$ <br><br> $\\textcolor{green}{C_{out} = 64}$ different filters <br> Each filter: $\\textcolor{blue}{3} \\times \\textcolor{purple}{5} \\times \\textcolor{purple}{5}$ <br><br> **Parameters**: $64 \\times 3 \\times 5 \\times 5 = 4{,}800$ weights | **Process**: <br> 1. Filter 1 convolves with all 3 input channels → output channel 1 <br> 2. Filter 2 convolves with all 3 input channels → output channel 2 <br> 3. ... <br> 4. Filter 64 convolves with all 3 input channels → output channel 64 <br><br> Each filter produces **one** feature map | $\\textcolor{green}{64} \\times \\textcolor{red}{32} \\times \\textcolor{red}{32}$ <br> (same padding) <br><br> $\\textcolor{green}{C_{out} = 64}$ feature maps <br> $\\textcolor{red}{H_{out} = 32, W_{out} = 32}$ <br> (spatial size preserved) |\n",
    "| **Example 3: Deep Layer (Multi-channel → Multi-channel)** | $\\textcolor{blue}{128} \\times \\textcolor{orange}{16} \\times \\textcolor{orange}{16}$ <br> (from previous layer) <br><br> $\\textcolor{blue}{C_{in} = 128}$ <br> $\\textcolor{orange}{H_{in} = 16, W_{in} = 16}$ | $\\textcolor{green}{256} \\times \\textcolor{blue}{128} \\times \\textcolor{purple}{3} \\times \\textcolor{purple}{3}$ <br><br> $\\textcolor{green}{C_{out} = 256}$ different filters <br> Each filter operates on **all** $\\textcolor{blue}{128}$ input channels <br> Each filter: $1{,}152$ weights <br><br> **Parameters**: $256 \\times 128 \\times 3 \\times 3 = 294{,}912$ weights | **Process**: <br> Each of the 256 filters: <br> 1. Convolves across all 128 input channels <br> 2. Sums contributions from all channels <br> 3. Produces one output feature map <br><br> Total: 256 different filters → 256 output channels <br><br> Each output pixel depends on $128 \\times 3 \\times 3 = 1{,}152$ input values | $\\textcolor{green}{256} \\times \\textcolor{red}{16} \\times \\textcolor{red}{16}$ <br> (same padding) <br><br> $\\textcolor{green}{C_{out} = 256}$ feature maps <br> $\\textcolor{red}{H_{out} = 16, W_{out} = 16}$ <br> (spatial size preserved) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca28a0",
   "metadata": {},
   "source": [
    "\n",
    "#### Parameter Count Formula\n",
    "\n",
    "For a convolutional layer:\n",
    "\n",
    "$$\\boxed{\\text{Parameters} = \\textcolor{green}{C_{out}} \\times \\textcolor{blue}{C_{in}} \\times \\textcolor{purple}{k_h} \\times \\textcolor{purple}{k_w} + \\textcolor{green}{C_{out}}}$$\n",
    "\n",
    "The $+ \\textcolor{green}{C_{out}}$ term accounts for **bias terms** (one per output channel).\n",
    "\n",
    "Without bias:\n",
    "$$\\boxed{\\text{Parameters} = \\textcolor{green}{C_{out}} \\times \\textcolor{blue}{C_{in}} \\times \\textcolor{purple}{k_h} \\times \\textcolor{purple}{k_w}}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Output Spatial Dimensions\n",
    "\n",
    "Using the general formula from before:\n",
    "\n",
    "$$\\boxed{H_{out} = \\left\\lfloor \\frac{{H_{in}} + 2p - d({k_h}-1) - 1}{s} \\right\\rfloor + 1}$$\n",
    "\n",
    "$$\\boxed{{W_{out}} = \\left\\lfloor \\frac{{W_{in}} + 2p - d({k_w}-1) - 1}{s} \\right\\rfloor + 1}$$\n",
    "\n",
    "Where $p$ = padding, $s$ = stride, $d$ = dilation.\n",
    "\n",
    "**Key Insight**: \n",
    "- Channels ($\\textcolor{blue}{C_{in}} \\to \\textcolor{green}{C_{out}}$) are controlled by the **number of filters**\n",
    "- Spatial dimensions ($\\textcolor{orange}{H_{in}, W_{in}} \\to \\textcolor{red}{H_{out}, W_{out}}$) are controlled by **kernel size, stride, padding**\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b8a9b",
   "metadata": {},
   "source": [
    "## Activation Functions and Bias in CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73428d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Why Do We Need Bias and Activation Functions?\n",
    "\n",
    "So far, we've discussed the **convolution operation** which computes weighted sums of inputs. However, two critical components complete a convolutional layer:\n",
    "\n",
    "1. **Bias term**: Provides an offset/shift to the convolution output\n",
    "2. **Activation function**: Introduces non-linearity into the network\n",
    "\n",
    "Without these, stacking multiple convolutional layers would be equivalent to a single linear transformation, severely limiting the network's ability to learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Bias Term\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "The **bias** is a learnable parameter added to the output of the convolution operation. Each output channel gets its own bias value.\n",
    "\n",
    "**Formula with Bias:**\n",
    "\n",
    "For a single output channel, the convolution with bias at position $(h, w)$ is:\n",
    "\n",
    "$$\\boxed{z_{h,w} = \\sum_{c=1}^{C_{in}} \\sum_{i=0}^{k_h-1} \\sum_{j=0}^{k_w-1} \\omega_{c,i,j} \\cdot x_{c, h+i, w+j} + b}$$\n",
    "\n",
    "Where:\n",
    "- $\\omega_{c,i,j}$ = filter weights\n",
    "- $x_{c, h+i, w+j}$ = input values\n",
    "- $\\textcolor{red}{b}$ = **bias term** (scalar, shared across spatial dimensions)\n",
    "\n",
    "**Why Do We Need Bias?**\n",
    "\n",
    "1. **Flexibility in Activation**: Without bias, if all inputs are zero, the output is forced to be zero. Bias allows the neuron to activate even when inputs are zero.\n",
    "\n",
    "2. **Shifting the Decision Boundary**: Bias shifts the activation function, allowing the network to learn patterns at different intensity levels.\n",
    "\n",
    "3. **Per-Channel Offset**: Each output channel has its own bias, allowing different feature maps to have different baseline values.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider detecting a bright edge vs. a dark edge:\n",
    "- Without bias: The filter might only detect edges of a specific brightness\n",
    "- With bias: The filter can adjust to detect edges regardless of overall brightness by shifting the threshold\n",
    "\n",
    "**Dimension Note:**\n",
    "\n",
    "For $C_{out}$ output channels, we have $C_{out}$ bias values:\n",
    "- **Weights**: $C_{out} \\times C_{in} \\times k_h \\times k_w$ parameters\n",
    "- **Biases**: $C_{out}$ parameters\n",
    "- **Total**: $C_{out} \\times (C_{in} \\times k_h \\times k_w + 1)$ parameters\n",
    "\n",
    "---\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "**What Are They?**\n",
    "\n",
    "An **activation function** is a non-linear function applied **element-wise** to the convolution output (after adding bias<br> It determines whether and how much a neuron should \"activate\" based on its input.\n",
    "\n",
    "**Complete Formula:**\n",
    "\n",
    "$$\\boxed{a_{h,w} = \\sigma\\left(\\sum_{c=1}^{C_{in}} \\sum_{i=0}^{k_h-1} \\sum_{j=0}^{k_w-1} \\omega_{c,i,j} \\cdot x_{c, h+i, w+j} + b\\right)}$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma(\\cdot)$ = activation function (e.g., ReLU, sigmoid, tanh)\n",
    "- $a_{h,w}$ = final activated output at position $(h,w)$\n",
    "\n",
    "**Simplified Notation:**\n",
    "\n",
    "$$\\boxed{a = \\sigma(z) = \\sigma(\\text{Conv}(x) + b)}$$\n",
    "\n",
    "Where $z$ is the pre-activation output (after convolution + bias).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Do We Need Non-Linearity?\n",
    "\n",
    "**The Problem with Linearity:**\n",
    "\n",
    "Without activation functions, stacking layers would just create a deeper linear transformation:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Layer 1:} \\quad & z^{(1)} = W^{(1)}x + b^{(1)} \\\\\n",
    "\\text{Layer 2:} \\quad & z^{(2)} = W^{(2)}z^{(1)} + b^{(2)} \\\\\n",
    "& = W^{(2)}(W^{(1)}x + b^{(1)}) + b^{(2)} \\\\\n",
    "& = \\underbrace{W^{(2)}W^{(1)}}_{\\text{Single matrix}}x + \\underbrace{W^{(2)}b^{(1)} + b^{(2)}}_{\\text{Single bias}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Result**: Multiple layers collapse into a single linear transformation! No benefit from depth.\n",
    "\n",
    "**Solution**: Non-linear activations break this collapse:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Layer 1:} \\quad & a^{(1)} = \\sigma(W^{(1)}x + b^{(1)}) \\\\\n",
    "\\text{Layer 2:} \\quad & a^{(2)} = \\sigma(W^{(2)}a^{(1)} + b^{(2)})\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now each layer can learn complex, non-linear transformations.\n",
    "\n",
    "#### Where Do We Apply Activation Functions?\n",
    "\n",
    "**Standard CNN Layer Structure:**\n",
    "\n",
    "```\n",
    "Input → Convolution → Bias → Activation → Output\n",
    "  ↓          ↓          ↓         ↓           ↓\n",
    " [H,W,C] → [H',W',C'] → add b → σ(·) → [H',W',C']\n",
    "```\n",
    "\n",
    "**In Practice (PyTorch example):**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Separate operations\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "# Forward pass\n",
    "z = conv(x)           # Convolution + bias (built into Conv2d)\n",
    "a = activation(z)     # Apply ReLU activation\n",
    "\n",
    "# Or use Sequential\n",
    "layer = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU()\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: The bias is typically **included by default** in convolutional layers (can be disabled with `bias=False`).\n",
    "\n",
    "---\n",
    "\n",
    "#### Complete Convolutional Layer Formula\n",
    "\n",
    "Putting it all together, a complete convolutional layer computes:\n",
    "\n",
    "$$\\boxed{a^{(l)}_{h,w} = \\sigma\\left(\\sum_{c=1}^{C_{in}} \\sum_{i=0}^{k_h-1} \\sum_{j=0}^{k_w-1} \\omega^{(l)}_{c,i,j} \\cdot a^{(l-1)}_{c, h+i, w+j} + b^{(l)}\\right)}$$\n",
    "\n",
    "Where:\n",
    "- $a^{(l-1)}$ = activation from previous layer (input to this layer)\n",
    "- $\\omega^{(l)}$ = weights for layer $l$\n",
    "- $b^{(l)}$ = bias for layer $l$\n",
    "- $\\sigma$ = activation function (e.g., ReLU)\n",
    "- $a^{(l)}$ = activation output for layer $l$\n",
    "\n",
    "**In vector form:**\n",
    "\n",
    "$$\\boxed{\\mathbf{a}^{(l)} = \\sigma(\\mathbf{W}^{(l)} * \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)})}$$\n",
    "\n",
    "Where $*$ denotes the convolution operation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Complete Layer Computation\n",
    "\n",
    "**Setup:**\n",
    "- Input: $3 \\times 5 \\times 5$ (RGB image)\n",
    "- Filter: $3 \\times 3$ kernel, 64 output channels\n",
    "- Padding: 1 (same padding)\n",
    "- Stride: 1\n",
    "- Activation: ReLU\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. **Convolution**: For each of 64 filters, compute weighted sum across all 3 input channels\n",
    "   - Output shape: $64 \\times 5 \\times 5$ (before activation)\n",
    "\n",
    "2. **Add Bias**: Add one bias value per output channel (broadcast across spatial dimensions)\n",
    "   - 64 different bias values: $b_1, b_2, ..., b_{64}$\n",
    "   - Each $5 \\times 5$ feature map gets its corresponding bias added\n",
    "\n",
    "3. **Apply ReLU**: Element-wise, set negative values to zero\n",
    "   $$a_{h,w,c} = \\max(0, z_{h,w,c})$$\n",
    "   - Output shape: $64 \\times 5 \\times 5$ (after activation)\n",
    "\n",
    "**Numerical Example (single position):**\n",
    "\n",
    "For output channel 1, position $(2, 2)$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "z_{2,2,1} &= \\underbrace{\\left[\\sum \\omega \\cdot x\\right]}_{\\text{Convolution}} + b_1 \\\\\n",
    "&= 0.5 + 0.3 = 0.8 \\quad \\text{(suppose)} \\\\\n",
    "a_{2,2,1} &= \\text{ReLU}(0.8) = \\max(0, 0.8) = 0.8\n",
    "\\end{aligned}$$\n",
    "\n",
    "For output channel 2, position $(2, 2)$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "z_{2,2,2} &= \\left[\\sum \\omega \\cdot x\\right] + b_2 \\\\\n",
    "&= -1.2 + 0.5 = -0.7 \\quad \\text{(suppose)} \\\\\n",
    "a_{2,2,2} &= \\text{ReLU}(-0.7) = \\max(0, -0.7) = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "This neuron is \"inactive\" (output is zero) for this spatial location.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "1. **Bias** allows the network to shift activation thresholds, giving each filter flexibility in what it detects\n",
    "\n",
    "2. **Activation functions** introduce non-linearity, allowing the network to learn complex patterns\n",
    "\n",
    "3. **ReLU** is the most common choice in CNNs due to:\n",
    "   - Fast computation\n",
    "   - Helps with gradient flow\n",
    "   - Sparse activations (many zeros)\n",
    "\n",
    "4. **Complete layer**: Convolution → Add Bias → Activation\n",
    "   $$\\text{output} = \\sigma(\\text{Conv}(\\text{input}) + b)$$\n",
    "\n",
    "5. **Depth matters**: Non-linearity between layers allows deep networks to learn hierarchical features\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dba47",
   "metadata": {},
   "source": [
    "## Pooling Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d02190",
   "metadata": {},
   "source": [
    "#### What is Pooling?\n",
    "\n",
    "**Pooling** (also called **subsampling** or **downsampling**) is an operation that **reduces the spatial dimensions** (height and width) of feature maps while retaining important information.\n",
    "\n",
    "**Key characteristics:**\n",
    "- **No learnable parameters** (unlike convolution)\n",
    "- Operates **independently on each channel**\n",
    "- Applies a fixed function (max, average, etc.) over local regions\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Do We Use Pooling?\n",
    "\n",
    "1. **Dimensionality Reduction**: Reduces spatial size → fewer parameters in later layers → faster computation\n",
    "\n",
    "2. **Translation Invariance**: Small shifts in input don't significantly change output\n",
    "   - Example: Detecting a cat shouldn't depend on exact pixel position\n",
    "\n",
    "3. **Controlled Information Loss**: Forces network to learn robust features that survive downsampling\n",
    "\n",
    "4. **Expanding Receptive Field**: Each neuron in later layers \"sees\" a larger area of the original input\n",
    "\n",
    "5. **Overfitting Prevention**: Reduces model capacity, acts as regularization\n",
    "\n",
    "---\n",
    "\n",
    "#### Types of Pooling\n",
    "\n",
    "| **Pooling Type** | **Operation** | **Formula** | **Use Case** | **Properties** |\n",
    "|------------------|---------------|-------------|--------------|----------------|\n",
    "| **Max Pooling** | Take the **maximum** value in each pooling window | $$z_{i,j} = \\max_{m,n \\in \\text{window}} x_{i+m, j+n}$$ | **Most common** <br> Feature detection, classification | ✅ Preserves strongest activations <br> ✅ Translation invariant <br> ✅ Retains sharp features <br> ❌ Loses spatial information <br> ❌ Discards weaker signals |\n",
    "| **Average Pooling** | Take the **mean** of values in each pooling window | $$z_{i,j} = \\frac{1}{k_h \\times k_w} \\sum_{m,n \\in \\text{window}} x_{i+m, j+n}$$ | Smooth downsampling <br> Final layers (global pooling) | ✅ Smoother output <br> ✅ Retains background info <br> ❌ Dilutes strong features <br> ❌ Less common in hidden layers |\n",
    "| **Global Average Pooling** | Average over **entire** feature map (spatial dimensions → 1×1) | $$z_c = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{c,i,j}$$ | **Output layer** <br> Replacing fully connected layers | ✅ No parameters <br> ✅ Prevents overfitting <br> ✅ Works with variable input sizes <br> Used in: ResNet, Inception |\n",
    "\n",
    "---\n",
    "\n",
    "#### Max Pooling: Detailed Example\n",
    "\n",
    "**Setup:**\n",
    "- Input: $4 \\times 4$ feature map\n",
    "- Pool size: $2 \\times 2$\n",
    "- Stride: $2$ (non-overlapping)\n",
    "\n",
    "**Input:**\n",
    "$$\\begin{bmatrix}\n",
    "1 & 3 & 2 & 4 \\\\\n",
    "5 & 6 & 1 & 2 \\\\\n",
    "7 & 2 & 3 & 0 \\\\\n",
    "4 & 1 & 8 & 5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Process:**\n",
    "\n",
    "| Region | Values | Max | Position in Output |\n",
    "|--------|--------|-----|---------------------|\n",
    "| Top-left | $\\begin{bmatrix} 1 & 3 \\\\ 5 & 6 \\end{bmatrix}$ | $\\mathbf{6}$ | $(0, 0)$ |\n",
    "| Top-right | $\\begin{bmatrix} 2 & 4 \\\\ 1 & 2 \\end{bmatrix}$ | $\\mathbf{4}$ | $(0, 1)$ |\n",
    "| Bottom-left | $\\begin{bmatrix} 7 & 2 \\\\ 4 & 1 \\end{bmatrix}$ | $\\mathbf{7}$ | $(1, 0)$ |\n",
    "| Bottom-right | $\\begin{bmatrix} 3 & 0 \\\\ 8 & 5 \\end{bmatrix}$ | $\\mathbf{8}$ | $(1, 1)$ |\n",
    "\n",
    "**Output:**\n",
    "$$\\begin{bmatrix}\n",
    "6 & 4 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Spatial dimensions reduced: $4 \\times 4 \\to 2 \\times 2$ (downsampled by 2×)\n",
    "\n",
    "---\n",
    "\n",
    "#### Average Pooling: Comparison\n",
    "\n",
    "Same input, $2 \\times 2$ pool, stride $2$:\n",
    "\n",
    "**Average Pooling Output:**\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{1+3+5+6}{4} = 3.75 & \\frac{2+4+1+2}{4} = 2.25 \\\\\n",
    "\\frac{7+2+4+1}{4} = 3.5 & \\frac{3+0+8+5}{4} = 4\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "3.75 & 2.25 \\\\\n",
    "3.5 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Notice:** Values are smoothed compared to max pooling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Pooling Hyperparameters\n",
    "\n",
    "**Output Size Formula:**\n",
    "\n",
    "Similar to convolution:\n",
    "$$\\boxed{H_{out} = \\left\\lfloor \\frac{H_{in} - k_h}{s} \\right\\rfloor + 1, \\quad W_{out} = \\left\\lfloor \\frac{W_{in} - k_w}{s} \\right\\rfloor + 1}$$\n",
    "\n",
    "Where:\n",
    "- $k_h, k_w$ = pool size (e.g., $2 \\times 2$)\n",
    "- $s$ = stride (typically equals pool size for non-overlapping pools)\n",
    "\n",
    "**Common Configurations:**\n",
    "\n",
    "| **Configuration** | **Pool Size** | **Stride** | **Effect** | **Usage** |\n",
    "|-------------------|---------------|------------|------------|-----------|\n",
    "| Standard | $2 \\times 2$ | $2$ | Halves spatial dimensions | **Most common** |\n",
    "| Aggressive | $3 \\times 3$ | $2$ | More downsampling, overlapping | Deeper networks |\n",
    "| Small | $2 \\times 2$ | $1$ | Overlapping pools, less reduction | Preserving resolution |\n",
    "| Global | $H \\times W$ | N/A | Reduces to $1 \\times 1$ | Final layer before FC |\n",
    "\n",
    "---\n",
    "\n",
    "#### Effect on Dimensions\n",
    "\n",
    "**Example: Multi-channel Feature Maps**\n",
    "\n",
    "Input: $64 \\times 32 \\times 32$ (64 channels, $32 \\times 32$ spatial)\n",
    "\n",
    "Apply $2 \\times 2$ max pooling, stride $2$:\n",
    "\n",
    "Output: $64 \\times 16 \\times 16$\n",
    "\n",
    "**Key Point:** Pooling operates **independently on each channel**\n",
    "- Number of channels: **unchanged**\n",
    "- Spatial dimensions: **halved**\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use Max vs. Average Pooling?\n",
    "\n",
    "| **Scenario** | **Recommendation** | **Reason** |\n",
    "|--------------|-------------------|------------|\n",
    "| **Hidden layers** | **Max pooling** | Preserves strongest features, better for detection tasks |\n",
    "| **Classification tasks** | **Max pooling** | Focuses on most prominent features |\n",
    "| **Smooth representations** | **Average pooling** | Retains more spatial information, less aggressive |\n",
    "| **Final layer** | **Global Average Pooling** | Replaces FC layers, reduces overfitting |\n",
    "| **Segmentation** | **Avoid aggressive pooling** | Need to preserve spatial resolution |\n",
    "\n",
    "---\n",
    "\n",
    "#### Pooling in Modern Architectures\n",
    "\n",
    "**Traditional (AlexNet, VGG):**\n",
    "```\n",
    "Conv → ReLU → Conv → ReLU → MaxPool → ...\n",
    "```\n",
    "\n",
    "**Modern (ResNet):**\n",
    "- Fewer pooling layers\n",
    "- Uses **stride in convolution** for downsampling\n",
    "- **Global Average Pooling** before final classification\n",
    "\n",
    "**Recent Trend:**\n",
    "- Moving away from pooling in some architectures\n",
    "- Replaced by **strided convolutions** (learnable downsampling)\n",
    "- Still widely used in practice\n",
    "\n",
    "---\n",
    "\n",
    "#### Complete Layer Visualization\n",
    "\n",
    "**Typical CNN Block:**\n",
    "\n",
    "```\n",
    "Input [3×224×224]\n",
    "    ↓\n",
    "Conv 3×3, 64 filters, stride=1, padding=1\n",
    "    ↓\n",
    "ReLU\n",
    "    ↓\n",
    "Output [64×224×224]\n",
    "    ↓\n",
    "MaxPool 2×2, stride=2\n",
    "    ↓\n",
    "Output [64×112×112]  ← Spatial dimensions halved, channels unchanged\n",
    "```\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
