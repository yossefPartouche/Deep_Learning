{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c959678",
   "metadata": {},
   "source": [
    "# Detection and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c30a0b",
   "metadata": {},
   "source": [
    "In the previous topics in this chapter we've been focusing on:\n",
    "- The construction of CNN\n",
    "- The Architectures of CNN\n",
    "- Classifcation using CNN\n",
    "- Feature Visualisation\n",
    "\n",
    "What is we wanted to classfiy multiple object in an image input, and also identify (via a tight bounding box around the object) the location of the classified object. \n",
    "\n",
    "**Objective**\n",
    "\n",
    "1. Single object Detection\n",
    "2. Multiple Object Detection:\n",
    "   - R-CNN\n",
    "   - Fast R-CNN\n",
    "   - Faster R-CNN\n",
    "3. Single Stage Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c52e88",
   "metadata": {},
   "source": [
    "## Single Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cbd3b",
   "metadata": {},
   "source": [
    "\n",
    "### Steps\n",
    "\n",
    "1. Choose a backbone network such as AlexNet/Resnet/DenseNet etc.\n",
    "2. From a FC create two outputs\n",
    "    - One for Classifcation (Vector of 4096 $\\to$ 1000)\n",
    "    - Box Coordinate (Vector of 4096 $\\to$ 4)\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/SingleObjDet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "#### Loss\n",
    "\n",
    "| **Input** | **Task** | **Output** | **Loss Function** |\n",
    "|-----------|----------|------------|-------------------|\n",
    "| (H, W, 3) | **Classification** | Class label $c \\in \\{1, \\ldots, C\\}$ | Cross-Entropy Loss: $-\\log p_c$ |\n",
    "| (H, W, 3) | **Localization** | Bounding box $(x, y, w, h)$ | L2 Loss: $\\sum (b_i - \\hat{b}_i)^2$ or Smooth L1 |\n",
    "\n",
    "\n",
    "Our Total Loss is computed as follows: \n",
    "\n",
    "$$L(y, P, b, b') = -\\log P(y) + \\lambda L_{regression}(b, b')$$\n",
    "\n",
    "### Evaluate \n",
    "\n",
    "- For **Classification**, we evaluate using the **accuracy** metric \n",
    "- For **Localization**, we compute the **Intersection over Union (IoU)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intersection over Union (IoU)**\n",
    "\n",
    "$$\\text{IoU}(A, B) = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}} = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "**Where:**\n",
    "- $A$ = predicted bounding box area\n",
    "- $B$ = ground truth bounding box area  \n",
    "- $A \\cap B$ = overlapping region (intersection)\n",
    "- $A \\cup B$ = total covered area (union)\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $\\text{IoU} \\in [0, 1]$\n",
    "  - $\\text{IoU} = 0$ → no overlap (completely wrong prediction)\n",
    "  - $\\text{IoU} = 1$ → perfect overlap (exact match)\n",
    "- **Common threshold:** $\\text{IoU} > 0.5$ is considered a \"good\" detection\n",
    "  - $\\text{IoU} \\geq 0.5$ → True Positive (TP)\n",
    "  - $\\text{IoU} < 0.5$ → False Positive (FP)\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/IOUCalc.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac0780",
   "metadata": {},
   "source": [
    "## Multi-Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba5f9c",
   "metadata": {},
   "source": [
    "$\\text{Input} \\ (W, H, 3)$\n",
    "\n",
    "$\\text{Output} \\ \\{(C_1, (x_1, y_1, w_1, h_1)), (C_2, (x_2, y_2, w_2, h_2)) \\dots (C_k, (x_k, y_k, w_k, h_k))\\}$\n",
    "\n",
    "Where $k$ is the number of objects to identify in the image\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/obj1.png\" width=\"295\"/>\n",
    "<img src=\"../images/chap8/obj2.png\" width=\"300\"/>\n",
    "<img src=\"../images/chap8/obj4.png\" width=\"335\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Challanges\n",
    "\n",
    "1. **Multiple Outputs** - Number of objects changes per image\n",
    "2. **Multiple types of Outputs** - We need to answer: \"what\" and \"where\" for every object\n",
    "3. **Multiple Lengths of outputs** - \n",
    "4. **Multiple Size** - Objects vary in size\n",
    "5. **Multiple Detections** - Objects can be detected multiple times\n",
    "6. **Occlusions** - Objects can hide part/total aspects of other objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160cb8b",
   "metadata": {},
   "source": [
    "### Naïve Approach - Sliding Window\n",
    "\n",
    "1. Apply CNN classification to many different crops of the image\n",
    "2. Classify each crop as **background** or a specific **object class**\n",
    "3. Vary the crop size and position to detect objects at different scales and locations\n",
    "\n",
    "**Complexity Issue**\n",
    "\n",
    "To create a bounding box, we need 2 corner points (4 coordinates total): $(x_1, y_1, x_2, y_2)$\n",
    "\n",
    "**Question:** Given an image of size $H \\times W$, how many different possible bounding boxes can be created?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Each bounding box is defined by:\n",
    "- Top-left corner: $(x_1, y_1)$ where $x_1 \\in \\{0, \\ldots, W-1\\}$, $y_1 \\in \\{0, \\ldots, H-1\\}$\n",
    "- Bottom-right corner: $(x_2, y_2)$ where $x_2 \\in \\{x_1+1, \\ldots, W\\}$, $y_2 \\in \\{y_1+1, \\ldots, H\\}$\n",
    "\n",
    "**Number of possible boxes:**\n",
    "\n",
    "$$\\binom{H}{2} \\times \\binom{W}{2} = \\frac{H(H-1)}{2} \\times \\frac{W(W-1)}{2} = O(H^2 W^2)$$\n",
    "\n",
    "**Why this is a problem:**\n",
    "- For a $224 \\times 224$ image: $\\approx 2.5 \\times 10^9$ possible boxes!\n",
    "- Running CNN on each crop is **computationally infeasible**\n",
    "- Need ~2.5 billion forward passes per image\n",
    "\n",
    "**This motivates region proposal methods (R-CNN, Fast R-CNN, Faster R-CNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55474003",
   "metadata": {},
   "source": [
    "### Background: Region Proposal \n",
    "\n",
    "**Region proposal algorithms** provide candidate bounding boxes where objects are likely located.\n",
    "\n",
    "**Advantages:**\n",
    "- Fast: generates ~2000 regions in seconds on CPU\n",
    "- Reduces search space from billions to thousands of candidates\n",
    "- No neural network required (traditional computer vision)\n",
    "\n",
    "**Example: Selective Search**\n",
    "\n",
    "1. **Over-segment** the image into many small regions (via graph-based segmentation)\n",
    "2. **Iteratively merge** similar regions based on color, texture, size, and shape\n",
    "3. Creates a **hierarchical structure** capturing objects at various scales\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/SelectiveS.png\" width=\"695\"/>\n",
    "<p><i>Selective Search: from fine-grained segments to object-level proposals</i></p>\n",
    "</div>\n",
    "\n",
    "**Output:** ~2000 region proposals per image (vs. 2.5 billion sliding windows!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e13dc",
   "metadata": {},
   "source": [
    "## R-CNN: Region-Based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63777bc2",
   "metadata": {},
   "source": [
    "**Key Idea** Use region proposals + CNN features + Classifiers to detect multiple objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fd3d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Step-by-Step Process**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step1RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "#### **Step 1: Generate Region Proposals**\n",
    "\n",
    "- Use **Selective Search** on input image\n",
    "- Generates ~2000 region proposals (candidate bounding boxes)\n",
    "- These are locations where objects *might* be\n",
    "\n",
    "**Input:** Image $(H, W, 3)$  \n",
    "**Output:** ~2000 regions $\\{R_1, R_2, \\ldots, R_{2000}\\}$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step2RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Warp Regions to Fixed Size**\n",
    "\n",
    "- Each region has different size/aspect ratio\n",
    "- CNN requires fixed input size (e.g., $224 \\times 224$ for AlexNet)\n",
    "- **Warp** (resize) each region to $224 \\times 224$\n",
    "\n",
    "**Why?** CNNs need fixed-size inputs\n",
    "\n",
    "**Case 1: Too Large:** Use (bilinear, bicubic) interpolation to reduce pixel count<br>\n",
    "**Case 2: Too Small:** Use interpolation to create new (estimated) pixels\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step3RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Extract CNN Features**\n",
    "\n",
    "- Pass each warped region through a **pre-trained CNN** (e.g., AlexNet)\n",
    "- Extract features from the last fully connected layer\n",
    "- Get a **4096-dimensional feature vector** per region\n",
    "\n",
    "**For each region $R_i$:**\n",
    "\n",
    "$$\\mathbf{f}_i = \\text{CNN}(R_i) \\in \\mathbb{R}^{4096}$$\n",
    "\n",
    "**Result:** 2000 feature vectors, one per region\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step4RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Classify Each Region**\n",
    "\n",
    "- Train **class-specific linear SVMs** (one per class)\n",
    "- Each SVM takes the 4096-dim feature vector as input\n",
    "- Outputs: **class scores** for each region\n",
    "\n",
    "**For region $i$ and class $c$:**\n",
    "\n",
    "$$\\text{score}_{i,c} = \\mathbf{w}_c^T \\mathbf{f}_i + b_c$$\n",
    "\n",
    "**Output:** Class probabilities for each of the 2000 regions\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step5RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Bounding Box Regression**\n",
    "\n",
    "- Initial proposals are rough\n",
    "- Train a **linear regressor** to refine bounding box coordinates\n",
    "- Learns to adjust $(x, y, w, h)$ for better localization\n",
    "\n",
    "**For each region $i$:**\n",
    "\n",
    "$$(\\Delta x, \\Delta y, \\Delta w, \\Delta h) = \\text{Regressor}(\\mathbf{f}_i)$$\n",
    "\n",
    "**Refined box:**\n",
    "\n",
    "$$\\hat{x} = x + \\Delta x, \\quad \\hat{y} = y + \\Delta y, \\quad \\hat{w} = w \\cdot e^{\\Delta w}, \\quad \\hat{h} = h \\cdot e^{\\Delta h}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step6RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 6: Non-Maximum Suppression (NMS)**\n",
    "\n",
    "**Problem:** Multiple overlapping boxes detect the same object\n",
    "\n",
    "**Solution:** Keep only the highest-scoring box, remove overlaps\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sort boxes by confidence score (descending)\n",
    "2. Select box with highest score\n",
    "3. Remove all boxes with $\\text{IoU} > 0.5$ with selected box\n",
    "4. Repeat until no boxes remain\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/B4MNS.png\" width=\"400\"/>\n",
    "<img src=\"../images/chap8/AfterMNS.png\" width=\"400\"/>\n",
    "<p><i>Before NMS (left) vs After NMS (right)</i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Training**\n",
    "\n",
    "**Three-stage training process:**\n",
    "\n",
    "| **Stage** | **What's Trained** | **Loss Function** |\n",
    "|-----------|-------------------|-------------------|\n",
    "| **1. Pre-train CNN** | CNN backbone on ImageNet | Classification loss |\n",
    "| **2. Fine-tune CNN** | CNN on detection dataset | Classification loss |\n",
    "| **3. Train SVMs** | Class-specific SVMs | Hinge loss |\n",
    "| **4. Train bbox regressor** | Bounding box refinement | L2 loss |\n",
    "\n",
    "**Training data:**\n",
    "- **Positive examples:** $\\text{IoU} \\geq 0.5$ with ground truth\n",
    "- **Negative examples:** $\\text{IoU} < 0.3$ with ground truth\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Mathematical Summary**\n",
    "\n",
    "**Given image $I$:**\n",
    "\n",
    "1. **Region proposals:** $\\{R_1, \\ldots, R_N\\} = \\text{SelectiveSearch}(I)$ where $N \\approx 2000$\n",
    "\n",
    "2. **CNN features:** $\\mathbf{f}_i = \\text{CNN}(\\text{Warp}(R_i)) \\in \\mathbb{R}^{4096}$\n",
    "\n",
    "3. **Classification scores:** $s_{i,c} = \\text{SVM}_c(\\mathbf{f}_i)$ for class $c$\n",
    "\n",
    "4. **Bbox refinement:** $\\hat{b}_i = b_i + \\text{Regressor}(\\mathbf{f}_i)$\n",
    "\n",
    "5. **NMS:** Keep boxes with $\\text{IoU} < \\tau$ (typically $\\tau = 0.5$)\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Performance**\n",
    "\n",
    "**Results on PASCAL VOC 2010:**\n",
    "- **mAP (mean Average Precision):** 53.7%\n",
    "- Improvement over previous best: +30% relative gain\n",
    "- First method to successfully apply CNNs to object detection\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages & Limitations**\n",
    "\n",
    "| **✅ Advantages** | **❌ Limitations** |\n",
    "|-------------------|-------------------|\n",
    "| Significant accuracy improvement | **Very slow:** ~47 seconds per image |\n",
    "| Leverages pre-trained CNNs | **Redundant computation:** CNN runs 2000 times |\n",
    "| Simple pipeline | **Multi-stage training:** CNN, SVM, regressor trained separately |\n",
    "| Works with any CNN backbone | **High memory usage:** Must cache features for all regions |\n",
    "| Pioneered region-based detection | **Fixed region proposals:** Cannot learn better proposals |\n",
    "\n",
    "**Key bottleneck:** Running CNN 2000 times per image!\n",
    "\n",
    "**This motivates Fast R-CNN** → Share CNN computation across regions\n",
    "\n",
    "---\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
