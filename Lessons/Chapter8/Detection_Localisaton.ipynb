{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c959678",
   "metadata": {},
   "source": [
    "# Detection and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c30a0b",
   "metadata": {},
   "source": [
    "In the previous topics in this chapter we've been focusing on:\n",
    "- The construction of CNN\n",
    "- The Architectures of CNN\n",
    "- Classifcation using CNN\n",
    "- Feature Visualisation\n",
    "\n",
    "What is we wanted to classfiy multiple object in an image input, and also identify (via a tight bounding box around the object) the location of the classified object. \n",
    "\n",
    "**Objective**\n",
    "\n",
    "1. Single object Detection\n",
    "2. Multiple Object Detection:\n",
    "   - R-CNN\n",
    "   - Fast R-CNN\n",
    "   - Faster R-CNN\n",
    "3. Single Stage Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c52e88",
   "metadata": {},
   "source": [
    "## Single Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cbd3b",
   "metadata": {},
   "source": [
    "\n",
    "### Steps\n",
    "\n",
    "1. Choose a backbone network such as AlexNet/Resnet/DenseNet etc.\n",
    "2. From a FC create two outputs\n",
    "    - One for Classifcation (Vector of 4096 $\\to$ 1000)\n",
    "    - Box Coordinate (Vector of 4096 $\\to$ 4)\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/SingleObjDet.png\" width=\"710\"/>\n",
    "</div>\n",
    "\n",
    "#### Loss\n",
    "\n",
    "| **Input** | **Task** | **Output** | **Loss Function** |\n",
    "|-----------|----------|------------|-------------------|\n",
    "| (H, W, 3) | **Classification** | Class label $c \\in \\{1, \\ldots, C\\}$ | Cross-Entropy Loss: $-\\log p_c$ |\n",
    "| (H, W, 3) | **Localization** | Bounding box $(x, y, w, h)$ | L2 Loss: $\\sum (b_i - \\hat{b}_i)^2$ or Smooth L1 |\n",
    "\n",
    "\n",
    "Our Total Loss is computed as follows: \n",
    "\n",
    "$$L(y, P, b, b') = -\\log P(y) + \\lambda L_{regression}(b, b')$$\n",
    "\n",
    "### Evaluate \n",
    "\n",
    "- For **Classification**, we evaluate using the **accuracy** metric \n",
    "- For **Localization**, we compute the **Intersection over Union (IoU)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intersection over Union (IoU)**\n",
    "\n",
    "$$\\text{IoU}(A, B) = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}} = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "**Where:**\n",
    "- $A$ = predicted bounding box area\n",
    "- $B$ = ground truth bounding box area  \n",
    "- $A \\cap B$ = overlapping region (intersection)\n",
    "- $A \\cup B$ = total covered area (union)\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $\\text{IoU} \\in [0, 1]$\n",
    "  - $\\text{IoU} = 0$ → no overlap (completely wrong prediction)\n",
    "  - $\\text{IoU} = 1$ → perfect overlap (exact match)\n",
    "- **Common threshold:** $\\text{IoU} > 0.5$ is considered a \"good\" detection\n",
    "  - $\\text{IoU} \\geq 0.5$ → True Positive (TP)\n",
    "  - $\\text{IoU} < 0.5$ → False Positive (FP)\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/IOUCalc.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac0780",
   "metadata": {},
   "source": [
    "## Multi-Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba5f9c",
   "metadata": {},
   "source": [
    "$\\text{Input} \\ (W, H, 3)$\n",
    "\n",
    "$\\text{Output} \\ \\{(C_1, (x_1, y_1, w_1, h_1)), (C_2, (x_2, y_2, w_2, h_2)) \\dots (C_k, (x_k, y_k, w_k, h_k))\\}$\n",
    "\n",
    "Where $k$ is the number of objects to identify in the image\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/obj1.png\" width=\"295\"/>\n",
    "<img src=\"../images/chap8/obj2.png\" width=\"300\"/>\n",
    "<img src=\"../images/chap8/obj4.png\" width=\"335\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Challanges\n",
    "\n",
    "1. **Multiple Outputs** - Number of objects changes per image\n",
    "2. **Multiple types of Outputs** - We need to answer: \"what\" and \"where\" for every object\n",
    "3. **Multiple Lengths of outputs** - \n",
    "4. **Multiple Size** - Objects vary in size\n",
    "5. **Multiple Detections** - Objects can be detected multiple times\n",
    "6. **Occlusions** - Objects can hide part/total aspects of other objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160cb8b",
   "metadata": {},
   "source": [
    "### Naïve Approach - Sliding Window\n",
    "\n",
    "1. Apply CNN classification to many different crops of the image\n",
    "2. Classify each crop as **background** or a specific **object class**\n",
    "3. Vary the crop size and position to detect objects at different scales and locations\n",
    "\n",
    "**Complexity Issue**\n",
    "\n",
    "To create a bounding box, we need 2 corner points (4 coordinates total): $(x_1, y_1, x_2, y_2)$\n",
    "\n",
    "**Question:** Given an image of size $H \\times W$, how many different possible bounding boxes can be created?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Each bounding box is defined by:\n",
    "- Top-left corner: $(x_1, y_1)$ where $x_1 \\in \\{0, \\ldots, W-1\\}$, $y_1 \\in \\{0, \\ldots, H-1\\}$\n",
    "- Bottom-right corner: $(x_2, y_2)$ where $x_2 \\in \\{x_1+1, \\ldots, W\\}$, $y_2 \\in \\{y_1+1, \\ldots, H\\}$\n",
    "\n",
    "**Number of possible boxes:**\n",
    "\n",
    "$$\\binom{H}{2} \\times \\binom{W}{2} = \\frac{H(H-1)}{2} \\times \\frac{W(W-1)}{2} = O(H^2 W^2)$$\n",
    "\n",
    "**Why this is a problem:**\n",
    "- For a $224 \\times 224$ image: $\\approx 2.5 \\times 10^9$ possible boxes!\n",
    "- Running CNN on each crop is **computationally infeasible**\n",
    "- Need ~2.5 billion forward passes per image\n",
    "\n",
    "**This motivates region proposal methods (R-CNN, Fast R-CNN, Faster R-CNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55474003",
   "metadata": {},
   "source": [
    "### Background: Region Proposal \n",
    "\n",
    "**Region proposal algorithms** provide candidate bounding boxes where objects are likely located.\n",
    "\n",
    "**Advantages:**\n",
    "- Fast: generates ~2000 regions in seconds on CPU\n",
    "- Reduces search space from billions to thousands of candidates\n",
    "- No neural network required (traditional computer vision)\n",
    "\n",
    "**Example: Selective Search**\n",
    "\n",
    "1. **Over-segment** the image into many small regions (via graph-based segmentation)\n",
    "2. **Iteratively merge** similar regions based on color, texture, size, and shape\n",
    "3. Creates a **hierarchical structure** capturing objects at various scales\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/SelectiveS.png\" width=\"695\"/>\n",
    "<p><i>Selective Search: from fine-grained segments to object-level proposals</i></p>\n",
    "</div>\n",
    "\n",
    "**Output:** ~2000 region proposals per image (vs. 2.5 billion sliding windows!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e13dc",
   "metadata": {},
   "source": [
    "## R-CNN: Region-Based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63777bc2",
   "metadata": {},
   "source": [
    "**Key Idea** Use region proposals + CNN features + Classifiers to detect multiple objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fd3d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Step-by-Step Process**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step1RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "#### **Step 1: Generate Region Proposals**\n",
    "\n",
    "- Use **Selective Search** on input image\n",
    "- Generates ~2000 region proposals (candidate bounding boxes)\n",
    "- These are locations where objects *might* be\n",
    "\n",
    "**Input:** Image $(H, W, 3)$  \n",
    "**Output:** ~2000 regions $\\{R_1, R_2, \\ldots, R_{2000}\\}$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step2RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Warp Regions to Fixed Size**\n",
    "\n",
    "- Each region has different size/aspect ratio\n",
    "- CNN requires fixed input size (e.g., $224 \\times 224$ for AlexNet)\n",
    "- **Warp** (resize) each region to $224 \\times 224$\n",
    "\n",
    "**Why?** CNNs need fixed-size inputs\n",
    "\n",
    "**Case 1: Too Large:** Use (bilinear, bicubic) interpolation to reduce pixel count<br>\n",
    "**Case 2: Too Small:** Use interpolation to create new (estimated) pixels\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step3RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Extract CNN Features**\n",
    "\n",
    "- Pass each warped region through a **pre-trained CNN** (e.g., AlexNet)\n",
    "- Extract features from the last fully connected layer\n",
    "- Get a **4096-dimensional feature vector** per region\n",
    "\n",
    "**For each region $R_i$:**\n",
    "\n",
    "$$\\mathbf{f}_i = \\text{CNN}(R_i) \\in \\mathbb{R}^{4096}$$\n",
    "\n",
    "**Result:** 2000 feature vectors, one per region\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step4RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Classify Each Region**\n",
    "\n",
    "- Train **class-specific linear SVMs** (one per class)\n",
    "- Each SVM takes the 4096-dim feature vector as input\n",
    "- Outputs: **class scores** for each region\n",
    "\n",
    "**For region $i$ and class $c$:**\n",
    "\n",
    "$$\\text{score}_{i,c} = \\mathbf{w}_c^T \\mathbf{f}_i + b_c$$\n",
    "\n",
    "**Output:** Class probabilities for each of the 2000 regions\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step5RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Bounding Box Regression**\n",
    "\n",
    "- Initial proposals are rough\n",
    "- Train a **linear regressor** to refine bounding box coordinates\n",
    "- Learns to adjust $(x, y, w, h)$ for better localization\n",
    "\n",
    "**For each region $i$:**\n",
    "\n",
    "$$(\\Delta x, \\Delta y, \\Delta w, \\Delta h) = \\text{Regressor}(\\mathbf{f}_i)$$\n",
    "\n",
    "**Refined box:**\n",
    "\n",
    "$$\\hat{x} = x + \\Delta x, \\quad \\hat{y} = y + \\Delta y, \\quad \\hat{w} = w \\cdot e^{\\Delta w}, \\quad \\hat{h} = h \\cdot e^{\\Delta h}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step6RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 6: Non-Maximum Suppression (NMS)**\n",
    "\n",
    "**Problem:** Multiple overlapping boxes detect the same object\n",
    "\n",
    "**Solution:** Keep only the highest-scoring box, remove overlaps\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sort boxes by confidence score (descending)\n",
    "2. Select box with highest score\n",
    "3. Remove all boxes with $\\text{IoU} > 0.5$ with selected box\n",
    "4. Repeat until no boxes remain\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/B4MNS.png\" width=\"400\"/>\n",
    "<img src=\"../images/chap8/AMNS.png\" width=\"400\"/>\n",
    "<p><i>Before NMS (left) vs After NMS (right)</i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Training**\n",
    "\n",
    "**Three-stage training process:**\n",
    "\n",
    "| **Stage** | **What's Trained** | **Loss Function** |\n",
    "|-----------|-------------------|-------------------|\n",
    "| **1. Pre-train CNN** | CNN backbone on ImageNet | Classification loss |\n",
    "| **2. Fine-tune CNN** | CNN on detection dataset | Classification loss |\n",
    "| **3. Train SVMs** | Class-specific SVMs | Hinge loss |\n",
    "| **4. Train bbox regressor** | Bounding box refinement | L2 loss |\n",
    "\n",
    "**Training data:**\n",
    "- **Positive examples:** $\\text{IoU} \\geq 0.5$ with ground truth\n",
    "- **Negative examples:** $\\text{IoU} < 0.3$ with ground truth\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Mathematical Summary**\n",
    "\n",
    "**Given image $I$:**\n",
    "\n",
    "1. **Region proposals:** $\\{R_1, \\ldots, R_N\\} = \\text{SelectiveSearch}(I)$ where $N \\approx 2000$\n",
    "\n",
    "2. **CNN features:** $\\mathbf{f}_i = \\text{CNN}(\\text{Warp}(R_i)) \\in \\mathbb{R}^{4096}$\n",
    "\n",
    "3. **Classification scores:** $s_{i,c} = \\text{SVM}_c(\\mathbf{f}_i)$ for class $c$\n",
    "\n",
    "4. **Bbox refinement:** $\\hat{b}_i = b_i + \\text{Regressor}(\\mathbf{f}_i)$\n",
    "\n",
    "5. **NMS:** Keep boxes with $\\text{IoU} < \\tau$ (typically $\\tau = 0.5$)\n",
    "\n",
    "---\n",
    "\n",
    "### **R-CNN Performance**\n",
    "\n",
    "**Results on PASCAL VOC 2010:**\n",
    "- **mAP (mean Average Precision):** 53.7%\n",
    "- Improvement over previous best: +30% relative gain\n",
    "- First method to successfully apply CNNs to object detection\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages & Limitations**\n",
    "\n",
    "| **✅ Advantages** | **❌ Limitations** |\n",
    "|-------------------|-------------------|\n",
    "| Significant accuracy improvement | **Very slow:** ~47 seconds per image |\n",
    "| Leverages pre-trained CNNs | **Redundant computation:** CNN runs 2000 times |\n",
    "| Simple pipeline | **Multi-stage training:** CNN, SVM, regressor trained separately |\n",
    "| Works with any CNN backbone | **High memory usage:** Must cache features for all regions |\n",
    "| Pioneered region-based detection | **Fixed region proposals:** Cannot learn better proposals |\n",
    "\n",
    "**Key bottleneck:** Running CNN 2000 times per image!\n",
    "\n",
    "**This motivates Fast R-CNN** → Share CNN computation across regions\n",
    "\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce950d",
   "metadata": {},
   "source": [
    "## Fast R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd129663",
   "metadata": {},
   "source": [
    "**Key Idea** Run the whole image through a CNN before obtaining region proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172826a",
   "metadata": {},
   "source": [
    "### **Step-by-Step Process**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step1RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Extract CNN Features**\n",
    "\n",
    "- Pass the **whole image** through a **pre-trained CNN** (e.g., AlexNet)\n",
    "- Extract features from the **last convolutional Layer**\n",
    "- Get a **512 features/Channels of 20x15**\n",
    "\n",
    "$$\\mathbf{f} = \\text{CNN}(R) \\in \\mathbb{R}^{512 \\times 20 \\times 15}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/imcovNet.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Generate Region Proposals**\n",
    "\n",
    "- Use **Selective Search** on input image\n",
    "- Generates ~2000 region proposals (candidate bounding boxes)\n",
    "- These are locations where objects *might* be\n",
    "\n",
    "**Input:** Image $(H, W, 3)$  \n",
    "**Output:** ~2000 regions $\\{R_1, R_2, \\ldots, R_{2000}\\}$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step2RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Matching Region dimensions in Feature Space: ROI Pooling and RoI Align**\n",
    "\n",
    "We assume:\n",
    "- Input image: $$ (3, H, W) $$\n",
    "- Feature map from backbone: $$ (C, H_f, W_f) $$\n",
    "\n",
    "**Running Example**\n",
    "\n",
    "$$\n",
    "\\text{Image size} = (3, 640, 480)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Feature map} = (512, 20, 15)\n",
    "$$\n",
    "\n",
    "- Each $R_i$ is defined by $(x_{i1}, y_{i1}, x_{i2}, y_{i2})$ in the **original image**. \n",
    "- **Compute the scaling factors:**\n",
    "  - $x_{scale} = \\frac{\\text{feature map width}}{\\text{image width}}$\n",
    "  - $y_{scale} = \\frac{\\text{feature map height}}{\\text{image height}}$\n",
    "    - In our example:\n",
    "    - $x_{\\text{scale}} = \\frac{15}{480} = \\frac{1}{32}$\n",
    "    - $y_{\\text{scale}} = \\frac{20}{640} = \\frac{1}{32}$\n",
    "- **Map RoI to Feature Map**\n",
    "  - $x_{i1}’ = x_{i1} \\cdot x_{\\text{scale}}, \\quad y_{i1}’ = y_{i1} \\cdot y_{\\text{scale}}$\n",
    "  - $x_{i2}’ = x_{i2} \\cdot x_{\\text{scale}}, \\quad y_{i2}’ = y_{i2} \\cdot y_{\\text{scale}}$\n",
    "  - This gives a region in feature map coordinates\n",
    "- **Crop the region**\n",
    "  - Use the scaled coordinates $(x_{i1}', y_{i1}', x_{i2}', y_{i2}')$ to select the corresponding rectangle from the feature map for all channel.\n",
    "  - Producing $$R_i’ \\in \\mathbb{R}^{C \\times h_i’ \\times w_i’}$$\n",
    "  - EAch spatial location contains a **C-dimensional feature vector**\n",
    "- **Divide into Fixed Grid**\n",
    "  - Divide $R_i’$ into fixed bins (e.g. $7 \\times 7$).\n",
    "  - Each bin size: \n",
    "    - $\\text{bin width} = \\frac{x_2’ - x_1’}{7}$\n",
    "    - $\\text{bin height} = \\frac{y_2’ - y_1’}{7}$\n",
    "- **RoI Pooling**\n",
    "  - For each bin: \n",
    "    1. Round bin boundaries to integer indices\n",
    "    2. Take all features values inside bin\n",
    "    3. Apply max pooling\n",
    "  - **Output** For N RoIs $(N, C, 7, 7)$\n",
    "\n",
    "#### RoI Pooling\n",
    "\n",
    "The issue is **quantization error** cause by rounding (step 1 in RoI Pooling).\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let the Feature Map size: $20 \\times 15$\n",
    "\n",
    "Our Mapped RoI: $(x_1’, y_1’, x_2’, y_2’) = (2.3, 3.7, 10.8, 9.2)$\n",
    "\n",
    "The Width: $10.8-2.3=8.5$\n",
    "\n",
    "The Hight: $9.2 - 3.7 = 5.5$\n",
    "\n",
    "Suppose we divide into 2x2 bins then:\n",
    "\n",
    "- Bin Width: $\\frac{8.5}{2} = 4.25$\n",
    "- bin Height: $\\frac{5.5}{2} = 2.75$\n",
    "\n",
    "Our boundaries are:\n",
    "\n",
    "- Top Left: $x \\in [2.3, 6.55] \\quad y \\in [6.45, 9.2]$\n",
    "- Top Right: $x \\in [6.55, 10.8] \\quad y \\in [6.45, 9.2]$\n",
    "- Bottom Left: $x \\in [2.3, 6.55] \\quad y \\in [3.7, 6.45]$\n",
    "- Bottom Right:$x \\in [6.55, 10.8] \\quad y \\in [3.7, 6.45]$\n",
    "  \n",
    "**What RoI Pooling does:**\n",
    "\n",
    "- Top Left: $x \\in [2.3 \\rightarrow 2, 6.55 \\rightarrow 7] \\quad y \\in [6.45 \\rightarrow 6, 9.2 \\rightarrow 9]$\n",
    "- Top Right: $x \\in [6.55 \\rightarrow 7 , 10.8 \\rightarrow 11] \\quad y \\in [6.45 \\rightarrow 6, 9.2 \\rightarrow 9]$\n",
    "- Bottom Left: $x \\in [2.3 \\rightarrow 2, 6.55 \\rightarrow 7] \\quad y \\in [3.7 \\rightarrow 4, 6.45 \\rightarrow 6]$\n",
    "- Bottom Right:$x \\in [6.55 \\rightarrow 7 , 10.8 \\rightarrow 11] \\quad y \\in [3.7 \\rightarrow 4, 6.45 \\rightarrow 6]$\n",
    "\n",
    "This causes spatial misalignment.\n",
    "\n",
    "#### RoI Align\n",
    "\n",
    "- **Do NOT Round coordinates**\n",
    "  - Our Mapped RoI: $(x_1’, y_1’, x_2’, y_2’) = (2.3, 3.7, 10.8, 9.2)$\n",
    "- **Divide into Exact Floating bins**\n",
    "  - Bin Width: $\\frac{8.5}{2} = 4.25$\n",
    "  - bin Height: $\\frac{5.5}{2} = 2.75$\n",
    "- **Sampling:** Choose sampling point(s) inside each bin (Often the center of the bin)\n",
    "  -  Top Left: $(5.575, 5.975)$\n",
    "  -  Top Right: $(7.525,  5.975)$\n",
    "  -  Bottom Left: $(5.575, 4.6)$\n",
    "  -  Bottom Right: $(7.525, 4.6)$\n",
    "- **Bilinear Interpolation** since the feature map only exists at interger grid points compute: \n",
    "  - Top Left: $i = \\lfloor 5.575 \\rfloor = 5 \\quad j = \\lfloor 5.975 \\rfloor = 5 \\\\ \\alpha = 5.575 - 5= 0.575 \\quad \\beta = 5.975 - 5 = 0.975 \\\\ \\text{The neares integer neighbors are: } (5, 5), (5, 6), (6, 5), (6,6) \\\\ \\text{Interpolate:} \\qquad F(x,y) = (1-\\alpha)(1-\\beta)F(5,5) \\cdot \\alpha(1-\\beta)F(5,5) \\cdot (1-\\alpha)\\beta F(6,5) \\cdot \\alpha\\beta F(6,6) \\\\ \\text{Where } F(x,y) \\text{ is the value of the feature map at spatial location } (x, y)$\n",
    "- **Aggregate:** If multiple sampling points are used per bin: \n",
    "  - Average them \n",
    "  - Retreive Max\n",
    "\n",
    "**Final Output** $(C, 7, 7)$ or $(C, 2, 2)$ in this example\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/Biinterpolation.png\" width=\"450\"/>\n",
    "<img src=\"../images/chap8/ROIPool.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "Note:\n",
    "\n",
    "Generally, $\\alpha$ and $\\beta \\in [0,1]$ are calculated as follows: \n",
    "\n",
    "$x_1 = \\lfloor x \\rfloor \\quad x_2 = \\lfloor y \\rfloor$\n",
    "\n",
    "$\\alpha = \\frac{x - x_1}{x_2 - x_1} \\quad \\beta = \\frac{y - y_1}{y_2 - y_1}$\n",
    "\n",
    "Since $x_2 = x_1 + 1 \\rightarrow x_2 - x_1 = 1$ so it simplifies to $\\alpha = x - x_1$\n",
    "\n",
    "For more information on Bilinear interpolation: https://github.com/yossefPartouche/Computer_Graphics/blob/main/Unit%201/L1.6_Anti-Aliasing.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4:Classify and Regress**\n",
    "\n",
    "- **Flatten** each pooled feature map.\n",
    "- **Pass through fully connected layers (2 Layers)** (shared for all RoIs).\n",
    "- **Output two heads**:\n",
    "  - **Classification head:** prediciting the class for each RoI.\n",
    "  - **Regression head:** Predicting the transform for each RoI.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/FRCNNpred.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5c79c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "### R-CNN vs. Fast R-CNN\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/chap8/RCNNvsFRCNN.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Key bottleneck:** Runtime is dominated by the Region Proposals\n",
    "\n",
    "**This motivates Faster R-CNN** Which uses a Region Proposal Network, to predict RP from CNN features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e39efd",
   "metadata": {},
   "source": [
    "## Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff3eef",
   "metadata": {},
   "source": [
    "**Key Idea** \n",
    "\n",
    "After the feature mapping (like the previous networks), we branch out (like the previous networks), but the bounding box branch becomes another small network to produce region proposal (unlike the previous networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff64acb",
   "metadata": {},
   "source": [
    "### **Step-by-Step Process**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step1RCNN.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Extract CNN Features**\n",
    "\n",
    "- Pass the **whole image** through a **pre-trained CNN** (e.g., AlexNet)\n",
    "- Extract features from the **last convolutional Layer**\n",
    "- Get a **$C_{in}$ features/Channels of WxH**\n",
    "\n",
    "$$CNN(\\text{Image}) = F \\in \\mathbb{R}^{C_{in} \\times H_f \\times W_f}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/imcovNet.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Define K Achor Boxes**\n",
    "\n",
    "Anchor boxes are predefined reference bounding boxes placed at each spatial location in the feature map.\n",
    "\n",
    "- They serve as boxes for detecting objects of different scales and aspect ratios.\n",
    "- At each spatial location in the feature map well try to fit these predefined achor boxes.\n",
    "- K is a hyperpameter\n",
    "\n",
    "Spatial location: is a single entry (i,j) across all channels.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/spatialLoc.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: 3x3 Convolution**\n",
    "\n",
    "- **Apply 3x3 convolution**\n",
    "   - This is to increase the receptive field of the network and not rely on the a single pixel for context.\n",
    "   - $F' = Conv_{3 \\times 3}(F)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Branch into Two Heads**\n",
    "From $F'$ we branch into two 1x1 convolutions\n",
    "1. **Classification/Objectness Head:** \n",
    "   - $\\text{Conv}_{1 \\times 1} = W_{cls} \\in \\mathbb{R}^{2k \\times C_{in} \\times 1 \\times 1}$\n",
    "   - **Output** $F_{1}'' \\in \\mathbb{R}^{2k \\times H_f \\times W_f}$\n",
    "2. **Regression Head:** \n",
    "   - $\\text{Conv}_{1 \\times 1} = W_{reg} \\in \\mathbb{R}^{4k \\times C_{in} \\times 1 \\times 1}$\n",
    "   - **Output** $F_{2}'' \\in \\mathbb{R}^{4k \\times H_f \\times W_f}$\n",
    "\n",
    "---\n",
    "  \n",
    "#### **Step 5: Compute Objectness Scores + BB Proposals**\n",
    "\n",
    "The output of the RPN's two head, can be thought of as scores (like a classic network). \n",
    "\n",
    "- Classifiction Score\n",
    "- Regression Score\n",
    "\n",
    "**Step 5a: Map Channel to Anchors**\n",
    "\n",
    "- Each spatial Location (i, j) in the feature map corresponds to a receptive field in the input image.\n",
    "- At each location, we have k-anchors, the channels of the heads are assigned to anchors in a fixed mapping.\n",
    "- We extract: $F'_{cls}[:, i, j] \\in \\mathbb{R}^{2k}$ and $F'_{reg}[:, i, j] \\in \\mathbb{R}^{4k}$ which are grouped per anchor.\n",
    "  \n",
    "  <div align=\"center\">\n",
    "\n",
    "  | **Head** | **Anchor** | **Channel Indices** | \n",
    "  |----------|------------|---------------------|\n",
    "  | Classification | 1 | 0-1|\n",
    "  | Classification | 2 | 2-3|\n",
    "  | $\\dots$ | $\\dots$ | $\\dots$|\n",
    "  | Classification | k | 2k-2, 2k-1 | \n",
    "  | Regression | 1 | 0-3|\n",
    "  | Regression | 2 | 4-7| \n",
    "  | $\\dots$ | $\\dots$ | $\\dots$|\n",
    "  | Regression | k | 4k - 4 $\\dots$ 4k-1|\n",
    "</div>\n",
    "\n",
    "**Step 5b: Compute Objectness Scores**\n",
    "\n",
    "1. **Extract Anchor scores**: For anchor $a \\in {1, \\dots, k}$ extract $[s_{bg}^{(a)}, s_{obj}^{(a)}]$.\n",
    "2. **Convert to probabilities**: Apply softamx: $p_{obj}^{(a)} = \\frac{\\exp(s_{obj}^{(a)})}{\\exp(s_{obj}^{(a)}) + \\exp(s_{bg}^{(a)})}$\n",
    "   - So we first iterate through each anchor at this location and compute the probability and then change locations.\n",
    "3. **Assign training labels using IoU**: \n",
    "   - Compute IoU between each anchor and all ground-truth boxes\n",
    "\n",
    "<div align=\"center\">\n",
    "  \n",
    "   |IoU condition | Label | Contributes to loss? | \n",
    "   |--------------|-------|----------------------|\n",
    "   |Highest IoU with a GT box| Positive (1) | Yes (classification + regression) | \n",
    "   | IoU ≥ 0.7 with any GT | Positive (1) | Yes |\n",
    "   | IoU ≤ 0.3 with all GT | Negative (0) | Yes (classification only) | \n",
    "   | 0.3 < IoU < 0.7 | Ignore | No | \n",
    "</div>\n",
    "\n",
    "4. **Compute Loss (CE)**:\n",
    "   - For each anchor used in training: $L_\\text{cls}^{(a)} = - \\Big[ y^{(a)} \\log p_{obj}^{(a)} + (1 - y^{(a)}) \\log p_{bg}^{(a)} \\Big]$ \n",
    "5. **Full image Classification Loss**:\n",
    "   -  $L_\\text{cls} = \\frac{1}{N_\\text{anchors}} \\sum_{a \\in \\text{used anchors}} L_\\text{cls}^{(a)}$\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```spatial location → anchor → IoU → label → softmax → CE loss```\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**Step 5c: Regression Per Anchor**\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap8/step5c.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "1. **Extract predicted offsets** \n",
    "   - For anchor $a \\in {1, \\dots, k}$ extract $[t_x^{(a)}, t_y^{(a)}, t_w^{(a)}, t_h^{(a)}]$.\n",
    "2. **Apply predicted offsets to produce RP**\n",
    "  - $x_{pred} = x_{anchor} + t_x^{(a)} \\cdot w_{anchor}$\n",
    "  - $y_{pred} = y_{anchor} + t_y^{(a)} \\cdot h_{anchor}$\n",
    "  - $w_{pred} = w_{anchor} \\cdot e^{t_w^{(a)}} $\n",
    "  - $h_{pred} = h_{anchor} \\cdot e^{t_h^{(a)}}$\n",
    "3. **Compute training targets for positice anchors:**\n",
    "   - Only **Postive** labels from 5b are used in regression.\n",
    "   - Target offsets are computed relative to the matched **ground truth** box $(x_\\text{gt}, y_\\text{gt}, w_\\text{gt}, h_\\text{gt})$\n",
    "   - $t_x^* = \\frac{x_\\text{gt} - x_\\text{anchor}}{w_\\text{anchor}}$\n",
    "   - $t_y^* = \\frac{y_\\text{gt} - y_\\text{anchor}}{h_\\text{anchor}}$\n",
    "   - $t_w^* = \\log \\frac{w_\\text{gt}}{w_\\text{anchor}}$\n",
    "   - $t_h^* = \\log \\frac{h_\\text{gt}}{h_\\text{anchor}}$\n",
    "- **Compute Regression Loss:**\n",
    "  - $L_\\text{reg} = \\text{SmoothL1}\\big([t_x^{(a)}, t_y^{(a)}, t_w^{(a)}, t_h^{(a)}] - [t_x^*, t_y^*, t_w^*, t_h^*]\\big)$\n",
    "  - Full regression Loss: $L_\\text{reg} = \\frac{1}{N_\\text{pos}} \\sum_{a \\in \\text{positive anchors}} L_\\text{reg}^{(a)}$\n",
    "\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dbbdb",
   "metadata": {},
   "source": [
    "## Faster R-CNN Optimization: Single Stage Object Detection (SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0a4d2",
   "metadata": {},
   "source": [
    "In the step 4 of the RP, were created 2k classification channels, to determine if and object existed or not, but we can directly determine **which** object it is including determinining if it's a background object!\n",
    "\n",
    "**Reminder**\n",
    "\n",
    "In chapter 4 we discussed **Multi-Class Classification**, so we can just extend the idea in this network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a10bd",
   "metadata": {},
   "source": [
    "#### **Step 1: Extract CNN Features (same)** \n",
    "#### **Step 2: Define K Achor Boxes (same)**\n",
    "#### **Step 3: 3x3 Convolution (same)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Branch into Two Heads (Optimized)**\n",
    "From $F'$ we branch into two 1x1 convolutions<br> $\\text{Given } C = \\text{ number of object Classes}$\n",
    "\n",
    "1. **Classification/Objectness Head:** \n",
    "   - $\\text{Conv}_{1 \\times 1} = W_{cls} \\in \\mathbb{R}^{(C+1)k \\times C_{in} \\times 1 \\times 1}$\n",
    "   - **Output** $F_{1}'' \\in \\mathbb{R}^{2k \\times H_f \\times W_f}$\n",
    "2. **Regression Head:** \n",
    "   - $\\text{Conv}_{1 \\times 1} = W_{reg} \\in \\mathbb{R}^{4k \\times C_{in} \\times 1 \\times 1}$\n",
    "   - **Output** $F_{2}'' \\in \\mathbb{R}^{4k \\times H_f \\times W_f}$\n",
    "---\n",
    "\n",
    "#### **Step 5: Compute Objectness Scores + BB Proposals (Optimized)**\n",
    "\n",
    "**Step 5a: Map Channel to Anchors**\n",
    "\n",
    "- Each spatial Location (i, j) in the feature map corresponds to a receptive field in the input image.\n",
    "- At each location, we have k-anchors, the channels of the heads are assigned to anchors in a fixed mapping.\n",
    "- We extract: $F'_{cls}[:, i, j] \\in \\mathbb{R}^{(C+1)k}$ and $F'_{reg}[:, i, j] \\in \\mathbb{R}^{4k}$ which are grouped per anchor.\n",
    "  \n",
    "<div align=\"center\">\n",
    "\n",
    "| **Head**         | **Anchor** | **Channel Indices**      |\n",
    "|------------------|------------|--------------------------|\n",
    "| Classification   | 1          | 0 to C                   |\n",
    "| Classification   | 2          | (C+1) to 2C+1            |\n",
    "| ...              | ...        | ...                      |\n",
    "| Classification   | k          | (k-1)(C+1) to k(C+1)-1   |\n",
    "| Regression       | 1          | 0-3                      |\n",
    "| Regression       | 2          | 4-7                      |\n",
    "| ...              | ...        | ...                      |\n",
    "| Regression       | k          | 4k-4 to 4k-1             |\n",
    "</div>\n",
    "\n",
    "**Step 5b: Compute Objectness Scores**\n",
    "\n",
    "1. **Extract Anchor scores**: For anchor $a \\in {1, \\dots, k}$ extract $[s_{bg}^{(a)}, s_{1}^{(a)}, s_{2}^{(a)} \\dots s_{C}^{(a)}]$.\n",
    "2. **Convert to probabilities**: Apply softmax over all C+1 classes:  \n",
    "For anchor $a$,  \n",
    "$\n",
    "p_{c}^{(a)} = \\frac{\\exp(s_{c}^{(a)})}{\\sum_{j=0}^{C} \\exp(s_{j}^{(a)})}\n",
    "$\n",
    "where $c = 0$ is background, $c = 1, \\dots, C$ are object classes.\n",
    "   - So we first iterate through each anchor at this location and compute the probability and then change locations.\n",
    "3. **Assign training labels using IoU (same)**: \n",
    "   - Compute IoU between each anchor and all ground-truth boxes\n",
    "\n",
    "<div align=\"center\">\n",
    "  \n",
    "   |IoU condition | Label | Contributes to loss? | \n",
    "   |--------------|-------|----------------------|\n",
    "   |Highest IoU with a GT box| Positive (1) | Yes (classification + regression) | \n",
    "   | IoU ≥ 0.7 with any GT | Positive (1) | Yes |\n",
    "   | IoU ≤ 0.3 with all GT | Negative (0) | Yes (classification only) | \n",
    "   | 0.3 < IoU < 0.7 | Ignore | No | \n",
    "</div>\n",
    "\n",
    "4. **Compute Loss (CE)**:\n",
    "   - For each anchor used in training: \n",
    "     $\n",
    "     L_\\text{cls}^{(a)} = -\\sum_{c=0}^{C} y_c^{(a)} \\log p_c^{(a)}\n",
    "     $ <br> where $y_c^{(a)}$ is 1 if anchor $a$'s true class is $c$, else 0, and $p_c^{(a)}$ is the predicted probability for class $c$.\n",
    "5. **Full image Classification Loss**:\n",
    "   -  $L_\\text{cls} = \\frac{1}{N_\\text{anchors}} \\sum_{a \\in \\text{used anchors}} L_\\text{cls}^{(a)}$\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```spatial location → anchor → IoU → label → softmax → CE loss```\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Step 5c: Regression Per Anchor (same)**\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
