{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50817b3",
   "metadata": {},
   "source": [
    "## Encoder: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19443b3",
   "metadata": {},
   "source": [
    "\n",
    "| **Component**         | **BERT Base/Details**                                         |\n",
    "|-----------------------|--------------------------------------------------------------|\n",
    "| **Vocabulary Size**   | 30,000 tokens                                                |\n",
    "| **Embedding Size**    | 1024 per token                                               |\n",
    "| **# Transformer Layers** | 24                                                        |\n",
    "| **Attention Heads/Layer** | 16                                                       |\n",
    "| **Q/K/V per Head**    | Each: $64 \\times 1024$                                       |\n",
    "| **Feedforward Hidden Dim** | 4096                                                    |\n",
    "| **Max Input Length**  | 512 tokens                                                   |\n",
    "| **Batch Size (Pretrain)** | 256                                                      |\n",
    "| **Pretraining Steps** | 1 million (~50 epochs)                                       |\n",
    "| **Pretraining Tasks** | Masked language modeling, next sentence prediction           |\n",
    "| **Fine-tuning Tasks** | Text classification, NER, span prediction (QA), etc.         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9137bf7",
   "metadata": {},
   "source": [
    "Just to gain a breif ideas of what this looks like from the inside, consider a single transformer Layer: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**One Transformer Layer BEGIN**\n",
    "\n",
    "</div>\n",
    "\n",
    "We denote $sa_{lm}$ where $l$ is the $l$ head mechanism out of 16 and $m$ is the \"center of attention\" \n",
    "\n",
    "\n",
    "$$\n",
    "sa_{11} = \\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_1})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_1})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_1}))}}\\right]\\cdot \\left(\\beta_{v_1} + \\Omega_{v1}\\mathbf{x_1}\\right)+ \\dots +\\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_N})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_1})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_1}))}}\\right]\\cdot \\left(\\beta_{v1} + \\Omega_{v1}\\mathbf{x_N}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "sa_{12} = \\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_2})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_1})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_1}))}}\\right]\\cdot \\left(\\beta_{v_1} + \\Omega_{v1}\\mathbf{x_1}\\right)+ \\dots +\\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_N})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_2})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_2}))}}\\right]\\cdot \\left(\\beta_{v1} + \\Omega_{v1}\\mathbf{x_N}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "sa_{1N} = \\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_1})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_N})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{k1}\\mathbf{x_N}))}}\\right]\\cdot \\left(\\beta_{v_1} + \\Omega_{v1}\\mathbf{x_1}\\right)+ \\dots +\\left[\\frac{\\exp{(\\beta_{k1} + \\Omega_{k1}\\mathbf{x_N})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_N})}}{\\sum_{i=1}^N \\exp{((\\beta_{k1} + \\Omega_{k1}\\mathbf{x_i})\\cdot(\\beta_{q1} + \\Omega_{q1}\\mathbf{x_N}))}}\\right]\\cdot \\left(\\beta_{v1} + \\Omega_{v1}\\mathbf{x_N}\\right)\n",
    "$$\n",
    "\n",
    "With this we concatonate the first self-attention mechanism: \n",
    "\n",
    "$$ SA_1 = \\left[sa_{11}, sa_{12}, \\dots, sa_{1N}\\right]$$\n",
    "\n",
    "Note in a single transformer layer we have 16 of these, to provide more depth and understanding: \n",
    "\n",
    "$$SA_{L1} = \\left[SA_1^T, SA_2^T, \\dots, SA_{16}^T\\right]$$\n",
    "\n",
    "As mentioned in the previous chapter over this we apply a linear transformation: \n",
    "\n",
    "$$\\mathbf{\\Omega}_{cL} SA_{L1}^T + \\mathbf{\\beta}_{cL}$$\n",
    "\n",
    "We then apply this to the skip connection (input joins this step) and apply LayerNorm.\n",
    "\n",
    "Then a Fully connected FFN: $$\\mathbb{R}^{1024} \\to \\mathbb{R}^{4096} \\to \\mathbf{GELU} \\to \\mathbb{R}^{1024}$$\n",
    "\n",
    "Then another Skip connection (Input to FNN joins this step) and LayerNorm. \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**One Transformer Layer DONE**\n",
    "\n",
    "Then Repeat this process 12 or 24 times depending on the BERT model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b9877",
   "metadata": {},
   "source": [
    "## Pre-training \n",
    "\n",
    "The parameters of the transformer architecture are learning using $selfâ€“supervision$ from a large source of text (i.e. Wikipedia).\n",
    "In doing so, the model learns general information about the statistics of language.\n",
    "One of the main benefits is that we can use a lot of data without requiring manual labels.\n",
    "\n",
    "**Components of self-supervision**\n",
    "\n",
    "- Completing the sentence \n",
    "- Fill in missing words. \n",
    "\n",
    "\n",
    "During training, the maximum input length is 512 tokens, and the batch size is 256. \n",
    "The system is training for a million steps, corresponding to around 50 epochs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b11603",
   "metadata": {},
   "source": [
    "## Fine Tuning \n",
    "\n",
    "In the fine-tuning stage, the model parameters are adjusted to specialize the network to a particular task.\n",
    "\n",
    "As such we append an additional layer to the current network, thus converting the output vectors to the desired output format.\n",
    "\n",
    "**Text Classification**\n",
    "\n",
    "Suppose our fine-tuning was sentiment analysis, the vector associated with the token is mapped to a single number and passed through sigmoid function, which we apply binary-cross entropy loss.\n",
    "\n",
    "**Word Classification**\n",
    "We can also fine-tune to the task $named \\ entity \\ recognition$ whcih classifies each as an element of a pre-defined set of nouns, including the an option for \"no-entity\". \n",
    "\n",
    "Each input embedding $\\mathbf{x_n}$ is mpped to a vector of dimension $E \\times 1$ where $E$ is the cardinality of the set containing the entities.\n",
    "\n",
    "We would then pass thoguh a softmax function to create probabilities for each class, this would contribute to the **Multi-class Cross-Entropy**\n",
    "\n",
    "**Text Span Prediction**\n",
    "\n",
    "Suppose we wanted to fine-tune the model to answer questions, where we provide a question and passage from Wikipedia containing the answer, these are concatonated and tokenized. \n",
    "\n",
    "BERT is then used to predict the text span in the passage that caontains the answer. \n",
    "\n",
    "Each token maps to two numbers indicating how likely it is that the text begins and ends at this location.\n",
    "\n",
    "The resulting two sets of numbers are put through two softmax functions. The Likelihood of any text span being the answer can be derived by combininng the probabililty of starting and ending at the appropriate places."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
