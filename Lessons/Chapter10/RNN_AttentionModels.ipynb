{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b5a3af",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018753ea",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks, is a class of models that deal with **Sequential Data** such as: \n",
    "- Text\n",
    "- Speech\n",
    "- Music\n",
    "- Audio\n",
    "- Video\n",
    "- Stock Prices\n",
    "- Gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e888d",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "In many real world problems the meaning of the current observation depends on the pervious observation. \n",
    "- To be able to answer a question one must be able to understand the question. \n",
    "- Videos and Film have plots based on past events in the video\n",
    "\n",
    "A FC NN treats the input independently and in an unordered manner, we already saw that with images this isn't helpful since we needed the context of the aspect of the image to be able to classify and localize. \n",
    "\n",
    "**Deep NN**\n",
    "\n",
    "- Applied combination of Linear and non-linear transformation.\n",
    "- Each internel Layer is and intermediate representation.\n",
    "- The intermediate representation of the data, is a step closer to solve our task more easily.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/FCvis.png\" width=\"210\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Recurrant NN** \n",
    "\n",
    "- We also use the intermediate representation.\n",
    "- In this model class, we pass this intermediate representation onwards to **another** intermediate representation which also takes in a new input.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/RNNInuition.png\" width=\"410\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b4d01",
   "metadata": {},
   "source": [
    "### Common RNN Structures\n",
    "\n",
    "|**Name**| **One to One** | **One to Many** | **Many to One** | **Many to Many** | **Many to Many**|\n",
    "|-----|--------------|-----------------|----------------|-----------------|----------------|\n",
    "|**Visual**| <img src=\"../images/chap10/O2O.png\" width=\"110\"/> | <img src=\"../images/chap10/O2M.png\" width=\"700\"/> | <img src=\"../images/chap10/M2O.png\" width=\"165\"/> | <img src=\"../images/chap10/M2M1.png\" width=\"1400\"/> | <img src=\"../images/chap10/M2M2.png\" width=\"170\"/>|\n",
    "|**Explanation**| Standard Vanilla Network | Trying to convert an image to string vector | Provide a vector of inputs through different intermediate representation and output a single value | Provide a vector of inputs through different representations, and previous layers pas the result to the next representation, which at some point output a vector | We provide a vector of inputs through different representations, each representatio layer, must output a value and also pass on its information to the next representation layer (who also recieve an input) | \n",
    "|**Task**| **Image Classification** | **Image Captioning** | **Sentiment Classification** | **Translation**  | **Video Frame Classification** | \n",
    "| **In $\\to$ Out** | Image $\\to$ Class | Image $\\to$ Seq. Words | Seq. Words $\\to$ Sentiment | Seq. Words $\\to$ Seq. Words | Video Frame $\\to$ Action |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a28a65",
   "metadata": {},
   "source": [
    "## RNN Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c36ac1",
   "metadata": {},
   "source": [
    "This class of networks, should be viewed as NN that recieve inputs at **some time steps**. In the CNN our input was at t=0 and produced an output \"directly\". \n",
    "Therefore, in general we have 3 layers that'll have time steps.\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^{d_{in}} := \\text{Input at time t}$\n",
    "- $W_x \\in \\mathbb{R}^{(d_h \\times d_{in})} := \\text{Input to Hidden weight matrix}$\n",
    "- $W_h \\in \\mathbb{R}^{(d_h \\times d_h)} := \\text{Hidden to Hidden weight matrix}$\n",
    "- $W_y \\in \\mathbb{R}^{d_h \\times d_{out}} := \\text{Hidden to Output weight matrix}$\n",
    "- $b \\in \\mathbb{R}^{d_h} := \\text{Bias hidden vector}$\n",
    "- $b_y \\in \\mathbb{R}^{d_{out}} := \\text{Bias output vector}$\n",
    "- $z_t = W_h h_{t-1} + W_x x_t + b \\in \\mathbb{R}^{d_h} := \\text{The \"score\" from input at time t and internal representation passed from t-1}$\n",
    "- $h_{t} \\in \\mathbb{R}^{d_h} := \\text{internal state vector time t}$\n",
    "  - $h_t = \\tanh(z_t) = f_W(x_t, h_{t-1})$\n",
    "  - This is our non-linear function applied **element wise**, and thus the dimension remain the same\n",
    "- $y_t = \\tanh(W_yh_t + b_y) \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "We assume that the hidden states have the same dimension througout time.\n",
    "\n",
    "---\n",
    "\n",
    "**Algebraic Modification**\n",
    "\n",
    "The concatonation operation over vector and matrices provided a clean way to represent multiple linear operations:\n",
    "\n",
    "$\\tilde{x}_t = \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_{in} + d_{h}}$\n",
    "\n",
    "$W = [W_x \\ W_h] \\in \\mathbb{R}^{(d_h \\times d_{in} + d_h)}$\n",
    "\n",
    "Everything remains the same, just makes our work tidier. \n",
    "\n",
    "$W \\tilde{x}_t = [W_x \\ W_h] \\cdot \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_h}$\n",
    "\n",
    "So now when applying our non-linear activation function we have: \n",
    "\n",
    "$$f_W(W \\tilde{x}_t + b) \\in \\mathbb{R}^{d_h}$$\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "$W$ is applied to **ALL** inputs and internal representation (unless said otherwise), it's independent of time.\n",
    "\n",
    "---\n",
    "\n",
    "**Weight Representation**\n",
    "\n",
    "|Weight Notation | One-Line Explanation | How it behaves | Example | \n",
    "|----------------|----------------------|----------------|---------|\n",
    "| $$W_h$$ | How **past** information **Influences** the **present**| If our values are binary $\\{0,1\\}$, the it represents what to keep and that to forget <br> If our values is $[0,1]$ then it represents what to amplify and what to supress | \"I did **not** enjoy the movie\" <br> Learns that negation persists, thereby affecting the words \"enjoy\"|\n",
    "| $$W_x$$ | How to **interprate current input** | What part of the input matters <br> how strong is their influence currently | If $x_t$ is a word embedding, $W_x$ learns: <br> which emedding dimension are important <br> How strongly a word should influence memory|\n",
    "| $$W_y$$ | How to **interprete** the internal state to **make** a **descision** | This doesn't affect the memory <br> Can be viewed like the \"classic\" wights in a DNN | Hidden state encodes: $\\{subject, negation, Action\\}$ <br> $W_y$ learns which hidden features to output label.|\n",
    "\n",
    "---\n",
    "\n",
    "**Hyperbolic Function Reminder**\n",
    "\n",
    "Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Definition**\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{2}{1 + e^{-2x}} - 1$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "**Range:** $(-1, 1)$\n",
    "\n",
    "**Shape:** S-shaped curve (zero-centered)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/tanh.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "| **✅ Advantages** | **❌ Disadvantages** |\n",
    "|------------------|---------------------|\n",
    "| **Zero-centered**: Output in $(-1, 1)$ → better gradient flow than sigmoid | **Vanishing gradients**: For $\\|x\\| > 3$, gradient $\\approx 0$ → problem persists in very deep networks |\n",
    "| **Stronger gradients**: $\\tanh'(x)_{\\text{max}} = 1$ (4× stronger than sigmoid!) | **Computationally expensive**: Still requires exponential calculations |\n",
    "| **Symmetric**: Easier optimization, less bias in weight updates | **Saturation**: Can still saturate at extremes ($\\pm 1$) |\n",
    "| **Smooth and differentiable**: Good gradient properties everywhere | **Not ideal for deep networks**: ReLU typically performs better |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab631e9",
   "metadata": {},
   "source": [
    "## Unrolling Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a567685",
   "metadata": {},
   "source": [
    "We shall now go through the training flow of the structures we introduced above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30729568",
   "metadata": {},
   "source": [
    "### Forward + Backward: Many to One \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/Many2one.png\" width=\"510\"/>\n",
    "\n",
    "$$\\begin{align} h_1 &= f_W(W(x_1, h_0)^T + b) \\\\ h_2 &= f_W(W(x_2, h_1)^T + b) \\\\  h_3 &= f_W(W(x_3, h_2)^T + b) \\\\ \\cdots &= \\cdots \\\\ h_T &= f_W(W(x_T, h_{t-1})+b) \\\\ y_T &= \\tanh(W_yh_T + b_y) \\end{align}$$\n",
    "\n",
    "$$\\mathbf{L} = l(y_T, y_{gt})$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f722e85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/ManyToOneBPP.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "We can directly compute the gradient of the loss w.r.t the output: $$\\frac{\\partial \\mathbf{L}}{\\partial y_T} = \\frac{\\partial l(\\tanh(W_yh_T + b_y), y_{gt})}{\\partial y_T}$$\n",
    "\n",
    "Now we can apply many sequence of chain rules: \n",
    "\n",
    "$$\\begin{align} \n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_T} &= \\frac{\\partial \\mathbf{L}}{\\partial y_T} \\frac{\\partial y_T}{\\partial h_T} \\\\ \n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\\\ \n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial f_W(W(x_T, h_{T-1})^T+b)}{\\partial h_{T-1}} \\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial f_W} \\frac{\\partial f_W}{\\partial h_{T-1}} \\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\left(1 - \\tanh^2(W(x_T, h_{T-1})+b)\\right) \\cdot W_h^T \\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-2}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\\\\n",
    "\\dots &= \\dots \\\\ \n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{0}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial h_{0}}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Line 3-5 are nearly expanded nearly completely to show what the partial derivative develops into, since this is a repetitive pattern. <br>\n",
    "Note that the partial derivative of an internal state w.r.t the activation function is applied **Element wise**. <br>\n",
    "Furthermore, we only computed the **main** backward flow, we musn't forget the partial derivative w.r.t all the parameters specifically: $\\{W_x, W_y, W_h, b, b_y\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906cc86",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/ManyToOneBPP.png\" width=\"510\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdcfec",
   "metadata": {},
   "source": [
    "As usual in the back-propagation algorithm we wish to determine the change in the paramters w.r.t the loss specifically:\n",
    "$\\{\\frac{\\partial L}{\\partial W_x}, \\frac{\\partial L}{\\partial W_y}, \\frac{\\partial L}{\\partial W_h}, \\frac{\\partial L}{\\partial b}, \\frac{\\partial L}{\\partial b_y}\\}$\n",
    "\n",
    "Consider computing the partial derivative of the loss wrt $W_x$ at time $T$ (we denote this as a time-stamp with the bar): \n",
    "\n",
    "$$\\left. \\frac{\\partial L}{\\partial W_x} \\right|_{t=T} = \\frac{\\partial L}{\\partial y_T} \\cdot \\frac{\\partial y_T}{\\partial h_T} \\cdot \\frac{\\partial h_T}{\\partial z_T} \\cdot \\frac{\\partial z_T}{\\partial W_x}$$\n",
    "\n",
    "Indeed we need to consider the change in the loss wrt $W_x$ everytime $t$ it was used: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = \\sum_{i=1}^T\\left. \\frac{\\partial L}{\\partial W_x}\\right|_{t=i}\n",
    "$$\n",
    "\n",
    "The following table summarises the partial derivative of the vanilla network:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|**Derivative** | **Formula** | \n",
    "|---------------|-------------|\n",
    "|$$\\frac{\\partial L}{\\partial W_x}$$ | $$\\sum_{i=1}^T\\left. \\frac{\\partial L}{\\partial W_x}\\right\\|_{t=i}$$\n",
    "| $$\\frac{\\partial L}{\\partial W_h}$$ | $$\\sum_{i=1}^T\\left. \\frac{\\partial L}{\\partial W_h}\\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial b}$$ | $$\\sum_{i=1}^T\\left. \\frac{\\partial L}{\\partial b}\\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial b_y}$$ | $$\\frac{\\partial L}{\\partial y_T} \\cdot \\frac{\\partial y_T}{\\partial b_y}$$ |\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
