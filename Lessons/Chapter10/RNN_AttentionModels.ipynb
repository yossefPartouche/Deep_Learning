{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b5a3af",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018753ea",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks, is a class of models that deal with **Sequential Data** such as: \n",
    "- Text\n",
    "- Speech\n",
    "- Music\n",
    "- Audio\n",
    "- Video\n",
    "- Stock Prices\n",
    "- Gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e888d",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "In many real world problems the meaning of the current observation depends on the pervious observation. \n",
    "- To be able to answer a question one must be able to understand the question. \n",
    "- Videos and Film have plots based on past events in the video\n",
    "\n",
    "A FC NN treats the input independently and in an unordered manner, we already saw that with images this isn't helpful since we needed the context of the aspect of the image to be able to classify and localize. \n",
    "\n",
    "**Deep NN**\n",
    "\n",
    "- Applied combination of Linear and non-linear transformation.\n",
    "- Each internel Layer is and intermediate representation.\n",
    "- The intermediate representation of the data, is a step closer to solve our task more easily.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/FCvis.png\" width=\"210\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Recurrant NN** \n",
    "\n",
    "- We also use the intermediate representation.\n",
    "- In this model class, we pass this intermediate representation onwards to **another** intermediate representation which also takes in a new input.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/RNNInuition.png\" width=\"410\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b4d01",
   "metadata": {},
   "source": [
    "### Common RNN Structures\n",
    "\n",
    "|**Name**| **One to One** | **One to Many** | **Many to One** | **Many to Many** | **Many to Many**|\n",
    "|-----|--------------|-----------------|----------------|-----------------|----------------|\n",
    "|**Visual**| ![](../images/chap10/O2O.png) | ![](../images/chap10/O2M.png) | ![](../images/chap10/M2O.png) | ![](../images/chap10/M2M1.png) | ![](../images/chap10/M2M2.png)|\n",
    "|**Explanation**| Standard Vanilla Network | Trying to convert an image to string vector | Provide a vector of inputs through different intermediate representation and output a single value | Provide a vector of inputs through different representations, and previous layers pas the result to the next representation, which at some point output a vector | We provide a vector of inputs through different representations, each representatio layer, must output a value and also pass on its information to the next representation layer (who also recieve an input) | \n",
    "|**Task**| **Image Classification** | **Image Captioning** | **Sentiment Classification** | **Translation**  | **Video Frame Classification** | \n",
    "| **In $\\to$ Out** | Image $\\to$ Class | Image $\\to$ Seq. Words | Seq. Words $\\to$ Sentiment | Seq. Words $\\to$ Seq. Words | Video Frame $\\to$ Action |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a28a65",
   "metadata": {},
   "source": [
    "## RNN Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c36ac1",
   "metadata": {},
   "source": [
    "This class of networks, should be viewed as NN that recieve inputs at **some time steps**. In the CNN our input was at t=0 and produced an output \"directly\". \n",
    "Therefore, in general we have 3 layers that'll have time steps.\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^{d_{in}} := \\text{Input at time t}$\n",
    "- $W_x \\in \\mathbb{R}^{(d_h \\times d_{in})} := \\text{Input to Hidden weight matrix}$\n",
    "- $W_h \\in \\mathbb{R}^{(d_h \\times d_h)} := \\text{Hidden to Hidden weight matrix}$\n",
    "- $W_y \\in \\mathbb{R}^{d_h \\times d_{out}} := \\text{Hidden to Output weight matrix}$\n",
    "- $b \\in \\mathbb{R}^{d_h} := \\text{Bias hidden vector}$\n",
    "- $b_y \\in \\mathbb{R}^{d_{out}} := \\text{Bias output vector}$\n",
    "- $z_t = W_h h_{t-1} + W_x x_t + b \\in \\mathbb{R}^{d_h} := \\text{The \"score\" from input at time t and internal representation passed from t-1}$\n",
    "- $h_{t} \\in \\mathbb{R}^{d_h} := \\text{internal state vector time t}$\n",
    "  - $h_t = \\tanh(z_t) = f_W(x_t, h_{t-1})$\n",
    "  - This is our non-linear function applied **element wise**, and thus the dimension remain the same\n",
    "- $y_t = \\tanh(W_yh_t + b_y) \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "We assume that the hidden states have the same dimension througout time.\n",
    "\n",
    "---\n",
    "\n",
    "**Algebraic Modification**\n",
    "\n",
    "The concatonation operation over vector and matrices provided a clean way to represent multiple linear operations:\n",
    "\n",
    "$\\tilde{x}_t = \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_{in} + d_{h}}$\n",
    "\n",
    "$W = [W_x \\ W_h] \\in \\mathbb{R}^{(d_h \\times d_{in} + d_h)}$\n",
    "\n",
    "Everything remains the same, just makes our work tidier. \n",
    "\n",
    "$W \\tilde{x}_t = [W_x \\ W_h] \\cdot \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_h}$\n",
    "\n",
    "So now when applying our non-linear activation function we have: \n",
    "\n",
    "$$f_W(W \\tilde{x}_t + b) \\in \\mathbb{R}^{d_h}$$\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "$W$ is applied to **ALL** inputs and internal representation (unless said otherwise), it's independent of time.\n",
    "\n",
    "---\n",
    "\n",
    "**Weight Representation**\n",
    "\n",
    "|Weight Notation | One-Line Explanation | How it behaves | Example | \n",
    "|----------------|----------------------|----------------|---------|\n",
    "| $$W_h$$ | How **past** information **Influences** the **present**| If our values are binary $\\{0,1\\}$, the it represents what to keep and that to forget <br> If our values is $[0,1]$ then it represents what to amplify and what to supress | \"I did **not** enjoy the movie\" <br> Learns that negation persists, thereby affecting the words \"enjoy\"|\n",
    "| $$W_x$$ | How to **interprate current input** | What part of the input matters <br> how strong is their influence currently | If $x_t$ is a word embedding, $W_x$ learns: <br> which emedding dimension are important <br> How strongly a word should influence memory|\n",
    "| $$W_y$$ | How to **interprete** the internal state to **make** a **descision** | This doesn't affect the memory <br> Can be viewed like the \"classic\" wights in a DNN | Hidden state encodes: $\\{subject, negation, Action\\}$ <br> $W_y$ learns which hidden features to output label.|\n",
    "\n",
    "---\n",
    "\n",
    "**Hyperbolic Function Reminder**\n",
    "\n",
    "Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Definition**\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{2}{1 + e^{-2x}} - 1$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "**Range:** $(-1, 1)$\n",
    "\n",
    "**Shape:** S-shaped curve (zero-centered)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/tanh.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "| **✅ Advantages** | **❌ Disadvantages** |\n",
    "|------------------|---------------------|\n",
    "| **Zero-centered**: Output in $(-1, 1)$ → better gradient flow than sigmoid | **Vanishing gradients**: For $\\|x\\| > 3$, gradient $\\approx 0$ → problem persists in very deep networks |\n",
    "| **Stronger gradients**: $\\tanh'(x)_{\\text{max}} = 1$ (4× stronger than sigmoid!) | **Computationally expensive**: Still requires exponential calculations |\n",
    "| **Symmetric**: Easier optimization, less bias in weight updates | **Saturation**: Can still saturate at extremes ($\\pm 1$) |\n",
    "| **Smooth and differentiable**: Good gradient properties everywhere | **Not ideal for deep networks**: ReLU typically performs better |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab631e9",
   "metadata": {},
   "source": [
    "## Unrolling Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a567685",
   "metadata": {},
   "source": [
    "We shall now go through the training flow of the structures we introduced above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30729568",
   "metadata": {},
   "source": [
    "### Forward: Many to One \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/Many2one.png\" width=\"510\"/>\n",
    "<img src=\"../images/chap10/VFWDRNN.png\" width=\"313\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a747509",
   "metadata": {},
   "source": [
    "### Backward: Many to One\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/ManyToOneBPP.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "We can directly compute the gradient of the loss with respect to the output:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial y_T} = \\frac{\\partial l(\\tanh(W_y h_T + b_y), y_{gt})}{\\partial y_T}\n",
    "$$\n",
    "\n",
    "Now, we apply the chain rule recursively through time:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_T} &= \\frac{\\partial \\mathbf{L}}{\\partial y_T} \\frac{\\partial y_T}{\\partial h_T} \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\\\\\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial z_T} \\frac{\\partial z_T}{\\partial h_{T-1}} \\\\\\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\left(1 - \\tanh^2(z_T)\\right) W_h^T \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-2}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\\\\\\\\n",
    "\\vdots \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_0} &= \\frac{\\partial \\mathbf{L}}{\\partial h_1} \\frac{\\partial h_1}{\\partial h_0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: The derivative of the activation function (tanh) is applied **elementwise**.\n",
    "\n",
    "---\n",
    "\n",
    "We also need to compute the gradients with respect to all parameters: $W_x$, $W_y$, $W_h$, $b$, $b_y$.\n",
    "\n",
    "For example, the partial derivative of the loss with respect to $W_x$ at time $t$ is:\n",
    "$$\n",
    "\\left. \\frac{\\partial L}{\\partial W_x} \\right|_{t} = \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "The **total** gradient is the sum over all time steps:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = \\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_x} \\right|_{t=i}\n",
    "$$\n",
    "\n",
    "---\n",
    "<div align=\"center\">\n",
    "\n",
    "**Summary Table of Gradients:**\n",
    "\n",
    "| **Derivative** | **Formula** |\n",
    "|---------------|-------------|\n",
    "| $$\\frac{\\partial L}{\\partial W_x}$$ | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_x} \\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial W_h}$$ | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_h} \\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial b}$$   | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial b} \\right\\|_{t=i}$$   |\n",
    "| $$\\frac{\\partial L}{\\partial b_y}$$ | $$\\frac{\\partial L}{\\partial y_T} \\cdot \\frac{\\partial y_T}{\\partial b_y}$$ |\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2464dd",
   "metadata": {},
   "source": [
    "### Forward: Many to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be9205",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/M2Mfwd.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "$$\\begin{align} \n",
    "z_1 &= W_x x_1 + W_h h_0 + b = W(x_1, h_0)^T + b\\\\\n",
    "h_1 &= f_W(z_1) \\\\\n",
    "y_1 &= f_{W_y}(W_yh_1 + b_y) \\\\\n",
    "L_1 &= l(y_1, y_{1_{GT}}) \\\\\n",
    "\\\\\n",
    "z_2 &= W_x x_2 + W_h h_1 + b = W(x_2, h_1)^T + b\\\\\n",
    "h_2 &= f_W(z_2) \\\\\n",
    "y_2 &= f_{W_y}(W_yh_2 + b_y) \\\\\n",
    "L_2 &= l(y_2, y_{2_{GT}})\\\\\n",
    "\\vdots \\\\\n",
    "z_T &= W_x x_t + W_h h_{T-1} + b = W(x_T, h_{T-1})^T + b \\\\\n",
    "h_T &= f_W(z_T) \\\\\n",
    "y_T &= f_{W_y}(W_yh_T + b_y) \\\\\n",
    "L_T &= l(y_T, y_{T_{GT}})\n",
    "\\end{align}$$\n",
    "\n",
    "$$L = \\sum_{t=1}^T L_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38faacee",
   "metadata": {},
   "source": [
    "### Backward: Many to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387ce8d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/M2Mbckwd.png\" width=\"510\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34a24f",
   "metadata": {},
   "source": [
    "In the **Many to Many**, evey time step $t$ has its own $y_t$ and $L_t$ so: \n",
    "\n",
    "$$L = \\sum_{i=1}^T L_i$$\n",
    "\n",
    "The gradient w.r.t each parameter is **summed over all time steps**\n",
    "$$\\frac{\\partial L}{\\partial b_y} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial b_y}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_y} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial W_y}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c468b",
   "metadata": {},
   "source": [
    "### Computation Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790e5d8",
   "metadata": {},
   "source": [
    "Running the backpropagation on a long sequence can become **very** expensive, so we must device a way to complete the task efficiently.\n",
    "\n",
    "**Divide and Conquer**\n",
    "\n",
    "Suppose we have a sequence length of size $T$, depending on the constraints of the task and the hardware we'll choose our window size $k$, a **hyperparameter** to create $\\lceil \\frac{T}{k} \\rceil$ chunks.\n",
    "\n",
    "```\n",
    "Initialise Internal state h_0\n",
    "for chunk i = 1 to k:\n",
    "    Run Forward Pass on chunk i\n",
    "    store final internal state of h_{i•k}\n",
    "    Run Backward Pass on chunk i\n",
    "    Set h_0 = h_{i*k} for next chunk\n",
    "```\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/chap10/optBPP1.png\" width=\"310\"/>\n",
    "  <img src=\"../images/chap10/optBPP2.png\" width=\"310\"/>\n",
    "  <img src=\"../images/chap10/optBPP3.png\" width=\"310\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
