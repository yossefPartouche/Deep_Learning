{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9306454e",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fe98e",
   "metadata": {},
   "source": [
    "## Processing text data\n",
    "\n",
    "Consider the following review on some restuarant:\n",
    "\n",
    "```\n",
    "\"The restaurant refused to serve me a ham sandwich because it only cooks vegetarian food.\n",
    "In the end, they just gave me two slices of bread.\n",
    "Their ambaince was just as good as the food and service\"\n",
    "```\n",
    "\n",
    "We'd like to provess this text into a representation suitable for downsttream tasks such tasks may be: \n",
    "\n",
    "- Classify if this review is positive or negative.\n",
    "- Answer context based questions such as \"Does this restaurant serve steak?\"\n",
    "\n",
    "Notice the following three observations: \n",
    "\n",
    "1. Depending on how we encode this text, it's representation can become very large very quickly:\n",
    "   - In words = 37\n",
    "   - Embedding space = 37 x 1024 = 37888\n",
    "2. This text was a single review of that size, and this size can change. \n",
    "3. Syntax alone isn't enough to resolve some of the tasks at hand, we require context and words that are important and thus should be paid $attention$.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26721d9b",
   "metadata": {},
   "source": [
    "## Dot Product and Self Attention\n",
    "\n",
    "A standard NN layer $f[x]$, takes a $D \\times 1$ input $\\mathbf{x}$ and applies a linear transformation follwed by an activation function like ReLU:\n",
    "\n",
    "$$f[\\mathbf{x}] = \\mathbf{ReLU}[\\beta + \\Omega \\mathbf{x}]$$\n",
    "\n",
    "A self-attention block $\\mathbf{sa}$ takes $N$ inputs $\\mathbf{x_1}, \\dots , \\mathbf{x_N}$, each of dimension $D \\times 1$ and returns $N$ outputs, each of size $D \\times 1$. \n",
    "\n",
    "The input can represent a word or a word fragment (discussed later, but relates to information theory)\n",
    "\n",
    "**Creating Self-Attention**\n",
    "\n",
    "1. For each input $\\mathbf{x_m}$ compute it's $value$: \n",
    "   $$\\mathbf{v_m} = \\beta_v + \\Omega_v\\mathbf{x_m}$$\n",
    "2. The $n^{th}$ output $\\mathbf{sa}_n\\left[\\mathbf{x_1}, \\dots, \\mathbf{x_n}\\right]$ is a weighted sum of the $n$ values.:\n",
    "   $$\\mathbf{sa}_n = \\sum_{i=1}^Na[\\mathbf{x}_i, \\mathbf{x}_n]\\mathbf{v}_i \\quad | \\quad a[\\mathbf{x}_i, \\mathbf{x}_n] \\in \\mathbb{R}^+ \\cup \\{0\\}, \\ \\sum_{i=1}^N a[•, \\mathbf{x}_i] = 1$$\n",
    "\n",
    "**Note**\n",
    "\n",
    "Like in RNN and LSTM we have $\\beta_v$ and  $\\Omega_v$ which are weights and biases independant of the the input.\n",
    "\n",
    "### Computing attention weights\n",
    "\n",
    "To compute the attention, we apply two linear trasnformations to the inputs: \n",
    "\n",
    "$$\\text{Queries: }\\mathbf{q}_n = \\beta_q + \\Omega_q \\mathbf{x}_n$$\n",
    "\n",
    "$$\\text{Keys: } \\mathbf{k}_m = \\beta_k + \\Omega_k \\mathbf{x}_m$$\n",
    "\n",
    "Indeed we compute the dot product between these **queries** and **keys** vectors and apply the **Softmax** function which defines our **attention**:\n",
    "\n",
    "$$\\begin{align}\n",
    "a[\\mathbf{x_m}, \\mathbf{x}_n] &= \\text{softmax}_m[\\mathbf{k}_{•} \\cdot \\mathbf{q}_n] \\\\ \n",
    "&= \\frac{\\exp[\\mathbf{k}_{m} \\cdot \\mathbf{q}_n]}{\\sum_{i=1}^N \\exp[\\mathbf{k}_{i} \\cdot \\mathbf{q}_n]}\n",
    "\\end{align}$$\n",
    "\n",
    "The dot product provides a measure describing similarity between two vectors, as such we're determining the relative similarities between the $n^{th}$ query and all the keys.\n",
    "\n",
    "Applying the softmax over this measure, provides the probabilistic aspect, of how probable it is for a respective key to be similar to the current query.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea20ea",
   "metadata": {},
   "source": [
    "### Dimension Check \n",
    "\n",
    "**Input dimension**\n",
    "\n",
    "$ \\mathbf{x_i} \\in \\mathbb{R}^{D_{in}}$\n",
    "\n",
    "**Value Dimension**\n",
    "You can freely choose the dimension of $\\mathbf{v_m} \\in \\mathbb{R}^{D_1}$ this means: \n",
    "\n",
    "$$\\boxed{\\mathbf{\\Omega_v} \\in \\mathbb{R}^{D_{in} \\times D_1} \\quad \\text{and} \\quad \\mathbf{\\beta_v} \\in \\mathbb{R}^{D_1}}$$\n",
    "\n",
    "Mathematically allowing for: $\\mathbf{v_m} = \\beta_v + \\Omega_v\\mathbf{x_i}$\n",
    "\n",
    "**Self Attention**\n",
    "\n",
    "You can freely choose the dimension for Queries and the keys, constrained to being equal in dimension.\n",
    "\n",
    "$$\\boxed{\\mathbf{\\text{dim}(\\Omega_k}) = \\text{dim}(\\mathbf{\\Omega_q}) \\in \\mathbb{R}^{D_{in} \\times D_2} \\quad \\text{and} \\quad \\mathbf{ \\text{dim}(\\beta_k)} = \\mathbf{ \\text{dim}(\\beta_q)} \\in \\mathbb{R}^{D_2}}$$\n",
    "\n",
    "Mathematically allowing for:\n",
    "\n",
    "$\\text{Queries: }\\mathbf{q}_n = \\beta_q + \\Omega_q \\mathbf{x}_n$\n",
    "\n",
    "$\\text{Keys: } \\mathbf{k}_m = \\beta_k + \\Omega_k \\mathbf{x}_m$\n",
    "\n",
    "Thereby allowing this to occur: $[\\mathbf{q}_n \\cdot  \\mathbf{k}_m]$\n",
    "\n",
    "| **Component**   | **Dimension**                  | **Freedom**                |\n",
    "|-----------------|-------------------------------|----------------------------|\n",
    "| Input           | $\\mathbf{x}_i \\in \\mathbb{R}^{D_{in}}$   | $D_{in}$: arbitrary       |\n",
    "| Value           | $\\mathbf{v}_m \\in \\mathbb{R}^{D_1}$      | $D_1$: freely chosen      |\n",
    "| Query/Key       | $\\mathbf{q}_n, \\mathbf{k}_m \\in \\mathbb{R}^{D_2}$ | $D_2$: freely chosen (must match for both) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d20c5",
   "metadata": {},
   "source": [
    "### Matrix Form\n",
    "\n",
    "We shall present the above in a more compact form.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|**Term** | **Notation** | **Matrix Dimension** | **Calculation** | \n",
    "|---------|--------------|----------------------|-------------------|\n",
    "| **Input** | $\\mathbf{X}$ | $\\mathbb{R}^{D_{in} \\times N}$| **Design Matrix** | \n",
    "| **Value** | $\\mathbf{V}[X]$ |  $\\mathbb{R}^{D_1 \\times N}$ | $\\mathbf{\\beta_v} \\cdot \\mathbf{1} + \\mathbf{\\Omega_v}\\mathbf{X}$ |\n",
    "| **Queries** | $\\mathbf{Q}[X]$ |  $\\mathbb{R}^{D_2 \\times N}$ | $\\mathbf{\\beta_q} \\cdot \\mathbf{1} + \\mathbf{\\Omega_q}\\mathbf{X}$ |\n",
    "| **Keys** | $\\mathbf{K}[X]$ |  $\\mathbb{R}^{D_2 \\times N}$ | $\\mathbf{\\beta_k} \\cdot \\mathbf{1} + \\mathbf{\\Omega_k}\\mathbf{X}$ |\n",
    "| **Self Attention** | $\\mathbf{SA}[X]$ |  $\\mathbb{R}^{D_1 \\times N}$ | $\\mathbf{V}[\\mathbf{X}] \\cdot \\mathbf{Softmax}\\left[\\mathbf{K}[X]^T\\mathbf{Q}[X]\\right]$|\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Scaled self-attention\n",
    "\n",
    "One of the issue with using the softmax, is that large values can dominate the result of the overall argument, which can skew the learning process as the smaller value will have little effect on the output. As such we can mitigate this effect by scaling by the dimension $\\sqrt{D_q}$:\n",
    "\n",
    "$$\\mathbf{SA}[X] = \\mathbf{V}[\\mathbf{X}] \\cdot \\mathbf{Softmax}\\left[\\frac{\\mathbf{K}[X]^T\\mathbf{Q}[X]}{\\sqrt{D_q}}\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/SAHead.png\" width=\"710\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328304b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Sequential processing requires importance of order between the input so we now present two methods to incorperate postional information.\n",
    "\n",
    "**Absollute positional encodings:**\n",
    "\n",
    "A matrix $\\Pi$ is added to the input $\\mathbf{X}$ that ecodes the positional information. <br> Each column of $\\Pi$ is unique and gence contrains information about the absolute position in the input sequence.\n",
    "\n",
    "- This matrix can be chosen in advance.\n",
    "- This matrix can be learned. \n",
    "\n",
    "To gain better grasp of this consider the following example: \n",
    "\n",
    "Suppose our input matrix $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 4}$ and define the positional encoding as follows $\\Pi \\in \\mathbb{R}^{3 \\times 4}$\n",
    "\n",
    "```\n",
    "X = [                            π = [                   \n",
    "  [x11, x12, x13, x14],             [0,   1,   2,   3  ],\n",
    "  [x21, x22, x23, x24],             [0.1, 0.1, 0.1, 0.1],\n",
    "  [x31, x32, x33, x34]              [0.5, 0.5, 0.5, 0.5] \n",
    "]                                    ] \n",
    "\n",
    "X_pos = X + π                                            \n",
    "```\n",
    "In practice the actual encoding are more complex, but the importance is uniqueness \n",
    "\n",
    "\n",
    "**Relative Positional encodings**\n",
    "\n",
    "In most cases the input to a self-attention mechanism can be: \n",
    "- Sentece\n",
    "- Many Sentences\n",
    "- Partial Sentence\n",
    "- Word\n",
    "- Letter\n",
    "\n",
    "The absolute position of a word isn't as important, we care about how an **earlier input** affects a **future input**.<br>\n",
    "To do this, each element of the attention matrix correcponds to a particular offset between key position $a$ and query position $b$.<br>\n",
    "Relative positional encodings learna a parameter $\\pi_{a,b}$ for each odffswet and use this modify the attention matrix by adding these values,multiplying by them, or alter the attention in some other way.\n",
    "\n",
    "Example:\n",
    "suppose we have 4 embedded inputs: $x_1, x_2, x_3, x_4$ \n",
    "\n",
    "1. **Compute Relative Positions**\n",
    "    - For each pair of positions $(a,b)$, compute the offset: $r  = a-b$\n",
    "    - Assume the offset are in this case $-3, -2, -1, 0, 1, 2, 3$\n",
    "2. **Learnable Relative Encoding Table**\n",
    "    - Create a table of learnable parameters $\\pi_r$, for each possible offset $r$:\n",
    "\n",
    "|$\\text{offset:}$| $-3$ | $-2$ | $1$| $0$ |  $1$ |  $2$ |  $3$|\n",
    "|----------------|------|------|----|-----|------|-----|------|\n",
    "|$\\pi_r$ |$v_{-3}$| $v_{-2}$| $v_{-1}$| $v_{0}$|  $v_1$ | $v_2$|  $v_3$ |\n",
    "\n",
    "1. **Modify Attention Score**\n",
    "    $$\\text{score}(a,b) = q_b \\cdot k_a + \\pi_{a-b} \\\\ \\text{or} \\\\ \\text{score}(a,b) = q_b (\\cdot k_a + \\pi_{a-b}) $$\n",
    "\n",
    "    $$\\text{Matrix Form:} \\quad  \\mathbf{SA}[X] = \\mathbf{V}[\\mathbf{X}] \\cdot \\mathbf{Softmax}\\left[\\mathbf{K}[X]^T\\mathbf{Q}[X] + \\Pi\\right]$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5463603",
   "metadata": {},
   "source": [
    "### Multiple Heads \n",
    "\n",
    "If we're able to produce a single Self-attention output, we can produce multiple head. The purpose for doing this, is that it'll provide **Richer Representation**, **Capture Diverse Relationships** and **Improved Expressiveness**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31c303",
   "metadata": {},
   "source": [
    "$\\text{Let H be the number of Heads we produce}$, a single head will now be denoted as: \n",
    "\n",
    "$$ \\mathbf{SA}_h[X] = \\mathbf{V}_h[\\mathbf{X}] \\cdot \\mathbf{Softmax}\\left[\\frac{\\mathbf{K}_h[X]^T\\mathbf{Q}_h[X]}{\\sqrt{D_q}} + \\Pi\\right]$$\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "$\\{\\beta_{vh}, \\Omega_{vh}\\} \\ \\{\\beta_{qh}, \\Omega_{qh}\\} \\ \\{\\beta_{kh}, \\Omega_{kh}\\}$\n",
    "\n",
    "Together we produce a concatonation of these heads: \n",
    "\n",
    "$$\\mathbf{MhSa}[\\mathbf{X}] = \\mathbf{\\Omega_c}\\left[\\mathbf{Sa}_1[X]^T,\\mathbf{Sa}_2[X]^T, \\dots, \\mathbf{Sa}_H[X]^T \\right]^T + \\mathbf{\\beta_c}$$\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/MhSA.png\" width=\"510\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e903324",
   "metadata": {},
   "source": [
    "## Transformer Layers\n",
    "\n",
    "The self-attention layer is a single component in the transformer layer.<br>\n",
    "It's followed by a fully connected network (Multi-Layer Perceptron).<br>\n",
    "After these two layer, it's typical to add a Layer-Norm, which normalises each embedding in each batch element seperately .\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{X'} &= \\mathbf{X} + \\mathbf{MhSa}[\\mathbf{X}] \\\\\n",
    "\\mathbf{X_{norm1}} &= \\mathbf{LayerNorm}[\\mathbf{X'}] \\\\\n",
    "\\mathbf{x_n} &= \\mathbf{x_n} + \\mathbf{mlp[x_n]} \\quad \\forall n \\in \\{1, \\dots, N\\} \\\\\n",
    "\\mathbf{X_{norm2}} &= \\mathbf{LayerNorm}[\\mathbf{X_{mlp}}]\n",
    "\\end{align}$$\n",
    "\n",
    "### Layer Normalisation\n",
    "\n",
    "Normalise across the features for each individual sample (i.e. token/embedding)\n",
    "\n",
    "$$\\mathbf{LayerNorm(x)} = \\gamma \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "$$\\mu = \\frac{1}{D}\\sum_{i=1}^dx_i$$\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{D} \\sum_{i=1}^D (x_i - \\mu)^2}$$\n",
    "\n",
    "**LayerNorm vs. BatchNorm**\n",
    "- LayerNorm normalizes across **FEATURES** for each individual sample, independent of the batch.\n",
    "- BatchNorm normalizes across **BATCH** for each features\n",
    "\n",
    "We use LayerNorm since BatchNorm is sensitive to batch size and less effective for sequence models like transformer.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/transform.png\" width=\"710\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff7265",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "A typical NLP pipeline starts with a $tokenizer$ that splits the text into words or word fragments, and for each token it is then mapped to a learned embedding.\n",
    "\n",
    "Here are some of the difficulties with tokenization:\n",
    "1. We may define a vocabulary, but it's very possible that some new words come up?\n",
    "2. How do we handle punctutation, since it's critical to these model?\n",
    "3. Words that have the same root are they the same or different? \n",
    "   - Walk, Walks, Walked, walking\n",
    "   - Journée, journal, journaliste\n",
    "   - שלם, תשלום, השלמה, משלם\n",
    "\n",
    "**Subword Tokenization**\n",
    "\n",
    "Break words into smaller units, allowing the model to handle unknown words, and variations of the same word.\n",
    "\n",
    "**Character-Level Tokenization**\n",
    "Split the text into individual characters, though this makes the sequence very large.\n",
    "\n",
    "**Unigram or word-Level Tokenization**\n",
    "\n",
    "Uses a fixed vocabulary of words, though as mentioned earlier would struggle with unknown words.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Each token in the vocabulary $\\mathbb{V}$ is mapped to a unique $word \\ embedding$, and the embeddings for the whole vocabulary are stored in a matrix $\\Omega_e \\in \\mathbb{R}^{D \\times |\\mathbb{V}|}$.\n",
    "\n",
    "The $N$ input tokens are first encoded in the matrix $\\mathbf{T} \\in \\mathbb{R}^{|\\mathbb{V}| \\times N}$, where the $n^{th}$ column corresponds to the $n^{th}$ token and is a $|\\mathbb{V}| \\times 1 \\ one-hot \\ vector$\n",
    "\n",
    "The input embeddings are computed as: $\\mathbf{X} = \\mathbf{\\Omega_e }\\mathbf{T}$ and indeed $\\mathbf{\\Omega_e}$ is a learned parameter.\n",
    "\n",
    "Typically, the emding size $D=1024$, and the vocabulary size is $|\\mathbb{V}| = 30,000$\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/embedding.png\" width=\"710\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
