{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b5a3af",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018753ea",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks, is a class of models that deal with **Sequential Data** such as: \n",
    "- Text\n",
    "- Speech\n",
    "- Music\n",
    "- Audio\n",
    "- Video\n",
    "- Stock Prices\n",
    "- Gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e888d",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "In many real world problems the meaning of the current observation depends on the pervious observation. \n",
    "- To be able to answer a question one must be able to understand the question. \n",
    "- Videos and Film have plots based on past events in the video\n",
    "\n",
    "A FC NN treats the input independently and in an unordered manner, we already saw that with images this isn't helpful since we needed the context of the aspect of the image to be able to classify and localize. \n",
    "\n",
    "**Deep NN**\n",
    "\n",
    "- Applied combination of Linear and non-linear transformation.\n",
    "- Each internel Layer is and intermediate representation.\n",
    "- The intermediate representation of the data, is a step closer to solve our task more easily.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/FCvis.png\" width=\"210\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Recurrant NN** \n",
    "\n",
    "- We also use the intermediate representation.\n",
    "- In this model class, we pass this intermediate representation onwards to **another** intermediate representation which also takes in a new input.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/RNNInuition.png\" width=\"410\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b4d01",
   "metadata": {},
   "source": [
    "### Common RNN Structures\n",
    "\n",
    "|**Name**| **One to One** | **One to Many** | **Many to One** | **Many to Many** | **Many to Many**|\n",
    "|-----|--------------|-----------------|----------------|-----------------|----------------|\n",
    "|**Visual**| ![](../images/chap10/O2O.png) | ![](../images/chap10/O2M.png) | ![](../images/chap10/M2O.png) | ![](../images/chap10/M2M1.png) | ![](../images/chap10/M2M2.png)|\n",
    "|**Explanation**| Standard Vanilla Network | Trying to convert an image to string vector | Provide a vector of inputs through different intermediate representation and output a single value | Provide a vector of inputs through different representations, and previous layers pas the result to the next representation, which at some point output a vector | We provide a vector of inputs through different representations, each representatio layer, must output a value and also pass on its information to the next representation layer (who also recieve an input) | \n",
    "|**Task**| **Image Classification** | **Image Captioning** | **Sentiment Classification** | **Translation**  | **Video Frame Classification** | \n",
    "| **In $\\to$ Out** | Image $\\to$ Class | Image $\\to$ Seq. Words | Seq. Words $\\to$ Sentiment | Seq. Words $\\to$ Seq. Words | Video Frame $\\to$ Action |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a28a65",
   "metadata": {},
   "source": [
    "## RNN Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c36ac1",
   "metadata": {},
   "source": [
    "This class of networks, should be viewed as NN that recieve inputs at **some time steps**. In the CNN our input was at t=0 and produced an output \"directly\". \n",
    "Therefore, in general we have 3 layers that'll have time steps.\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "| Notation | Explanation |\n",
    "|----------|-------------|\n",
    "| $x_t \\in \\mathbb{R}^{d_{in}}$ | Input at time $t$ |\n",
    "| $W_x \\in \\mathbb{R}^{d_h \\times d_{in}}$ | Input-to-hidden weight matrix |\n",
    "| $W_h \\in \\mathbb{R}^{d_h \\times d_h}$ | Hidden-to-hidden weight matrix |\n",
    "| $W_y \\in \\mathbb{R}^{d_{out} \\times d_h}$ | Hidden-to-output weight matrix |\n",
    "| $b \\in \\mathbb{R}^{d_h}$ | Bias for hidden layer |\n",
    "| $b_y \\in \\mathbb{R}^{d_{out}}$ | Bias for output layer |\n",
    "| $z_t = W_h h_{t-1} + W_x x_t + b \\in \\mathbb{R}^{d_h}$ | Pre-activation (\"score\") at time $t$ |\n",
    "| $h_t = \\tanh(z_t) = f_W(x_t, h_{t-1}) \\in \\mathbb{R}^{d_h}$ | Hidden state at time $t$ <br> (elementwise nonlinearity) |\n",
    "| $y_t = \\tanh(W_y h_t + b_y) \\in \\mathbb{R}^{d_{out}}$ | Output at time $t$ |\n",
    "\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "We assume that the hidden states have the same dimension througout time.\n",
    "\n",
    "---\n",
    "\n",
    "**Algebraic Modification**\n",
    "\n",
    "The concatonation operation over vector and matrices provided a clean way to represent multiple linear operations:\n",
    "\n",
    "$\\tilde{x}_t = \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_{in} + d_{h}}$\n",
    "\n",
    "$W = [W_x \\ W_h] \\in \\mathbb{R}^{(d_h \\times d_{in} + d_h)}$\n",
    "\n",
    "Everything remains the same, just makes our work tidier. \n",
    "\n",
    "$W \\tilde{x}_t = [W_x \\ W_h] \\cdot \\left[x_t \\ h_{t-1}\\right]^T \\in \\mathbb{R}^{d_h}$\n",
    "\n",
    "So now when applying our non-linear activation function we have: \n",
    "\n",
    "$$f_W(W \\tilde{x}_t + b) \\in \\mathbb{R}^{d_h}$$\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "$W$ is applied to **ALL** inputs and internal representation (unless said otherwise), it's independent of time.\n",
    "\n",
    "---\n",
    "\n",
    "**Weight Representation**\n",
    "\n",
    "|Weight Notation | One-Line Explanation | How it behaves | Example | \n",
    "|----------------|----------------------|----------------|---------|\n",
    "| $$W_h$$ | How **past** information **Influences** the **present**| If our values are binary $\\{0,1\\}$, the it represents what to keep and that to forget <br> If our values is $[0,1]$ then it represents what to amplify and what to supress | \"I did **not** enjoy the movie\" <br> Learns that negation persists, thereby affecting the words \"enjoy\"|\n",
    "| $$W_x$$ | How to **interprate current input** | What part of the input matters <br> how strong is their influence currently | If $x_t$ is a word embedding, $W_x$ learns: <br> which emedding dimension are important <br> How strongly a word should influence memory|\n",
    "| $$W_y$$ | How to **interprete** the internal state to **make** a **descision** | This doesn't affect the memory <br> Can be viewed like the \"classic\" wights in a DNN | Hidden state encodes: $\\{subject, negation, Action\\}$ <br> $W_y$ learns which hidden features to output label.|\n",
    "\n",
    "---\n",
    "\n",
    "**Hyperbolic Function Reminder**\n",
    "\n",
    "Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Definition**\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{2}{1 + e^{-2x}} - 1$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "**Range:** $(-1, 1)$\n",
    "\n",
    "**Shape:** S-shaped curve (zero-centered)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/tanh.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "| **✅ Advantages** | **❌ Disadvantages** |\n",
    "|------------------|---------------------|\n",
    "| **Zero-centered**: Output in $(-1, 1)$ → better gradient flow than sigmoid | **Vanishing gradients**: For $\\|x\\| > 3$, gradient $\\approx 0$ → problem persists in very deep networks |\n",
    "| **Stronger gradients**: $\\tanh'(x)_{\\text{max}} = 1$ (4× stronger than sigmoid!) | **Computationally expensive**: Still requires exponential calculations |\n",
    "| **Symmetric**: Easier optimization, less bias in weight updates | **Saturation**: Can still saturate at extremes ($\\pm 1$) |\n",
    "| **Smooth and differentiable**: Good gradient properties everywhere | **Not ideal for deep networks**: ReLU typically performs better |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab631e9",
   "metadata": {},
   "source": [
    "## Unrolling Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a567685",
   "metadata": {},
   "source": [
    "We shall now go through the training flow of the structures we introduced above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30729568",
   "metadata": {},
   "source": [
    "### Forward: Many to One \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/Many2one.png\" width=\"510\"/>\n",
    "<img src=\"../images/chap10/VFWDRNN.png\" width=\"313\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a747509",
   "metadata": {},
   "source": [
    "### Backward: Many to One\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/ManyToOneBPP.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "We can directly compute the gradient of the loss with respect to the output:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial y_T} = \\frac{\\partial l(\\tanh(W_y h_T + b_y), y_{gt})}{\\partial y_T}\n",
    "$$\n",
    "\n",
    "Now, we apply the chain rule recursively through time:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_T} &= \\frac{\\partial \\mathbf{L}}{\\partial y_T} \\frac{\\partial y_T}{\\partial h_T} \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\\\\\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial z_T} \\frac{\\partial z_T}{\\partial h_{T-1}} \\\\\\\\\n",
    "&= \\frac{\\partial \\mathbf{L}}{\\partial h_T} \\left(1 - \\tanh^2(z_T)\\right) W_h^T \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_{T-2}} &= \\frac{\\partial \\mathbf{L}}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\\\\\\\\n",
    "\\vdots \\\\\\\\\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial h_0} &= \\frac{\\partial \\mathbf{L}}{\\partial h_1} \\frac{\\partial h_1}{\\partial h_0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: The derivative of the activation function (tanh) is applied **elementwise**.\n",
    "\n",
    "---\n",
    "\n",
    "We also need to compute the gradients with respect to all parameters: $W_x$, $W_y$, $W_h$, $b$, $b_y$.\n",
    "\n",
    "For example, the partial derivative of the loss with respect to $W_x$ at time $t$ is:\n",
    "$$\n",
    "\\left. \\frac{\\partial L}{\\partial W_x} \\right|_{t} = \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "The **total** gradient is the sum over all time steps:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = \\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_x} \\right|_{t=i}\n",
    "$$\n",
    "\n",
    "---\n",
    "<div align=\"center\">\n",
    "\n",
    "**Summary Table of Gradients:**\n",
    "\n",
    "| **Derivative** | **Formula** |\n",
    "|---------------|-------------|\n",
    "| $$\\frac{\\partial L}{\\partial W_x}$$ | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_x} \\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial W_h}$$ | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial W_h} \\right\\|_{t=i}$$ |\n",
    "| $$\\frac{\\partial L}{\\partial b}$$   | $$\\sum_{i=1}^T \\left. \\frac{\\partial L}{\\partial b} \\right\\|_{t=i}$$   |\n",
    "| $$\\frac{\\partial L}{\\partial b_y}$$ | $$\\frac{\\partial L}{\\partial y_T} \\cdot \\frac{\\partial y_T}{\\partial b_y}$$ |\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2464dd",
   "metadata": {},
   "source": [
    "### Forward: Many to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be9205",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/M2Mfwd.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "$$\\begin{align} \n",
    "z_1 &= W_x x_1 + W_h h_0 + b = W(x_1, h_0)^T + b\\\\\n",
    "h_1 &= f_W(z_1) \\\\\n",
    "y_1 &= f_{W_y}(W_yh_1 + b_y) \\\\\n",
    "L_1 &= l(y_1, y_{1_{GT}}) \\\\\n",
    "\\\\\n",
    "z_2 &= W_x x_2 + W_h h_1 + b = W(x_2, h_1)^T + b\\\\\n",
    "h_2 &= f_W(z_2) \\\\\n",
    "y_2 &= f_{W_y}(W_yh_2 + b_y) \\\\\n",
    "L_2 &= l(y_2, y_{2_{GT}})\\\\\n",
    "\\vdots \\\\\n",
    "z_T &= W_x x_t + W_h h_{T-1} + b = W(x_T, h_{T-1})^T + b \\\\\n",
    "h_T &= f_W(z_T) \\\\\n",
    "y_T &= f_{W_y}(W_yh_T + b_y) \\\\\n",
    "L_T &= l(y_T, y_{T_{GT}})\n",
    "\\end{align}$$\n",
    "\n",
    "$$L = \\sum_{t=1}^T L_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38faacee",
   "metadata": {},
   "source": [
    "### Backward: Many to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387ce8d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/M2Mbckwd.png\" width=\"510\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34a24f",
   "metadata": {},
   "source": [
    "In the **Many to Many**, evey time step $t$ has its own $y_t$ and $L_t$ so: \n",
    "\n",
    "$$L = \\sum_{i=1}^T L_i$$\n",
    "\n",
    "The gradient w.r.t each parameter is **summed over all time steps**\n",
    "$$\\frac{\\partial L}{\\partial b_y} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial b_y}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_y} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial W_y}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c468b",
   "metadata": {},
   "source": [
    "### Computation Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790e5d8",
   "metadata": {},
   "source": [
    "Running the backpropagation on a long sequence can become **very** expensive, so we must devise a way to complete the task efficiently.\n",
    "\n",
    "**Divide and Conquer**\n",
    "\n",
    "Suppose we have a sequence length of size $T$, depending on the constraints of the task and the hardware we'll choose our window size $k$, a **hyperparameter** to create $\\lceil \\frac{T}{k} \\rceil$ chunks.\n",
    "\n",
    "```\n",
    "Initialise Internal state h_0\n",
    "for chunk i = 1 to k:\n",
    "    Run Forward Pass on chunk i\n",
    "    store final internal state of h_{i•k}\n",
    "    Run Backward Pass on chunk i\n",
    "    Set h_0 = h_{i*k} for next chunk\n",
    "```\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/chap10/optBPP1.png\" width=\"225\"/>\n",
    "  <img src=\"../images/chap10/optBPP2.png\" width=\"260\"/>\n",
    "  <img src=\"../images/chap10/optBPP3.png\" width=\"=100\" height=\"220\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b43246",
   "metadata": {},
   "source": [
    "### Gradient and Memory Issues "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5197f",
   "metadata": {},
   "source": [
    "Consider the general chain rule applied on the internal state $h_1$ and consider if $T=1000$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{L}}{\\partial h_1} = \\left(\\frac{\\partial \\mathbf{L}}{\\partial L_T} \\frac{\\partial L_T}{\\partial y_T} \\frac{\\partial y_T}{\\partial h_T}\\right) \\cdot \\left(\\frac{ \\partial h_T}{\\partial h_{T-1}} \\right) \\cdot \\left( \\frac{ \\partial h_{T-1}}{\\partial h_{T-2}} \\right) \\cdots \\left (\\frac{ \\partial h_{2}}{\\partial h_{1}}\\right)$$\n",
    "\n",
    "If our largest eigen-value of $W_h$ (which is what these partials produce) is less than one then **Gradient will vanish**!<br>\n",
    "If our smallest eigen-value of $W_h$ is greater than 1 then **Gradient can explode**<br>\n",
    "\n",
    "Both of these issues, represents information being **lost** (when gradient vanishes) or **destabilized** (when gradient explodes), which occurs more frequently the further back in the sequence we propagate, or forward feed. For us, this means that the integrity of contextual information isn't sustained. This defeats the point of this model's task.\n",
    "\n",
    "This means that the **vanilla RNN** is only able to retain information from recent time steps **short-term** memory.\n",
    "\n",
    "This motivates the initial development of a model that keeps track of information independant to the resulting matrix multiplication **Long Short-Term Memory (LSTM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a105e",
   "metadata": {},
   "source": [
    "## RNN with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec1a3d",
   "metadata": {},
   "source": [
    "We can define a RNN **Many-One** connecting **One-Many**, this is otherwise known as **Encoder-Decoder** Network.\n",
    "\n",
    "##### As before:\n",
    "\n",
    "**Input:** $\\{x_1, x_2, x_3, \\dots, x_T\\}$<br>\n",
    "**Output:** $\\{y_1, y_2, y_3, \\dots, y_T\\}$<br>\n",
    "**Internal State/Encoder:** $h_t = f_{W_e}(x_t, h_{t-2})$ (e for encoder)\n",
    "\n",
    "##### New:<br>\n",
    "Our **final encoder** $h_T$ outputs to our **first decoder** state therfore: $s_0 = h_T$\n",
    "\n",
    "\n",
    "**Internal State/Decoder:** $s_t = f_{W_d}(y_{t-1}, s_{t-1})$ (d for decoder)\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/encode_decode.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "**Problem** In the current architechture, the decorder internal states $s_t$ would need to remember the entire input sequence $\\{x_1, x_2, \\dots, x_T\\}$, this is an issue since they may not have enough space to store all relevant information from long or complex input sequences. Also, as the sequence gets longer, the models becomes weaker at extracting and retaining current relevant information.\n",
    "\n",
    "#### Add Context vector\n",
    "\n",
    "A **context vector** is a vector that summarizes the relevant information fromt he encoder's internal states to the decoder at each step. \n",
    "\n",
    "**In Classic encoder-decoder**\n",
    "\n",
    "$$c = h_T$$\n",
    "\n",
    "**Internal State/Decoder:** <br> $$s_t = f_{W_d}(y_{t-1}, s_{t-1}, c) = \\tanh(W_y y_{t-1} + W_s s_{t-1} + W_c c + b_s)$$\n",
    "\n",
    "  \n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap10/attneCon.png\" width=\"510\"/>\n",
    "</div>\n",
    "\n",
    "So we've improved from the vanilla Encoder-Decoder model, where we have a context to store the information retained from the encoder architecture. \n",
    "\n",
    "**Problem**\n",
    "This is still static, in a model class we defined to be dynamic by nature.\n",
    "\n",
    "- The context vector is a static summary. \n",
    "- All information the decoder uses is fixed after the encoding, regardless of what the decoder is currently generating. \n",
    "- The decoder cannot dynamicaly focus on different parts of the input sequence as it generates each output, it's still too general and **not precise to current context**\n",
    "\n",
    "\n",
    "**With attention** \n",
    "\n",
    "$$c_t = \\sum_{i=1}^T \\alpha_{t,i}h_i$$\n",
    "\n",
    "$$\n",
    "s_t = f_{W_d}(y_{t-1}, s_{t-1}, c) = \\tanh(W_y y_{t-1} + W_s s_{t-1} + W_c c_t + b_s)\n",
    "$$\n",
    "\n",
    "|**Step 1** |**Step 2** | **Step 3** |\n",
    "|-----------|-----------|------------|\n",
    "|<img src=\"../images/chap10/atten1.png\" width=\"360\"/> | <img src=\"../images/chap10/atten2.png\" width=\"410\"/> | <img src=\"../images/chap10/atten3.png\" width=\"460\"/>| "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
