{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b696233",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "In the previous chapter we saw that as the number of hidden units increase, the greater the complexity of a function can be described using a shallow network. <br>\n",
    "The transition from a shallow network to a Deep Network came to existence from functions that would have required so many hidden units that it would be impractical. <br>\n",
    "Deep Networks provide the advantage which with the same number of parameters we're able to produce many more linear regions than a shallow network, which means, they are more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a3598",
   "metadata": {},
   "source": [
    "## Composing neural network\n",
    "\n",
    "Suppose we have two shallow networks, we want to know how can we \"concatonate\" these networks. \n",
    "\n",
    "$$\\boxed{\\begin{aligned}\n",
    "h_1 &= a[\\theta_{10}+ \\theta_{11}x]\\\\\n",
    "h_2 &= a[\\theta_{20}+ \\theta_{21}x]\\\\\n",
    "h_3 &= a[\\theta_{30}+ \\theta_{31}x]\\\\\n",
    "\\textcolor{brown}{y}   &= \\textcolor{brown}{\\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3}\n",
    "\\end{aligned}}$$\n",
    "\n",
    "$$\\boxed{\\begin{aligned}\n",
    "h_1' &= a[\\theta_{10}'+ \\theta_{11}'y]\\\\\n",
    "h_2' &= a[\\theta_{20}'+ \\theta_{21}'y]\\\\\n",
    "h_3' &= a[\\theta_{30}'+ \\theta_{31}'y]\\\\\n",
    "y'    &= \\phi_0' + \\phi_1' h_1' + \\phi_2' h_2' + \\phi_3' h_3'\n",
    "\\end{aligned}}$$\n",
    "\n",
    "Notice that the output from the first shallow network is directly fed through as the input of the next shallow network.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img  src=\"images/chap3/concatnet.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "</div>\n",
    "\n",
    "The effect of this is that the input function we at most 3 regions is now passed through another function that can produce at most 3 regions thus <br> we obtain a total of 9 regions which is already greater than a function with 6 regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df89b82",
   "metadata": {},
   "source": [
    "## Connecting to Deep Networks\n",
    "\n",
    "The concatonation technique is actually a special case of a deep network with two hidden layer\n",
    "\n",
    "Consider the above example we can open out the final activation functions:\n",
    "\n",
    "$$h_1' = a[\\theta_{10}+ \\theta_{11}\\textcolor{brown}{y}] = a[\\theta_{10} + \\theta_{11}\\textcolor{brown}{(\\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3)}] = a[\\textcolor{cyan}{\\theta_{10} + \\theta_{11}\\phi_0} + \\textcolor{royalblue}{\\theta_{11}\\phi_1 }h_1 + \\textcolor{gold}{\\theta_{11}\\phi_2 }h_2 + \\textcolor{pink}{\\theta_{11}\\phi_3} h_3]$$\n",
    "\n",
    "$$h_2' = a[\\theta_{20}+ \\theta_{21}\\textcolor{brown}{y}] = a[\\theta_{20} + \\theta_{21}\\textcolor{brown}{(\\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3)}] = a[\\textcolor{cyan}{\\theta_{20} + \\theta_{21}\\phi_0 } + \\textcolor{royalblue}{\\theta_{21}\\phi_1} h_1 + \\textcolor{gold}{\\theta_{21}\\phi_2 }h_2 + \\textcolor{pink}{\\theta_{21}'\\phi_3} h_3]$$\n",
    "\n",
    "$$\n",
    "h_3' = \\underbrace{a[\\theta_{30}+ \\theta_{31}\\textcolor{brown}{y}]}_{\\text{Feedback Form}} \n",
    "= \\underbrace{a[\\theta_{30} + \\theta_{31}\\textcolor{brown}{(\\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3)}]}_{\\text{Substitute } y} \n",
    "= \\underbrace{a[\\textcolor{cyan}{\\theta_{30} + \\theta_{31}\\phi_0} + \\textcolor{royalblue}{ \\theta_{31}\\phi_1 }h_1 + \\textcolor{gold}{\\theta_{31}\\phi_2} h_2 + \\textcolor{pink}{\\theta_{31}\\phi_3} h_3]}_{\\text{Group by Terms}}\n",
    "$$\n",
    "\n",
    "Then we can simplify the terms:\n",
    "$$h_1' = a[\\textcolor{cyan}{\\psi_{10}} + \\textcolor{royalblue}{\\psi_{11}} h_1 + \\textcolor{gold}{\\psi_{12}} h_2 + \\textcolor{pink}{\\psi_{13}} h_3]$$\n",
    "\n",
    "$$h_2' = a[\\textcolor{cyan}{\\psi_{20}} + \\textcolor{royalblue}{\\psi_{21}} h_1 + \\textcolor{gold}{\\psi_{22}} h_2 + \\textcolor{pink}{\\psi_{23}} h_3]$$\n",
    "\n",
    "$$h_3' = a[\\textcolor{cyan}{\\psi_{30} }+ \\textcolor{royalblue}{\\psi_{31}} h_1 + \\textcolor{gold}{\\psi_{32}} h_2 + \\textcolor{pink}{\\psi_{33}} h_3]$$\n",
    "\n",
    "Finally as before: \n",
    "\n",
    "$$y' = \\phi_0' + \\phi_1'h_1 + \\phi_2' h_2 + \\phi_3' h_3$$\n",
    "\n",
    "This represents a broader family because latter versions of $h_1', h_2' \\text{ and } h_3'$ the nine slope parameters $\\psi_{11} \\ \\psi_{12} \\ \\psi_{13} \\dots \\psi_{33}$ can be set any values whereas the first version that was shown are constrained by the outer product $\\left[\\theta_{11}', \\ \\theta_{21}', \\ \\theta_{31}'\\right]^T \\left[\\phi_1, \\ \\phi_2, \\ \\phi_3 \\right] = \\begin{bmatrix} \\theta_{11}'\\phi_1 & \\theta_{11}'\\phi_2 & \\theta_{11}'\\phi_3 \\\\ \\theta_{21}'\\phi_1 & \\theta_{21}'\\phi_2 & \\theta_{21}'\\phi_3 \\\\ \\theta_{31}'\\phi_1 & \\theta_{31}'\\phi_2 & \\theta_{31}'\\phi_3 \\end{bmatrix}$. (This produces a matrix (3x3))\n",
    "\n",
    "Note that each row is a scalar multiple of every other row. \n",
    "Which means there's only a single linearly independent row. In contrast with the unconstrained $\\psi$ matrix allow all 9 parameters to vary independently, so it can have rank up to 3, thus making the network more expressive and thus more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b016d",
   "metadata": {},
   "source": [
    "#### Another interpretation on deep network construction\n",
    "\n",
    "1. The three hidden units $h_1, h_2, h_3$ are constructed by forming linear functions (as we know) from the input and then passed to the activation functions (individually).\n",
    "2. We then use the results (view this as preactivation from **second layer**) to form another three linear functions, and now we're back to our shallow network structure.\n",
    "3. We take these (second layer) pre-activation functions and apply ReLU to each of the function.\n",
    "4. Apply a weighted sum on the results on the activated functions from the third layer.\n",
    "\n",
    "If one really wanted it could be written in one shot...\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y' &= \\phi_0' + \\phi_1' a\\left[\\psi_{10} + \\psi_{11}a[\\theta_{10} + \\theta_{11}x] + \\psi_{12}a[\\theta_{20} + \\theta_{21}x] + \\psi_{13}a[\\theta_{30} + \\theta_{31}x]\\right]\\\\\n",
    "&\\quad + \\phi_2' a\\left[\\psi_{20} + \\psi_{21}a[\\theta_{10} + \\theta_{11}x] + \\psi_{22}a[\\theta_{20} + \\theta_{21}x] + \\psi_{23}a[\\theta_{30} + \\theta_{31}x]\\right]\\\\\n",
    "&\\quad + \\phi_3' a\\left[\\psi_{30} + \\psi_{31}a[\\theta_{10} + \\theta_{11}x] + \\psi_{32}a[\\theta_{20} + \\theta_{21}x] + \\psi_{33}a[\\theta_{30} + \\theta_{31}x]\\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "However, it can cloud a lot of the aforementioned insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e5134",
   "metadata": {},
   "source": [
    "### Hyperparameters and Termonolgy\n",
    "\n",
    "Indeed this can be extended to even larger deep networks, thus in general we have a few terms to keep in mind:\n",
    "\n",
    "$\\textcolor{lightblue}{Width \\ of \\ a \\ network :=} \\text{The number of hidden units in each layer}$\n",
    "\n",
    "$\\textcolor{lightblue}{Depth \\ of \\ a \\ network :=} \\text{The number of hidden layers}$\n",
    "\n",
    "$\\textcolor{lightblue}{Capacity \\ of \\ a \\ network :=} \\text{The total number of hidden units}$\n",
    "\n",
    "$\\text{Let K denote the depth of the network} \\\\ D_1, D_2, \\dots, D_K \\text{ the width of each layer}$\n",
    "\n",
    "The above quantities are refered to as $\\textcolor{lightblue}{hyperparameters}$, that are chosen before we learn the model parameters,<br> once these quantities are fixed does the model describe a family of functions, <br> where the values of these parameters will describe a perticular function within this family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edec1d",
   "metadata": {},
   "source": [
    "## Matrix Notation and General Formulation\n",
    "\n",
    "Deep neural network are going be handling complicated relations between provided inputs and desired outputs.<br>\n",
    "So we're going to see large Multi-layered networks, up until now we've been writing everything to gain good intuition behind a shallow network and a deep network etc. <br> However, writing all the linear functions and the activations is very tedious and cumbersome.<br>\n",
    "We convert from scalar multiplications and summation to vectors and matrix operations.\n",
    "\n",
    "To begin lets rewrite the above formulation in Matrix/Vector notation: \n",
    "\n",
    "\n",
    "$$ \\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3\\end{bmatrix} = a \\left[ \\begin{bmatrix} \\theta_{10} \\\\ \\theta_{20} \\\\ \\theta_{30} \\end{bmatrix} + \\begin{bmatrix} \\theta_{11} \\\\ \\theta_{21} \\\\ \\theta_{31} \\end{bmatrix}x \\right]$$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} h_1' \\\\ h_2' \\\\ h_3' \\end{bmatrix} = a \\left[ \n",
    "\\begin{bmatrix} \\textcolor{cyan}{\\psi_{10}} \\\\ \\textcolor{cyan}{\\psi_{20}} \\\\ \\textcolor{cyan}{\\psi_{30}} \\end{bmatrix} + \n",
    "\\begin{bmatrix} \n",
    "\\textcolor{royalblue}{\\psi_{11}} & \\textcolor{gold}{\\psi_{12}} & \\textcolor{pink}{\\psi_{13}} \\\\ \n",
    "\\textcolor{royalblue}{\\psi_{21}} & \\textcolor{gold}{\\psi_{22}} & \\textcolor{pink}{\\psi_{23}} \\\\ \n",
    "\\textcolor{royalblue}{\\psi_{31}} & \\textcolor{gold}{\\psi_{32}} & \\textcolor{pink}{\\psi_{33}} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3\\end{bmatrix} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "$$y' = \\phi_0' + \\left[ \\phi_1' \\quad \\phi_2' \\quad \\phi_3' \\right] \\begin{bmatrix} h_1' \\\\ h_2' \\\\ h_3' \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$\\text{Let } h_k \\text{ be a vector describing the hidden units of hidden layer k (do not confuse with the above } h_k \\text{ which is a single activation unit) }$\n",
    "$\\text{Let } \\beta_k \\text{ be a vector describing the bias vector that's being applied on the results of the Kth layer.} \\\\ \\text{That is being summed for the (K+1) hidden layer} \\\\ \\text{Let } \\Omega_K \\text{ be the matrix describing the slopes being applied to the Kth layer}$\n",
    "\n",
    "With these notations we'll describe the general proccess of the deep neural network. \n",
    "\n",
    "$$\\begin{align}\n",
    "h_1 &= a\\left[\\beta_0 + \\Omega_0 x\\right] \\\\ \n",
    "h_2 &= a\\left[\\beta_1 + \\Omega_1 h_1\\right] \\\\\n",
    "h_3 &= a\\left[\\beta_2 + \\Omega_2 h_2\\right] \\\\\n",
    "\\dots &= \\dots \\\\\n",
    "h_K &= a\\left[\\beta_{K-1} + \\Omega_{K-1} h_{K-1}\\right] \\\\\n",
    "y &= \\beta_K + \\Omega_K h_K\n",
    "\\end{align}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img  src=\"images/chap3/fcNet.png\" alt=\"ReLU Function\" width=\"700\" />\n",
    "</div>\n",
    "\n",
    "#### Dimsionality matching \n",
    "\n",
    "Because we now work with vectors and matrices, tracking input/output dimensions across layers is crucial for training, fitting the model, and implementing these networks in code.\n",
    "\n",
    "Our parameters is summurised concicely as follows: $\\phi = \\left\\{\\beta_{k}, \\Omega_{k}\\right\\}_{k=0}^K$\n",
    "\n",
    "If the Kth layer has $D_k$ hidden units then the bias vector $\\beta_{K-1} \\in \\mathbb{R}^{D_k}$ and so $\\Omega_{K-1} \\in \\mathbb{R}^{D_k \\times D_{k-1}}$\n",
    "The last bias vector $\\beta_K \\in \\mathbb{R}^{D_o}$\n",
    "\n",
    "The first weight matrix $\\Omega_{0} \\in \\mathbb{R}^{D_1 \\times D_i}$\n",
    "\n",
    "The last weight matrix $\\Omega_{K} \\in \\mathbb{R}^{D_o \\times D_K}$\n",
    "\n",
    "This can be written recursively in a single function:\n",
    "\n",
    "$$\n",
    "y = \\beta_K + \\Omega_K\\, a\\Big[\\,\\beta_{K-1} + \\Omega_{K-1} a\\Big[\\, \\dots + \\beta_2 + \\Omega_2 a\\big[\\, \\beta_1 + \\Omega_1 a[\\, \\beta_0 + \\Omega_0 x \\,] \\big] \\Big] \\Big]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ff042",
   "metadata": {},
   "source": [
    "## Number of linear regions per parameter\n",
    "\n",
    "#### Shallow Network\n",
    "\n",
    "- In a shallow network with one input and output where D > 2 hidden units can create up to D+1 linear regions: \n",
    "$$ \\text{D hidden units means you can bend the line in at most D places thereby forming D+1 Linear regions}$$\n",
    "- In this shallow network we'd have 3D + 1 parameters\n",
    "\n",
    "#### Deep Network\n",
    "\n",
    "- In a deep network with one input and output, K layers of D > 2 hidden units can create a function with up to $(D+1)^k$ linear regions:\n",
    "    - $\\text{The shallow network is base case for a deep network}$\n",
    "  $$\\begin{align}N_k &= (D+1)N_{K-1} \\\\ &= (D+1)(D+1)N_{K-2}  \\\\ &= \\quad \\vdots  \\\\N_K &= (D+1)^K \\end{align} $$\n",
    "- In the deep network we'd have 3D + 1 (K-1)D(D+1) parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4a63b",
   "metadata": {},
   "source": [
    "### Discrete Neural Lookup\n",
    "\n",
    "Below presents an example of constructing a neural network that can **perfectly memorize and retrieve** a set of $N$ unique binary input vectors $\\mathbf{x}_i \\in \\{0,1\\}^d$ and their associated targets $y_i \\in \\mathbb{R}$.<br>This construction demonstrates how a simple two-layer network can act as a lookup table, outputting the correct $y_k$ for any given input $\\mathbf{x}_k$ from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5703f3",
   "metadata": {},
   "source": [
    "**Algorithm: Discrete Neural Lookup (DNL)**\n",
    "\n",
    "Given $N$ unique inputs $\\mathbf{x}_i \\in \\{0, 1\\}^d$ and targets $y_i \\in \\mathbb{R}$:\n",
    "\n",
    "1. **Mapping Function ($\\phi$):**  \n",
    "   $\\phi(\\mathbf{x}) = 2\\mathbf{x} - \\mathbf{1}$, mapping $\\{0,1\\}^d \\to \\{-1,1\\}^d$\n",
    "\n",
    "2. **Layer 1 (Detection):**  \n",
    "   Define weight matrix $\\mathbf{W} \\in \\mathbb{R}^{N \\times d}$ and bias $\\mathbf{b} \\in \\mathbb{R}^N$: <br>\n",
    "   $\n",
    "   \\mathbf{W} = \\begin{bmatrix} \\phi(\\mathbf{x}_1)^T \\\\ \\vdots \\\\ \\phi(\\mathbf{x}_N)^T \\end{bmatrix}, \\quad\n",
    "   \\mathbf{b} = \\begin{bmatrix} 1-d \\\\ \\vdots \\\\ 1-d \\end{bmatrix}\n",
    "   $\n",
    "   <br>\n",
    "   <br>\n",
    "   $\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{W}\\phi(\\mathbf{x}) + \\mathbf{b})$\n",
    "\n",
    "3. **Layer 2 (Selection):**  \n",
    "   $f(\\mathbf{x}) = \\sum_{i=1}^N h_i y_i$\n",
    "\n",
    "4. **Correctness:**  \n",
    "   For input $\\mathbf{x}_k$, only $h_k = 1$ (others are $0$), so $f(\\mathbf{x}_k) = y_k$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
