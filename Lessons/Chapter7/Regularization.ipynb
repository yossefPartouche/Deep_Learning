{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158a436a",
   "metadata": {},
   "source": [
    "# Regularization & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bff50b",
   "metadata": {},
   "source": [
    "In the previous section we discussed the main three sources of error of generalisation, in this section we discuss $refularization$ techniques. \n",
    "\n",
    "These are methods that reduce the gap between the training and testing, there are:\n",
    "\n",
    "1. Explicit Regularization\n",
    "2. Implicit Regularization\n",
    "3. Heuristic Methods\n",
    "\n",
    "Finally, we will discuss in great detail hyperparameters invovled in a Deep Network and methods for finding optimal Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7020714",
   "metadata": {},
   "source": [
    "## Explicit Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be849ae5",
   "metadata": {},
   "source": [
    "$\\text{Consider fitting a model } f[x, \\phi] \\text{with parameters } \\phi \\text{ using a training set } \\{x_i, y_i\\} \\text{ of input/output pairs. We seek:}$\n",
    "\n",
    "$$\\begin{align} \\hat{\\phi} \n",
    "&= \\mathbf{argmin}_{\\phi}[L[\\phi]] \\\\\n",
    "&= \\mathbf{argmin}_{\\phi}[\\sum_{i=1}^N l_i[x_i, y_i]]\n",
    "\\end{align}$$\n",
    "\n",
    "So far in the previous section we've mentioned that by providing more data the model will be able to generalise better. \n",
    "\n",
    "We now focus with the fact in mind that we don't have so much data so instead we can apply **constraints** on the model.\n",
    "\n",
    "$$\\mathbf{argmin}_{\\phi}\\left(\\sum_{i=1}^N l_i[x_i, y_i] + \\lambda \\cdot g[\\phi]\\right)$$\n",
    "\n",
    "$g[\\phi]: \\quad $ A scalar function which provides additional pentalties to parameters that don't perform well. <br>\n",
    "$0 \\le \\lambda: \\quad$ Controls how much \"should\" this penalty contribute to the overall loss function.\n",
    "\n",
    "- So why does this work? \n",
    "- What's happening by adding this regularization term?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09837803",
   "metadata": {},
   "source": [
    "### Probabilistic view\n",
    "\n",
    "\n",
    "$\\text{Recall the Maximum Likelihood Criterion: }$\n",
    "\n",
    "$$\\hat{\\phi} = \\mathbf{argmax}_{\\phi}\\left[\\prod_{i=1}^N Pr(y_i | x_i, \\phi) \\right]$$\n",
    "\n",
    "The Rgularization term can be considered as a $Prior \\ Pr(\\phi)$ term, that represents knowledge we have on the parameters **before** we observe the data therefore: \n",
    "\n",
    "$$\\hat{\\phi} = \\mathbf{argmax}_{\\phi}\\left[\\prod_{i=1}^N Pr(y_i | x_i, \\phi)Pr(\\phi) \\right]$$\n",
    "\n",
    "We then apply the NLL: \n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\phi} \n",
    "&= \\mathbf{argmin}_{\\phi}\\left[-\\log{\\prod_{i=1}^N Pr(y_i | x_i, \\phi)Pr(\\phi)}\\right] \\\\\n",
    "&= \\mathbf{argmin}_{\\phi} \\left[- \\sum_{i=1}^N Pr(y_i | x_i, \\phi) + Pr(\\phi) \\right] \\\\\n",
    "&= \\mathbf{argmin}_{\\phi} \\left[ - \\left(\\sum_{i=1}^N Pr(y_i | x_i, \\phi) + \\sum_{i=1}^N Pr(\\phi)\\right) \\right] \\\\\n",
    "&= \\mathbf{argmin}_{\\phi}\\left(\\sum_{i=1}^N l_i[x_i, y_i] + \\lambda \\cdot g[\\phi]\\right)\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5a6d7",
   "metadata": {},
   "source": [
    "### L2 Regularization (Weight Decay)\n",
    "\n",
    "**Definition:**\n",
    "$$\\text{L2}[\\phi] = \\|\\boldsymbol{\\phi}\\|^2 = \\sum_{j} \\phi_j^2$$\n",
    "\n",
    "**Key Properties:**\n",
    "- Typically applied only to **weights**, not bias terms → hence called **weight decay**\n",
    "- Encourages smaller weight magnitudes → produces **smoother** output functions\n",
    "- Hyperparameter $\\lambda$ controls the strength of regularization\n",
    "\n",
    "**Why Does This Help?**\n",
    "\n",
    "| Scenario | Effect of L2 Regularization |\n",
    "|----------|---------------------------|\n",
    "| **Overfitting** | Forces the model to balance fitting training data against keeping weights small. The network can't memorize noise because large weights are penalized. |\n",
    "| **Over-parameterization** | When the model has excess capacity (especially in regions with sparse/no training data), L2 favors smooth interpolation between nearby training points rather than erratic predictions. |\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Think of L2 regularization as applying \"friction\" to the weights:\n",
    "- Without regularization: Weights can grow arbitrarily large to perfectly fit every training point (including noise)\n",
    "- With L2 regularization: Large weights are expensive, so the model uses smaller weights and creates smoother, more generalizable functions\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Without L2: w = [10.2, -8.5, 12.1, -9.8] → fits training data perfectly but wiggly\n",
    "With L2:    w = [2.1, -1.8, 2.3, -2.0]  → fits training data well, smoother predictions\n",
    "```\n",
    "\n",
    "**Probabilistic Interpretation:**\n",
    "\n",
    "L2 regularization corresponds to placing a **Gaussian prior** on the parameters:\n",
    "$$\\text{Pr}(\\phi) = \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "This encodes the belief that parameters should be small and centered around zero before seeing any data.\n",
    "\n",
    "**Practical Tips:**\n",
    "- Start with $\\lambda \\in [0.001, 0.1]$ and tune via validation set\n",
    "- Larger $\\lambda$ → stronger regularization → simpler model\n",
    "- Too large $\\lambda$ → underfitting (model too constrained)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687f51e",
   "metadata": {},
   "source": [
    "### L1 Regularization (Lasso)\n",
    "\n",
    "**Definition:**\n",
    "$$\\text{L1}[\\phi] = \\|\\boldsymbol{\\phi}\\|_1 = \\sum_{j} |\\phi_j|$$\n",
    "\n",
    "**Key Properties:**\n",
    "- Penalizes the **absolute values** of weights (not squared)\n",
    "- Encourages **sparsity** → drives many weights exactly to zero\n",
    "- Acts as an automatic **feature selection** mechanism\n",
    "- Hyperparameter $\\lambda$ controls the strength of regularization\n",
    "\n",
    "\n",
    "**Why Does This Help?**\n",
    "\n",
    "| Scenario | Effect of L1 Regularization |\n",
    "|----------|---------------------------|\n",
    "| **Feature Selection** | Automatically identifies and eliminates irrelevant features by setting their weights to exactly zero. Useful when you suspect only a subset of features matter. |\n",
    "| **High-Dimensional Data** | When you have many input features but limited data, L1 creates simpler, more interpretable models by keeping only the most important features. |\n",
    "| **Overfitting** | Reduces model complexity by forcing the network to use fewer parameters, preventing it from memorizing noise. |\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Think of L1 regularization as a \"harsh judge\":\n",
    "- **L1** (lasso): Prefers few large weights, zeros out the rest → \"winner takes all\"\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Without L1: w = [2.3, -1.8, 0.4, -0.9, 1.2, 0.3, -0.5]  → uses all features\n",
    "With L1:    w = [4.1,  0.0, 0.0,  0.0, 2.8, 0.0,  0.0]  → only 2 features survive\n",
    "```\n",
    "\n",
    "**Probabilistic Interpretation:**\n",
    "\n",
    "L1 regularization corresponds to placing a **Laplace (double exponential) prior** on the parameters:\n",
    "$$\\text{Pr}(\\phi) = \\frac{1}{2b}\\exp\\left(-\\frac{|\\phi|}{b}\\right)$$\n",
    "\n",
    "This prior has a sharp peak at zero, explaining why L1 drives weights to exactly zero.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb3c55",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**L1 vs L2 Comparison:**\n",
    "\n",
    "| Property | L1 (Lasso) | L2 (Ridge) |\n",
    "|----------|-----------|-----------|\n",
    "| **Formula** | $\\sum_j \\|\\phi_j\\|$ | $\\sum_j \\phi_j^2$ |\n",
    "| **Effect on weights** | Sparse (many zeros) | Shrinks uniformly |\n",
    "| **Feature selection** | ✅ Yes (automatic) | ❌ No (keeps all features) |\n",
    "| **Solution uniqueness** | Not always unique | Always unique |\n",
    "| **Best for** | High-dimensional data, interpretability | Correlated features, smooth functions |\n",
    "| **Computational cost** | Higher (non-differentiable at 0) | Lower (smooth everywhere) |\n",
    "</div>\n",
    "\n",
    "**Example: L1 vs L2 Regularization**\n",
    "\n",
    "Given $\\mathbf{x} = [2, 1, 1, 1]$ and three weight vectors:\n",
    "- $\\mathbf{w}_1 = [0.5, 0, 0, 0]$\n",
    "- $\\mathbf{w}_2 = [0.125, 0.25, 0.25, 0.25]$\n",
    "- $\\mathbf{w}_3 = [0, 0.5, 0.5, 0]$\n",
    "\n",
    "All produce the same output: $\\mathbf{w} \\cdot \\mathbf{x} = 1$\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**Regularization Values:**\n",
    "\n",
    "| Weight Vector | L1 Norm | L2 Norm |\n",
    "|--------------|---------|---------|\n",
    "| $\\mathbf{w}_1$ | 0.5 | 0.25 |\n",
    "| $\\mathbf{w}_2$ | 0.875 | 0.203 |\n",
    "| $\\mathbf{w}_3$ | 1.0 | 0.5 |\n",
    "\n",
    "</div>\n",
    "\n",
    "**Conclusion:**\n",
    "- **L1 prefers $\\mathbf{w}_1$**: Sparse solution (3 zeros) with smallest L1 norm\n",
    "- **L2 prefers $\\mathbf{w}_2$**: Spreads weights evenly with smallest L2 norm\n",
    "\n",
    "**Why?**\n",
    "- L1 penalizes the sum of absolute values → favors fewer non-zero weights\n",
    "- L2 penalizes the sum of squares → favors many small weights over few large ones\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "\n",
    "\n",
    "| L1 Constraint Region | L2 Constraint Region |\n",
    "|:-------------------:|:-------------------:|\n",
    "| Diamond shape with corners on axes | Circular shape |\n",
    "| Optimal solution likely hits corner (→ sparsity) | Optimal solution hits smooth boundary |\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../images/chap7/L2L1.png\" width=\"450\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Practical Tips:**\n",
    "- Start with $\\lambda \\in [0.0001, 0.01]$ for L1 (typically smaller than L2)\n",
    "- Use L1 when you suspect many features are irrelevant\n",
    "- Use L2 when all features might contribute (even slightly)\n",
    "- Consider **Elastic Net** (L1 + L2 combined) for the best of both worlds:\n",
    "  $$g[\\phi] = \\lambda_1 \\sum_j |\\phi_j| + \\lambda_2 \\sum_j \\phi_j^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75680965",
   "metadata": {},
   "source": [
    "## Implicit Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afc5be",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Discovery:** Research from 2017-2019 revealed that SGD doesn't converge to arbitrary minima—it exhibits an implicit bias toward solutions with lower \"complexity\" (smaller norms, flatter minima), acting as a regularizer even without explicit penalty terms.\n",
    "\n",
    "---\n",
    "\n",
    "### Continuous vs. Discrete Gradient Descent\n",
    "\n",
    "**Discrete Update Rule:**\n",
    "$$\\phi_1 = \\phi_0 + \\alpha \\cdot g[\\phi_0]$$\n",
    "\n",
    "where $g[\\phi_0] = -\\frac{\\partial L}{\\partial \\phi}\\bigg|_{\\phi_0}$ is the negative gradient and $\\alpha$ is the step size.\n",
    "\n",
    "**Continuous Limit:**\n",
    "As $\\alpha \\to 0$, gradient descent follows the differential equation:\n",
    "$$\\frac{d\\phi}{dt} = g[\\phi]$$\n",
    "\n",
    "**Key Insight:** For typical step sizes $\\alpha$, the discrete and continuous versions converge to **different solutions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Error Analysis\n",
    "\n",
    "**Goal:** Find a correction term $g_1[\\phi]$ such that the modified continuous dynamics:\n",
    "\n",
    "$$\\frac{d\\phi}{dt} \\approx g[\\phi] + \\alpha g_1[\\phi] + O(\\alpha^2)$$\n",
    "\n",
    "produces the same trajectory as the discrete update.\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "Consider a Taylor expansion of the modified continuous solution around $\\phi_0$:\n",
    "\n",
    "$$\\phi[\\alpha] \\approx \\phi + \\alpha\\frac{d\\phi}{dt} + \\frac{\\alpha^2}{2}\\frac{d^2\\phi}{dt^2}\\bigg|_{\\phi=\\phi_0}$$\n",
    "\n",
    "Substituting the modified dynamics:\n",
    "\n",
    "$$\\phi[\\alpha] \\approx \\phi + \\alpha(g[\\phi] + \\alpha g_1[\\phi]) + \\frac{\\alpha^2}{2}\\left(\\frac{\\partial g[\\phi]}{\\partial \\phi}\\frac{d\\phi}{dt} + \\alpha\\frac{\\partial g_1[\\phi]}{\\partial \\phi}\\frac{d\\phi}{dt}\\right)\\bigg|_{\\phi=\\phi_0}$$\n",
    "\n",
    "Using $\\frac{d\\phi}{dt} = g[\\phi]$:\n",
    "\n",
    "$$\\phi[\\alpha] \\approx \\phi + \\alpha g[\\phi] + \\alpha^2\\left(g_1[\\phi] + \\frac{1}{2}\\frac{\\partial g[\\phi]}{\\partial \\phi}g[\\phi]\\right)\\bigg|_{\\phi=\\phi_0}$$\n",
    "\n",
    "**Matching Terms:** The first two terms match the discrete update $\\phi_0 + \\alpha g[\\phi_0]$. To ensure equivalence, the $O(\\alpha^2)$ term must vanish:\n",
    "\n",
    "$$g_1[\\phi] = -\\frac{1}{2}\\frac{\\partial g[\\phi]}{\\partial \\phi}g[\\phi]$$\n",
    "\n",
    "---\n",
    "\n",
    "### Equivalent Regularized Loss\n",
    "\n",
    "Since $g[\\phi] = -\\frac{\\partial L}{\\partial \\phi}$, the modified dynamics become:\n",
    "\n",
    "$$\\frac{d\\phi}{dt} \\approx -\\frac{\\partial L}{\\partial \\phi} - \\frac{\\alpha}{2}\\frac{\\partial^2 L}{\\partial \\phi^2}\\frac{\\partial L}{\\partial \\phi}$$\n",
    "\n",
    "This is equivalent to performing continuous gradient descent on the **regularized loss**:\n",
    "\n",
    "$$\\bar{L}[\\phi] = L[\\phi] + \\frac{\\alpha}{4}\\left\\|\\frac{\\partial L}{\\partial \\phi}\\right\\|^2$$\n",
    "\n",
    "**Proof:** Taking the gradient of $\\bar{L}[\\phi]$:\n",
    "\n",
    "$$\\frac{\\partial \\bar{L}}{\\partial \\phi} = \\frac{\\partial L}{\\partial \\phi} + \\frac{\\alpha}{2}\\frac{\\partial^2 L}{\\partial \\phi^2}\\frac{\\partial L}{\\partial \\phi}$$\n",
    "\n",
    "which matches the modified dynamics above.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**Implicit Regularization Term:**\n",
    "$$\\text{Penalty} = \\frac{\\alpha}{4}\\left\\|\\nabla L[\\phi]\\right\\|^2$$\n",
    "\n",
    "**Key Consequences:**\n",
    "- **Larger learning rate** $\\alpha$ → **stronger implicit regularization**\n",
    "- Penalizes regions with **steep gradients** → favors **flat minima**\n",
    "- Flat minima generalize better (less sensitive to parameter perturbations)\n",
    "\n",
    "**Formal Statement:**\n",
    "\n",
    "> Discrete gradient descent with step size $\\alpha$ is equivalent to continuous gradient flow on a regularized objective that includes a penalty proportional to the squared gradient norm.\n",
    "\n",
    "This explains why SGD (with finite learning rate and stochasticity) acts as an implicit regularizer, preferring solutions in flatter regions of the loss landscape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b035f",
   "metadata": {},
   "source": [
    "### Implicit Regularization in SGD\n",
    "\n",
    "**Extension to Stochastic Gradient Descent:**\n",
    "\n",
    "For SGD with batch size $B$, the effective regularized loss becomes:\n",
    "\n",
    "$$L_{\\text{SGD}}[\\phi] = L[\\phi] + \\frac{\\alpha}{4}\\left\\|\\frac{\\partial L}{\\partial \\phi}\\right\\|^2 + \\frac{\\alpha}{4B}\\sum_{b=1}^{B}\\left\\|\\frac{\\partial L_b}{\\partial \\phi} - \\frac{\\partial L}{\\partial \\phi}\\right\\|^2$$\n",
    "\n",
    "where $L_b$ is the loss on the $b$-th batch.\n",
    "\n",
    "**The Extra Term:**\n",
    "\n",
    "$$\\text{Batch variance penalty} = \\frac{\\alpha}{4B}\\sum_{b=1}^{B}\\left\\|\\nabla L_b - \\nabla L\\right\\|^2$$\n",
    "\n",
    "This is the **variance of batch gradients** — it measures how much different batches disagree about the gradient direction.\n",
    "\n",
    "**Key Consequences:**\n",
    "\n",
    "| Property | Effect |\n",
    "|----------|--------|\n",
    "| **Favors consensus** | Prefers solutions where all batches agree on gradient direction |\n",
    "| **Smaller batches** | Stronger variance penalty (larger $1/B$ coefficient) |\n",
    "| **Better generalization** | Solutions that fit *all* data consistently, not just some data extremely well |\n",
    "\n",
    "**Why SGD generalizes better:**\n",
    "\n",
    "Beyond just exploration via randomness, SGD's implicit regularization encourages solutions where the model performs uniformly well across all data subsets, rather than overfitting to specific subsets.\n",
    "\n",
    "**Practical insight:** Smaller batch sizes → stronger implicit regularization → often better test performance.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3d6bf",
   "metadata": {},
   "source": [
    "## Heuristic Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5c7bc",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "**Definition** \n",
    "\n",
    "Stopping training procedure before it has fully converged.\n",
    "\n",
    "**Effect** \n",
    "\n",
    "Reduce overfitting, if the model as already captures the general shape of the underlying function, but still hasn't had enough time to incoorperate the unwanted noise.\n",
    "\n",
    "There are potential views: \n",
    "\n",
    "1. Since the weights are initialised to small vlaues, they don't have enough time to become large, thus has a similar effect to L2 regularisation.\n",
    "2. Early stopping reduces the models complexity, so with the Bias/Variance trade-off we move away from the critical region and performace improves.\n",
    "   \n",
    "**How to determine when to Stop**\n",
    "\n",
    "1. **Split data:** Training set + Validation set (held-out data not used for training)\n",
    "2. **Track both losses:** Monitor training loss AND validation loss after each epoch\n",
    "3. **Stop when validation loss stops improving**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9a934",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "\n",
    "**Definition:**\n",
    "Train multiple models and combine their predictions to improve generalization.\n",
    "\n",
    "**Combination Methods:**\n",
    "\n",
    "| Task Type | Combination Strategy |\n",
    "|-----------|---------------------|\n",
    "| **Regression** | Mean or median of outputs |\n",
    "| **Classification** | Mean of pre-softmax activations, or mode of predicted classes |\n",
    "\n",
    "\n",
    "**Three Main Approaches:**\n",
    "\n",
    "1. **Different Initializations**\n",
    "   - Train same architecture with different random weight initializations\n",
    "   - Each model captures different aspects of the data\n",
    "   - Averaging reduces variance in uncertain regions\n",
    "\n",
    "2. **Bootstrap Aggregating (Bagging)**\n",
    "   - Resample training data with replacement to create multiple datasets\n",
    "   - Train separate model on each dataset\n",
    "   - Reduces overfitting by introducing diversity\n",
    "\n",
    "3. **Model Diversity**\n",
    "   - Use different hyperparameters (learning rate, architecture, etc.)\n",
    "   - Train different model families (CNNs, Transformers, etc.)\n",
    "   - Combines complementary strengths\n",
    "\n",
    "**Why it Works:** Individual models make different errors → averaging cancels out mistakes → better generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604e21d",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "**Definition:**\n",
    "Randomly set a fraction of hidden units to zero during training at each iteration.\n",
    "\n",
    "**Hyperparameter:** Dropout rate $p \\in [0, 1]$ (probability of dropping a unit)\n",
    "\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "1. **During Training:**\n",
    "   - For each training iteration, randomly select $p \\cdot n$ units to \"drop\" (set to 0)\n",
    "   - Forward/backward pass uses only the remaining $(1-p) \\cdot n$ active units\n",
    "   - Different random subset dropped at each iteration\n",
    "\n",
    "2. **During Testing:**\n",
    "   - Use **all** units (no dropout)\n",
    "   - Scale outputs by $(1-p)$ to account for more active units\n",
    "\n",
    "**Three Main Application Methods:**\n",
    "\n",
    "| Method | Description | Formula |\n",
    "|--------|-------------|---------|\n",
    "| **Standard Dropout** | Apply to hidden layers | $h_{\\text{train}} = h \\odot m$ where $m \\sim \\text{Bernoulli}(1-p)$ |\n",
    "| **Input Dropout** | Apply to input layer (use small $p \\approx 0.2$) | $x_{\\text{train}} = x \\odot m$ |\n",
    "| **DropConnect** | Drop weights instead of activations | $h = f(W \\odot m \\cdot x)$ |\n",
    "\n",
    "\n",
    "**Why it Works:**\n",
    "\n",
    "- **Prevents co-adaptation:** Forces network to learn redundant representations (can't rely on any single unit)\n",
    "- **Implicit ensemble:** Training different \"sub-networks\" at each iteration\n",
    "- **Encourages smaller weights:** Must spread information across many paths\n",
    "\n",
    "**Typical values:** $p = 0.5$ for hidden layers, $p = 0.2$ for input layer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc333c2",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "**Definition:**\n",
    "Use a model pre-trained on a large dataset and adapt it to a new, related task with limited data.\n",
    "\n",
    "**Key Idea:** Features learned on one task (e.g., ImageNet) are often useful for other tasks (e.g., medical imaging).\n",
    "\n",
    "**Two Main Approaches:**\n",
    "\n",
    "1. **Feature Extraction (Frozen Layers)**\n",
    "   - Keep pre-trained weights fixed\n",
    "   - Only train new final layers for your task\n",
    "   - Fast, works well when data is very limited\n",
    "\n",
    "2. **Fine-Tuning**\n",
    "   - Initialize with pre-trained weights\n",
    "   - Train entire network (or later layers) on new task\n",
    "   - Use small learning rate to avoid destroying pre-trained features\n",
    "\n",
    "**Typical Strategy:**\n",
    "\n",
    "```\n",
    "Pre-trained model → Remove final layer → Add new task-specific layer → Train\n",
    "```\n",
    "\n",
    "**Why it Works:**\n",
    "- Early layers learn general features (edges, textures)\n",
    "- Later layers learn task-specific features\n",
    "- Pre-training provides better initialization than random weights\n",
    "\n",
    "**Practical Tips:**\n",
    "- More data → fine-tune more layers\n",
    "- Less data → freeze more layers\n",
    "- Always use lower learning rate than training from scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a83b04",
   "metadata": {},
   "source": [
    "### Self-Supervised Learning\n",
    "\n",
    "**Definition:**\n",
    "Train a model on a pretext task using unlabeled data, then transfer learned representations to downstream tasks.\n",
    "\n",
    "**Key Idea:** Create \"free\" supervision from the data itself without human labels.\n",
    "\n",
    "\n",
    "**Common Pretext Tasks:**\n",
    "\n",
    "| Task | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Contrastive Learning** | Learn representations by pulling similar samples together and pushing dissimilar ones apart | SimCLR, MoCo |\n",
    "| **Masked Prediction** | Predict masked portions of input | BERT (text), MAE (images) |\n",
    "| **Rotation Prediction** | Predict rotation angle applied to image | Rotate image 0°, 90°, 180°, 270° |\n",
    "| **Jigsaw Puzzles** | Predict correct arrangement of shuffled patches | Rearrange 9 image patches |\n",
    "\n",
    "\n",
    "**Why it Works:**\n",
    "- Forces model to learn meaningful features from data structure\n",
    "- Leverages vast amounts of unlabeled data\n",
    "- Pre-trained representations transfer well to supervised tasks\n",
    "\n",
    "**Typical Workflow:**\n",
    "\n",
    "```\n",
    "Unlabeled data → Self-supervised pretraining → Fine-tune on labeled task\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces need for expensive labeled data\n",
    "- Often outperforms training from scratch\n",
    "- Scales well with data (unlike supervised learning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7019a4",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization (HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4ecdd",
   "metadata": {},
   "source": [
    "### What is Hyperparameter Optimization?\n",
    "\n",
    "**Hyperparameters** are configuration settings that control the learning process but are **not learned from data**. Unlike model parameters (weights and biases), which are optimized during training, hyperparameters must be set before training begins.\n",
    "\n",
    "**Goal of HPO:** Find the hyperparameter configuration that yields the best model performance on unseen data (validation set).\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Hyperparameters\n",
    "\n",
    "#### 1. Optimization Hyperparameters\n",
    "Control how the model learns from data:\n",
    "\n",
    "| Hyperparameter | Description | Typical Range |\n",
    "|----------------|-------------|---------------|\n",
    "| **Learning rate** $\\alpha$ | Step size for gradient updates | $[10^{-5}, 10^{-1}]$ |\n",
    "| **Batch size** | Number of samples per gradient update | $[16, 512]$ |\n",
    "| **Number of epochs** | Total passes through training data | $[10, 500]$ |\n",
    "| **Optimizer type** | SGD, Adam, RMSprop, etc. | Categorical |\n",
    "| **Momentum** $\\beta$ | (for SGD with momentum) | $[0.9, 0.99]$ |\n",
    "| **Weight decay** $\\lambda$ | L2 regularization strength | $[10^{-5}, 10^{-2}]$ |\n",
    "\n",
    "#### 2. Architecture Hyperparameters\n",
    "Define the model structure:\n",
    "\n",
    "| Hyperparameter | Description | Typical Range |\n",
    "|----------------|-------------|---------------|\n",
    "| **Number of layers** | Depth of network | $[2, 100+]$ |\n",
    "| **Hidden units per layer** | Width of network | $[32, 1024+]$ |\n",
    "| **Activation function** | ReLU, tanh, sigmoid, etc. | Categorical |\n",
    "| **Dropout rate** $p$ | Fraction of units to drop | $[0.0, 0.5]$ |\n",
    "\n",
    "#### 3. Regularization Hyperparameters\n",
    "Control overfitting:\n",
    "\n",
    "| Hyperparameter | Description | Typical Range |\n",
    "|----------------|-------------|---------------|\n",
    "| **L1 regularization** $\\lambda_1$ | Lasso penalty strength | $[10^{-6}, 10^{-2}]$ |\n",
    "| **L2 regularization** $\\lambda_2$ | Ridge penalty strength | $[10^{-5}, 10^{-2}]$ |\n",
    "| **Dropout rate** | Probability of dropping units | $[0.1, 0.5]$ |\n",
    "| **Early stopping patience** | Epochs to wait before stopping | $[5, 50]$ |\n",
    "\n",
    "#### 4. Data Preprocessing Hyperparameters\n",
    "\n",
    "| Hyperparameter | Description |\n",
    "|----------------|-------------|\n",
    "| **Data augmentation** | Rotation, flip, crop parameters |\n",
    "| **Normalization** | Mean/std for standardization |\n",
    "| **Train/val/test split** | Ratio for data splitting |\n",
    "\n",
    "---\n",
    "\n",
    "### How to Identify Your Hyperparameters\n",
    "\n",
    "**Step 1: List all configurable settings**\n",
    "- Review your model architecture, optimizer, loss function, and training loop\n",
    "- Any value you manually set (not learned) is a hyperparameter\n",
    "\n",
    "**Step 2: Categorize by impact**\n",
    "- **High impact:** Learning rate, batch size, architecture size\n",
    "- **Medium impact:** Regularization strength, optimizer choice\n",
    "- **Low impact:** Momentum, scheduler parameters\n",
    "\n",
    "**Step 3: Start with high-impact parameters**\n",
    "- Focus optimization efforts on the most influential hyperparameters\n",
    "- Fix low-impact parameters to reasonable defaults\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Search Methods\n",
    "\n",
    "| Method | Process | Pros | Cons | When to Use |\n",
    "|--------|---------|------|------|-------------|\n",
    "| **Manual Search** | Try different values based on intuition and experience | ✅ Can leverage domain expertise<br>✅ Good for understanding model behavior | ❌ Time-consuming<br>❌ Not reproducible<br>❌ Biased by human intuition | Initial exploration, small models |\n",
    "| **Grid Search** | Define discrete set of values for each hyperparameter and exhaustively evaluate all combinations<br><br>**Example:**<br>• Learning rates: [0.001, 0.01, 0.1]<br>• Batch sizes: [32, 64, 128]<br>• Hidden sizes: [64, 128, 256]<br>• Total: 3 × 3 × 3 = 27 experiments | ✅ Simple to implement<br>✅ Reproducible<br>✅ Guaranteed to find best configuration in search space | ❌ Exponential growth: $n_1 \\times n_2 \\times \\cdots \\times n_k$ combinations<br>❌ Wastes computation on unimportant dimensions<br>❌ Infeasible for $>3$ hyperparameters | Few hyperparameters ($\\leq 3$), small search spaces |\n",
    "| **Random Search** | Sample hyperparameter combinations randomly from defined distributions<br><br>**Key Insight (Bergstra & Bengio, 2012):**<br>Random search is more efficient than grid search because it explores more unique values along each dimension | ✅ More efficient than grid search<br>✅ Can search larger spaces with same budget<br>✅ Parallelizable | ❌ No guarantee of finding optimal configuration<br>❌ Doesn't learn from previous trials | $>3$ hyperparameters, limited computational budget |\n",
    "| **Log-Scale Sampling** | For hyperparameters spanning multiple orders of magnitude<br><br>**Wrong approach:**<br>`lr = uniform(0.00001, 0.1)`  *# Most samples near 0.1*<br><br>**Correct approach:**<br>`log_lr = uniform(-5, -1)`<br>`lr = 10^log_lr`  *# Evenly distributed across orders* | ✅ **Use for:**<br>• Learning rate<br>• Weight decay (regularization strength)<br>• Any parameter spanning multiple orders of magnitude | ❌ **Don't use for:**<br>• Number of layers (discrete, small range)<br>• Batch size (power of 2, small range)<br>• Dropout rate (bounded [0,1]) | Parameters with exponential scale (learning rate, regularization) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Practical HPO Strategy\n",
    "\n",
    "### Step 1: Coarse Search\n",
    "- Use **random search**\n",
    "- Wide ranges: learning rate $[10^{-5}, 10^{-1}]$, batch size $[16, 512]$\n",
    "- Budget: 20-50 trials with early stopping\n",
    "\n",
    "### Step 2: Refinement\n",
    "- Narrow ranges around best region found\n",
    "- Budget: 50-100 trials\n",
    "\n",
    "### Step 3: Validation\n",
    "- Train best 3-5 configurations with different seeds\n",
    "- Report mean ± std on validation set\n",
    "- Evaluate final model on **held-out test set only once**\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Always use a validation set**\n",
    "   - Never tune on test data (leads to overfitting)\n",
    "   - Use cross-validation if data is limited\n",
    "\n",
    "2. **Use log-scale for learning rate**\n",
    "   - Sample from $10^{\\text{uniform}(-5, -1)}$ not uniform(0.00001, 0.1)\n",
    "\n",
    "3. **Start simple**\n",
    "   - Begin with single-layer models, few trials\n",
    "   - Add complexity only when necessary\n",
    "\n",
    "4. **Monitor training dynamics**\n",
    "   - Plot learning curves for different configurations\n",
    "   - Watch for underfitting vs. overfitting\n",
    "\n",
    "5. **Budget computational resources**\n",
    "   - Set maximum trials and wall-clock time limits\n",
    "   - Use early stopping aggressively during search\n",
    "\n",
    "6. **Document everything**\n",
    "   - Track all hyperparameters, seeds, and results\n",
    "   - Use tools like MLflow, Weights & Biases, or TensorBoard\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "| Mistake | Consequence | Solution |\n",
    "|---------|-------------|----------|\n",
    "| **Testing on validation set** | Overfitting to validation data | Use separate test set, evaluate only once |\n",
    "| **Not using log-scale for LR** | Wasting trials on narrow range | Use exponential/log distributions |\n",
    "| **Too small validation set** | Noisy performance estimates | Use at least 10-20% of training data |\n",
    "| **Grid search for many parameters** | Exponential computational cost | Use random search |\n",
    "| **Single seed evaluation** | High variance in results | Average over 3-5 seeds |\n",
    "| **Ignoring training time** | Wasting resources on slow configs | Set time/epoch budgets |\n",
    "\n",
    "\n",
    "**Key Takeaway:** Start with random search using log-scale sampling for learning rate and regularization parameters. <Br> Use validation set for all tuning, and only evaluate on test set once with your final configuration.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
