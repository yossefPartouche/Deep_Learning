{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f0a150",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "\n",
    "The goal, is to assing an input data instance $x$ to one of $K > 2$ clsses so $y \\in \\{1, 2, \\dots, K\\}$.\n",
    "- Predicting which of $K=10$ digits $y$ is present in an image $x$, a handwritten number.\n",
    "- Predicting which of $K$ possible words $y$ follows an incomplete sentence $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ea997",
   "metadata": {},
   "source": [
    "## Multi-Class Perceptron\n",
    "\n",
    "In a similar fashion to our discussion in Binary Classifcation, we'll first discuss how the preceptron computes the loss over multiple classes.\n",
    "\n",
    "Just like the binary - Perceptron, given the models parameters $W$ we compute our $z$ values.<br>\n",
    "In this case we actually compute $K$ z-vlaues $\\{z_1, z_2, \\dots, z_K\\}$, this follows the exact method as we saw in the shallow network when we have multiple inpute and multiple outputs.<br>\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\cdots \\\\ z_K \\end{bmatrix} = \n",
    "\\begin{bmatrix}{\\beta_1} \\\\ {\\beta_2} \\\\ \\cdots \\\\ {\\beta_K} \\end{bmatrix} + \n",
    "\\begin{bmatrix} \n",
    "{w_{11}} & {w_{12}} & \\cdots & {w_{1D}} \\\\ \n",
    "{w_{21}} & {w_{22}} & \\cdots & {w_{2D}} \\\\ \n",
    "& \\cdots \\\\\n",
    "{w_{K1}} & {w_{K2}} & \\cdots & {w_{KD}}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_D\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{z} = \\vec{\\beta} +  W\\vec{x}\n",
    "$$ \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap4/MClassNet.png\" width=\"300\" />\n",
    "</div>\n",
    "\n",
    "### One vs. All\n",
    "\n",
    "We can view the Parameter $W$ as computing $D$ cases of binary classification where:\n",
    "- One is the probability of the class we're looking for \n",
    "- Zero are the rest of the classes or **NOT** our class\n",
    "\n",
    "So if we're provided with a training instance $(x, c)$ where $c$ is the correct class then our model aims to do as follows:\n",
    "\n",
    "$$\\boxed{\\begin{aligned} z_c(x) = w_c^Tx + \\beta_c \\gt 0 \\\\\n",
    "\n",
    "\n",
    " z_{j \\ne c}(x) = w_j^Tx + \\beta_j \\lt 0\\end{aligned}}$$\n",
    "\n",
    "For the $c^{th}$ class corresponding to the class of the input, we want the $c^{th}$ row paramteers to output a positive classification.\n",
    "\n",
    "For $ \\forall j^{th}$ classes that are not the $c^{th}$ class, we want the $j^{th}$ row parameters to output a negative classification.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap4/mLinAlgview.png\" width=\"300\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12a05a",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Geomterically this isn't unique for every classification. \n",
    "\n",
    "##### Problematic Example\n",
    "\n",
    "Looking at the above visual:\n",
    "\n",
    "- Look at the outward pizza slice formed from the intersection of the red classifer (line) and the blue classifer (line).\n",
    "- What if we had two points within that pizza slice area one maybe a bit closer to the blue classifier and one to the red classifer.\n",
    "- By the above construction we'll obtain two positive classification.\n",
    "- But we only wanted a single answer One vs All!\n",
    "\n",
    "### Potential Solution\n",
    "\n",
    "Instead of providing a discrete threshold, we can allow our threshold being relative or a range.<br>\n",
    "Indeed if an instance is closer to one of the classifiers then surely it should be classified according to this class. \n",
    "\n",
    "$$\n",
    "\\boxed{z_c(x) \\gt z_j(x), \\quad \\forall j \\ne c}\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../images/chap4/altSol1.png\" width=\"300\" style=\"display:inline-block; margin:5px;\" />\n",
    "<img src=\"../images/chap4/altSol2.png\" width=\"300\" style=\"display:inline-block; margin:5px;\" />\n",
    "<img src=\"../images/chap4/altSol3.png\" width=\"300\" style=\"display:inline-block; margin:5px;\" />\n",
    "<img src=\"../images/chap4/altSol4.png\" width=\"300\" style=\"display:inline-block; margin:5px;\" />\n",
    "</div>\n",
    "\n",
    "### Perceptron Loss\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "    L_p(W, x, c) = \\sum_{j \\ne c} \\text{max}\\left[0,\\Delta -\\left(z_c(x)-z_j(x)\\right)\\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "More advance techniques can be applied to this loss function such as:\n",
    "\n",
    "- Hinge Loss \n",
    "- Regulizers \n",
    "- SVMs\n",
    "\n",
    " \n",
    "**Gradient for a single instance $(x, c)$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_p(W, x, c)}{\\partial W_k} =\n",
    "\\begin{cases}\n",
    "    -\\sum_{j \\ne c} \\mathbb{1}\\left[\\Delta - (z_c(x) - z_j(x)) > 0\\right] x & \\text{if } k = c \\\\\n",
    "    \\mathbf{1} \\cdot \\left[\\Delta - (z_c(x) - z_k(x)) > 0\\right] x & \\text{if } k \\ne c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}[\\cdot]$ is the indicator function (1 if condition is true, 0 otherwise).\n",
    "\n",
    "---\n",
    "\n",
    "**For the whole dataset:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(W)}{\\partial W_k} = \\frac{1}{N} \\sum_{i=1}^N\n",
    "\\begin{cases}\n",
    "    -\\sum_{j \\ne c_i} \\mathbb{1}\\left[\\Delta - (z_{c_i}(x_i) - z_j(x_i)) > 0\\right] x_i & \\text{if } k = c_i \\\\\n",
    "    \\mathbf{1} \\cdot \\left[\\Delta - (z_{c_i}(x_i) - z_k(x_i)) > 0\\right] x_i & \\text{if } k \\ne c_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "Suppose we've trained our model our inference at this point would just extract the class based on the largest z score:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "    y = \\text{argmax}_i(z_i) \\quad y \\in \\{1,2, \\dots, K\\}\n",
    "}\n",
    "\n",
    "$$\n",
    "\n",
    "Once Again, when discussing gradient descent this example will come back in that context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758f933",
   "metadata": {},
   "source": [
    "## Multi-Class Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54247de",
   "metadata": {},
   "source": [
    "#### Reminder on the General Method\n",
    "\n",
    "$$\\boxed{\\begin{aligned}\n",
    "&1. \\text{ Given the output choose a suitable probability distribution } Pr(y | \\theta) \\text{ defined over the domain of predictions} \\\\\n",
    "\\\\\n",
    "&2. \\text{ Set the ML model } f[x, \\phi] \\text{ to predict all independant parameters } \\\\\n",
    "&\\quad \\text{(and compute the rest of the parameters based on what's learnt) so } \\theta = f[x, \\phi] \\text{ and } Pr(y | \\theta) = Pr(y | f[x, \\phi]) \\\\\n",
    "\\\\\n",
    "&3. \\text{ We train the model to find the network parameters } \\hat{\\phi} \\text{ that minimizes the negative log-likelihood} \\\\\n",
    "&\\quad \\text{over the training dataset } \\{x_i, y_i\\}_{i=1}^N \\\\\n",
    "\\\\\n",
    "&4. \\text{ When needed to perform the inference we'll apply the argmax of the distribution } Pr(y | f[x, \\hat{\\phi}])\n",
    "\\end{aligned}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fc3cd",
   "metadata": {},
   "source": [
    "1. Since we have $y \\in \\{1, 2, \\dots, K\\}$ we will choose a $categorical \\ distribution$ which is defined on this domain. <br> This has $K$ parameters $\\lambda_1 \\ \\lambda_2 \\dots \\lambda_K$ $$Pr(Y = k) = \\lambda_k$$\n",
    "2. We use the network $f[x, \\phi]$ with $K$ outputs (as shown above) which won't necessarily obey the probability range $[0,1]$ <br> This means that we need the output to be fed through an additional function that will meet this constraint: $$\\boxed{\\text{softmax}_k [z] = \\frac{e^{z_k}}{\\sum_{k'=1}^Ke^{z_k'}}}$$\n",
    "   The likelihood that input $x$ has label $y = k$ is therefore: $$\\boxed{Pr(Y = k | X) = \\text{softmax}_k\\big[f[x, \\phi]\\big]}$$\n",
    "3. We train the model find the network parameters $\\phi$ that minimizes the NLL over the training dataset: $$\\begin{align}L[\\phi] &= - \\sum_{i=1}^N \\log\\big[ \\text{softmax}_{y_i}\\left[f[x_i, \\phi]\\right]\\big] \\\\ &=- \\sum_{i=1}^N \\left( f_{y_i}[x_i, \\phi] - \\log \\left[ \\sum_{k'=1}^K e^{f_{k'}(x_i, \\phi)}\\right]\\right)\\end{align}$$<br> \n",
    "   Where $f_{y_i}[x_i, \\phi]$ is the $y_i^{th}$ output and $f_{k'}[x_i, \\phi]$ is the $k'^{th}$ output\n",
    "4. If we want to make an inference then we'll take the most probable catergory $\\hat{y} = \\text{argmax}_{k}\\left[Pr(y=k | f[x, \\hat{\\phi}])\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37074b",
   "metadata": {},
   "source": [
    "**Gradient for a single instance $(x, y)$:**\n",
    "\n",
    "Let $y$ be the true class label (as an integer), $K$ the number of classes, and $z_k = f_k(x, \\phi)$ the output for class $k$.\n",
    "\n",
    "For each class $k$:\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{softmax}}(\\phi, x, y)}{\\partial \\phi_k} =\n",
    "\\left( \\text{softmax}_k(z) - \\mathbb{1}[y = k] \\right) x\n",
    "$$\n",
    "\n",
    "where $\\text{softmax}_k(z) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}$ and $\\mathbb{1}[y = k]$ is 1 if $y = k$, 0 otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient for the whole dataset $\\{(x_i, y_i)\\}_{i=1}^N$:**\n",
    "\n",
    "For each class $k$:\n",
    "$$\n",
    "\\frac{\\partial L(\\phi)}{\\partial \\phi_k} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\text{softmax}_k(z^{(i)}) - \\mathbb{1}[y_i = k] \\right) x_i\n",
    "$$\n",
    "\n",
    "where $z^{(i)}$ is the vector of outputs for sample $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315a339",
   "metadata": {},
   "source": [
    "**Matrix form of the multi-class softmax gradient:**\n",
    "\n",
    "---\n",
    "\n",
    "**Single instance $(x, y)$:**\n",
    "\n",
    "Let $Z = f(x, \\phi)$ be the vector of logits for all classes,  \n",
    "$\\text{softmax}(Z)$ is the vector of predicted probabilities,  \n",
    "$Y$ is a one-hot vector for the true class $y$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{softmax}}(\\phi, x, y)}{\\partial W} = \\left( \\text{softmax}(Z) - Y \\right) x^T\n",
    "$$\n",
    "\n",
    "- $W$ is the weight matrix ($K \\times D$).\n",
    "- $\\text{softmax}(Z)$ and $Y$ are $K \\times 1$ column vectors.\n",
    "- $x^T$ is a $1 \\times D$ row vector.\n",
    "- The result is a $K \\times D$ matrix.\n",
    "\n",
    "---\n",
    "\n",
    "**Whole dataset $\\{(x_i, y_i)\\}_{i=1}^N$:**\n",
    "\n",
    "Let $X$ be the $N \\times D$ data matrix,  \n",
    "$Y$ be the $N \\times K$ one-hot label matrix,  \n",
    "$\\text{softmax}(Z)$ be the $N \\times K$ matrix of predicted probabilities.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\phi)}{\\partial W} = \\frac{1}{N} \\left[ \\left( \\text{softmax}(Z) - Y \\right)^T X \\right]\n",
    "$$\n",
    "\n",
    "- $\\text{softmax}(Z) - Y$ is $N \\times K$.\n",
    "- $X$ is $N \\times D$.\n",
    "- The result is $K \\times D$.\n",
    "\n",
    "\n",
    "**Division Rule for Gradients:**\n",
    "\n",
    "If $f(x)$ and $g(x)$ are differentiable functions,\n",
    "$$\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) = \\frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Derivative of the softmax with respect to the score:**\n",
    "\n",
    "Let $z$ be the vector of scores, and $\\text{softmax}_k(z) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}$.\n",
    "\n",
    "For $k, l \\in \\{1, \\dots, K\\}$:\n",
    "$$\n",
    "\\frac{\\partial\\, \\text{softmax}_k(z)}{\\partial z_l} =\n",
    "\\begin{cases}\n",
    "    \\text{softmax}_k(z) \\left[1 - \\text{softmax}_k(z)\\right] & \\text{if } k = l \\\\\n",
    "    -\\text{softmax}_k(z)\\, \\text{softmax}_l(z) & \\text{if } k \\ne l\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
