{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47cc00a9",
   "metadata": {},
   "source": [
    "## Multivariate Regression via Maximum Likelihood\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We have:\n",
    "- Input: $\\mathbf{x} \\in \\mathbb{R}^D$ (D-dimensional features)\n",
    "- Output: $\\mathbf{y} \\in \\mathbb{R}^K$ (K-dimensional target)\n",
    "- Training data: $\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N$\n",
    "- Model parameters: $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{D \\times K}$ (weight matrix)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Choose the Distribution\n",
    "\n",
    "For multivariate continuous outputs, we use the **Multivariate Normal Distribution**:\n",
    "\n",
    "$$\\boxed{\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})}$$\n",
    "\n",
    "The probability density function is:\n",
    "\n",
    "$$Pr(\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{K/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\mu} \\in \\mathbb{R}^K$ is the mean vector\n",
    "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ is the covariance matrix\n",
    "- $|\\boldsymbol{\\Sigma}|$ is the determinant of $\\boldsymbol{\\Sigma}$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Model Predicts Distribution Parameters\n",
    "\n",
    "Our model predicts the mean:\n",
    "\n",
    "$$\\boxed{\\boldsymbol{\\mu}_i = f[\\mathbf{x}_i, \\boldsymbol{\\Phi}] = \\boldsymbol{\\Phi}^T \\mathbf{x}_i}$$\n",
    "\n",
    "For simplicity, we assume **independent outputs with constant variance**:\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}_K$$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$Pr(\\mathbf{y}_i | \\mathbf{x}_i, \\boldsymbol{\\Phi}) = \\frac{1}{(2\\pi\\sigma^2)^{K/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|\\mathbf{y}_i - \\boldsymbol{\\Phi}^T\\mathbf{x}_i\\|^2\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Minimize Negative Log-Likelihood\n",
    "\n",
    "The likelihood over all training samples:\n",
    "\n",
    "$$L(\\boldsymbol{\\Phi}) = \\prod_{i=1}^N Pr(\\mathbf{y}_i | \\mathbf{x}_i, \\boldsymbol{\\Phi})$$\n",
    "\n",
    "Taking the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\log L(\\boldsymbol{\\Phi}) &= -\\sum_{i=1}^N \\log Pr(\\mathbf{y}_i | \\mathbf{x}_i, \\boldsymbol{\\Phi}) \\\\\n",
    "&= -\\sum_{i=1}^N \\left[-\\frac{K}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|\\mathbf{y}_i - \\boldsymbol{\\Phi}^T\\mathbf{x}_i\\|^2\\right] \\\\\n",
    "&= \\frac{NK}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^N \\|\\mathbf{y}_i - \\boldsymbol{\\Phi}^T\\mathbf{x}_i\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the first term is constant w.r.t. $\\boldsymbol{\\Phi}$, minimizing NLL is equivalent to:\n",
    "\n",
    "$$\\boxed{\\hat{\\boldsymbol{\\Phi}} = \\arg\\min_{\\boldsymbol{\\Phi}} \\sum_{i=1}^N \\|\\mathbf{y}_i - \\boldsymbol{\\Phi}^T\\mathbf{x}_i\\|^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Inference\n",
    "\n",
    "For a new input $\\mathbf{x}^*$, the predicted output is:\n",
    "\n",
    "$$\\boxed{\\hat{\\mathbf{y}} = \\arg\\max_{\\mathbf{y}} Pr(\\mathbf{y} | \\mathbf{x}^*, \\hat{\\boldsymbol{\\Phi}}) = \\hat{\\boldsymbol{\\Phi}}^T \\mathbf{x}^*}$$\n",
    "\n",
    "The maximum of a Gaussian is its mean.\n",
    "\n",
    "---\n",
    "\n",
    "### Matrix Form and Least Squares Solution\n",
    "\n",
    "Define the data matrices:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix} \\in \\mathbb{R}^{N \\times D}, \\quad \\mathbf{Y} = \\begin{bmatrix} \\mathbf{y}_1^T \\\\ \\mathbf{y}_2^T \\\\ \\vdots \\\\ \\mathbf{y}_N^T \\end{bmatrix} \\in \\mathbb{R}^{N \\times K}$$\n",
    "\n",
    "The loss can be written as:\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\Phi}) = \\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\Phi}\\|_F^2 = \\text{tr}\\left[(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\Phi})^T(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\Phi})\\right]$$\n",
    "\n",
    "Where $\\|\\cdot\\|_F$ is the Frobenius norm.\n",
    "\n",
    "**Taking the gradient and setting to zero:**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\Phi}} = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\Phi}) = \\mathbf{0}$$\n",
    "\n",
    "**Solving for $\\boldsymbol{\\Phi}$:**\n",
    "\n",
    "$$\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\Phi} = \\mathbf{X}^T\\mathbf{Y}$$\n",
    "\n",
    "$$\\boxed{\\hat{\\boldsymbol{\\Phi}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}}$$\n",
    "\n",
    "This is the **multivariate least squares solution**, which emerges naturally from maximum likelihood estimation with Gaussian assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Step | Univariate ($y \\in \\mathbb{R}$) | Multivariate ($\\mathbf{y} \\in \\mathbb{R}^K$) |\n",
    "|------|--------------------------------|---------------------------------------------|\n",
    "| **Distribution** | $y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ | $\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ |\n",
    "| **Model** | $\\mu = \\boldsymbol{\\phi}^T \\mathbf{x}$ | $\\boldsymbol{\\mu} = \\boldsymbol{\\Phi}^T \\mathbf{x}$ |\n",
    "| **NLL** | $\\sum_{i=1}^N (y_i - \\boldsymbol{\\phi}^T\\mathbf{x}_i)^2$ | $\\sum_{i=1}^N \\|\\mathbf{y}_i - \\boldsymbol{\\Phi}^T\\mathbf{x}_i\\|^2$ |\n",
    "| **Solution** | $\\hat{\\boldsymbol{\\phi}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$ | $\\hat{\\boldsymbol{\\Phi}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$ |\n",
    "| **Inference** | $\\hat{y} = \\hat{\\boldsymbol{\\phi}}^T \\mathbf{x}^*$ | $\\hat{\\mathbf{y}} = \\hat{\\boldsymbol{\\Phi}}^T \\mathbf{x}^*$ |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
