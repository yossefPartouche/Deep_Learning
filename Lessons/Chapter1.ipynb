{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8ff703",
   "metadata": {},
   "source": [
    "# Machine Learning Recap\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this chapter we go over two main problems that are solved in Machine Learning: \n",
    "1. Regression Problem\n",
    "2. Classification Problem\n",
    "\n",
    "These two methods are used and discussed throughout the rest of the chapters\n",
    "\n",
    "Before getting started below describes general overview of the machine Learning Taxonomy and how we describe different types of problems and different methods of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c6659",
   "metadata": {},
   "source": [
    "# Machine Learning Taxonomy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da234b8c",
   "metadata": {},
   "source": [
    "In Machine learning have two main types of taxonomy:\n",
    "\n",
    "### Taxonomy by Supervision (The \"How\")\n",
    "\n",
    "- ### Supervised Learning\n",
    "  - We're provided with labeled data to train on.\n",
    "  - We wish to learn to predict from the corresponding set of labels on new samples.\n",
    "  - We usually do this using Regression or Classification based on the label type.\n",
    "- ### Unsupervised Learning \n",
    "  - The data we're provided contains no labels.\n",
    "  - We aim to discover structural/spacial relations based on the data.\n",
    "  - Common tasks: Clustering (K-means), Dimensionality Reduction, Association\n",
    "\n",
    "- ### Semi-Supervised Learning\n",
    "  - Some of the data we're provided is labeled or most of the data isn't labeled.\n",
    "  - The general task is for the model to be able to label to full data-set otherwise known as self-training\n",
    "  - We apply similar or hybrid solutions to general approach described above.\n",
    "  \n",
    "- ### Self-Supervised Learning\n",
    "  - The task is designed so that the data is its own label\n",
    "  - There's no need for human annotation.\n",
    "  - We aim to teach a model to extract the useful features of the data provided.\n",
    "  - Only later on in the modelling would we use this pre-trained model for specific tasks\n",
    "\n",
    "### Taxonomy by Model Objective (The \"What\")\n",
    "\n",
    "Regardless of the supervision above, the model will usually fall into one of the following buckets:\n",
    "\n",
    "- ### Discriminative Models\n",
    "  - Focus on mapping inputs to outputs directly\n",
    "  - Example: Provide an image with a dog and the model classifies the image as a dog from a cat/dog label set\n",
    "- ### Generative Models\n",
    "  - Focus on understanding how the data was \"made\"\n",
    "  - Example: Provide an \"Empty Matrix\" and then model outputs an image of a dog <br> from a pool of images of different dogs and cats it can create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120953f1",
   "metadata": {},
   "source": [
    "# Supervised Learning \n",
    "\n",
    "We now dive into the main concepts that are applied in supervised learning since these are the foundamental blocks that are used repetatively throughout Deep Learning (no matter the complexity of the model).\n",
    "\n",
    "Supervised learning can summarised as a model the produces a mathematical function, such that given an input the function produces an output.\n",
    "<br>The output is referred as the $\\color{lightblue}inference\\color{silver}^1$.\n",
    "\n",
    "This function contains $\\color{lightblue}parameter\\color{silver}^2$, which affect the output from a given input.<br> As such the model equation (this function) describes a family of possible relationships between inputs and outputs, where the parameters specifiy a particular relationship between input instance and its output.\n",
    "\n",
    "$\\color{lightblue}training\\color{silver}^3$ a model essentially means trying to find the parameters that describe the true relationship between the inputs and outputs. \n",
    "\n",
    "We train the model by following a procedure of trial and error over the training set where for each trial we measure the error and then correct the parameters in the direction that will reduce the error in the next run of trials and errors (this is our hope/hypothesis).\n",
    "\n",
    "After training a model, we asses its performance; we run the model on a seperate test dataset to see how well it performs or how well it $\\color{lightblue}generalises$\n",
    "\n",
    "If the results are good enough then the model is ready for deployment (brought to the outside world).\n",
    "\n",
    "#### Formalization\n",
    "\n",
    "$\\text{Let } \\vec x \\in \\R^n \\text{ be our input and } \\vec y \\in \\R^m  \\text{ be the output}$\n",
    "\n",
    "$\\text{To make a prediction we need a model } f[•] \\text{ which takes x and returns y}$\n",
    "\n",
    "$$ f[x]= y\\color{silver}^1 $$\n",
    "\n",
    "$\\text{Since we need parameters to describe the relation then } f \\text{ needs to accept these parameters } \\phi\\color{silver}^2 \\text{ therefore:}$ \n",
    "\n",
    "$$ f[x, \\phi] = y$$\n",
    "\n",
    "$\\text{To train the model we quantify the degree of mismatch between the inference and the true values:}$\n",
    "\n",
    "$$ \\hat\\phi\\color{silver}^3 = \\argmin_{\\phi}\\left[L[\\phi]\\right] = \\argmin_{\\phi}\\left[L[\\{x_i, y_i\\}, \\phi]\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf9dee",
   "metadata": {},
   "source": [
    "## 1D Linear Regression Model Example\n",
    "\n",
    "This is a problem where we aim to find a linear function that best fits the points on the graph \n",
    "\n",
    "Assume our input and output are scalar values\n",
    "\n",
    "$\\text{model: }$  $$y = f[x, \\phi] = \\phi_0 + \\phi_1x$$\n",
    "\n",
    "$\\text{Parameters: }$  $$\\phi = [\\phi_0, \\phi_2]$$\n",
    "\n",
    "$\\text{Loss Function: }$ $$L[\\phi] = \\sum_{i=1}^N(f[x_i, \\phi] - y_i)^2 = \\sum_{i=1}^N(\\phi_0 + \\phi_1x - y_i)^2$$\n",
    "\n",
    "\n",
    "$\\text{Our Goal: }$ $$\\hat\\phi = \\argmin_{\\phi} \\left[\\sum_{i=1}^N(\\phi_0+\\phi_1x - y_i)^2\\right]$$\n",
    "\n",
    "The following link provides a visualization of the above concepts:\n",
    "1. 1D linear model\n",
    "2. Least Square loss Function\n",
    "3. Loss function space with respect to parameters\n",
    "\n",
    "https://udlbook.github.io/udlfigures/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924886a",
   "metadata": {},
   "source": [
    "## Extending Linear Regression to higher Dimension\n",
    "\n",
    "Suppose we have d features $x = (x_1, x_2, ..., x_d)$\n",
    "\n",
    "The general structure remains the same, from a mathematical and implementation stand point we now represent this using vectors and vector operation.\n",
    "\n",
    "$\\text{model: }$ $$y = f[x, \\phi] = \\phi_0 + \\sum_{i=1}^dx_i\\phi_i =  \\phi^Tx$$\n",
    "\n",
    "$\\text{Parameters: }$ $$\\phi = [\\phi_0, \\, \\phi_1, \\dots, \\phi_d]$$\n",
    "\n",
    "If this applies to a single instance then we can apply to a subset of the training data and this can be represented by:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "— & x^1 & — \\\\\n",
    "— & x^2 & — \\\\\n",
    "— & x^3 & — \\\\\n",
    " & \\vdots & \\\\\n",
    "— & x^N & — \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: It's easy to get confused since there's a lot of notation:\n",
    "- superscript = feature \n",
    "- subscript = instance\n",
    "  \n",
    "Where each $x_{i} \\in \\R^d$ that is a row vector containing d-values representing the d-features the inference would be: \n",
    "\n",
    "$$ \n",
    "f[X, \\phi]  = X\\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_d \\end{bmatrix} + \\phi_0\n",
    "\\begin{bmatrix} \n",
    "1 \\\\ 1 \\\\ \\vdots \\\\ 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We also present what's happening to our dimensions\n",
    "$$\n",
    "\n",
    "(N \\times 1) = (N \\times d) (d \\times 1) + (N \\times 1)\n",
    "$$\n",
    "\n",
    "$\\text{Loss Function: }$ $$L[\\phi] = \\frac{1}{2N}\\sum_{i=1}^N(f[X, \\phi] - y)^2  = \\frac{1}{2N} ||f[X, \\phi] - y||^2$$\n",
    "\n",
    "To Clarify: \n",
    "- We're summing of over the number of **instances** N\n",
    "- The division by 2 is a convinience which we'll see later (this doesn't affect the training process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8afa1d",
   "metadata": {},
   "source": [
    "## Analytically Solving Linear Regression\n",
    "We mentioned earlier that: <br>\"training the model is trying to find the parameters that describe the true relationship between the inputs and outputs.\"<br>\n",
    "In theory we're done training when our inference matches the true results: $$(x^i)\\phi - y^i = 0$$ <br>\n",
    "Ideally we want the Loss function to be equal to 0, this in essence would represent the parameters are best aligned to the data.<br>\n",
    "\n",
    "Which means we're solving for $\\phi$. We do this by solving the partial derivative. \n",
    "\n",
    "This is because being able to understand how the change in the parameters affect the loss is essentially showing us how to tune our parameters for better results.\n",
    "\n",
    "This also means that any **change** in the parameters can lead to a non-zero Loss values therefore:\n",
    "\n",
    "$$\\frac{\\partial L }{\\partial \\phi} = 0 = \\lim_{h \\to 0} \\frac{(\\phi + h) - L(\\phi)}{h}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial L }{\\partial \\phi_0} = 0 = \\lim_{h \\to 0} \\frac{(\\phi_0 + h) - L(\\phi_0)}{h} $$\n",
    "\n",
    "Can be interpreted as:\n",
    "\n",
    "\"How does the Loss **changes/is affected** if we change the parameters\"\n",
    "\n",
    "Or Formally: \n",
    "\n",
    "\"The partial derivative with respect to the parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9cc94",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\phi} &= \\frac{\\partial }{\\partial \\phi} \\frac{\\|X\\phi - y\\|^2}{2N} \\\\\n",
    "&= \\frac{1}{2N} \\frac{\\partial}{\\partial \\phi} \\|X\\phi - y \\|^2 \\\\\n",
    "&= \\frac{2}{2N} X^T (X\\phi - y) \\\\\n",
    "&= \\frac{1}{N} X^T (X\\phi - y) \\\\\n",
    "&= 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "We now solve for for the parameters $\\phi$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^T(X \\phi - y) &= 0 \\\\\n",
    "X^T X \\phi - X^T y &= 0 \\\\\n",
    "X^T X \\phi &= X^T y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We now have two cases X is invertible or isn't\n",
    "\n",
    "If there exists an inverse for the matrix $(X^T X)$, denoted as $(X^T X)^{-1}$ (which can be verified via a non-zero determinant), we can solve for $\\phi$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(X^T X)^{-1} X^T X \\phi &= (X^T X)^{-1} X^T y \\\\\n",
    "I \\phi &= (X^T X)^{-1} X^T y \\\\\n",
    "\\phi &= (X^T X)^{-1} X^T y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "If the determinant of $(X^T X)$ is zero, the inverse does not exist. This typically occurs in two scenarios:\n",
    "1. **Redundant Features**: Two or more features are perfectly correlated (Linearly Dependent).\n",
    "2. **Data Sparsity**: There are more features than training samples ($d > N$).\n",
    "\n",
    "In these cases, we use the **Moore-Penrose Pseudoinverse**, denoted as $(X^T X)^{+}$, which provides the best \"least-squares\" solution even when the matrix is singular:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi &= (X^T X)^{+} X^T y \\\\\n",
    "     &= X^{+} y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Alternatively, we apply **Regularization** (like Ridge Regression) to force the matrix to be invertible by adding a small value $\\lambda$ to the diagonal:\n",
    "\n",
    "$$\n",
    "\\phi = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaa5b9",
   "metadata": {},
   "source": [
    "### Analytical Solution fails in Deep Learning\n",
    "\n",
    "While $\\phi = (X^T X)^{-1} X^T y$ is elegant, it is impractical for Deep Learning for three reasons:\n",
    "\n",
    "1. **Scalability**: Inverting a matrix is $O(d^3)$. With $d > 10^6$, this is computationally prohibitive.\n",
    "2. **Memory**: Storing the $(d \\times d)$ matrix $(X^T X)$ requires $O(d^2)$ space, which exceeds modern GPU memory.\n",
    "3. **Non-Convexity**: Deep Neural Networks use non-linear activations, making the loss surface non-convex. There is no closed-form algebraic solution for:\n",
    "   $$ \\nabla_\\phi L(\\phi) = 0 $$\n",
    "\n",
    "Therefore, we use a general method known as **Gradient Descent**, this is an iterative method for find the optimal paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3953e",
   "metadata": {},
   "source": [
    "### Summary: The Transition to Iterative Methods\n",
    "\n",
    "While the analytical solution is mathematically beautiful, it acts as a bottleneck for high-dimensional data. In modern Deep Learning, where we deal with millions of parameters and non-linear transformations, we shift from **solving** for the minimum to **searching** for it.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ Questions Remaining\n",
    "\n",
    "> **1. Non-Linearity**\n",
    "> The current model assumes $y$ is a linear combination of $X$. What if the underlying relationship is curved or complex? \n",
    "> * *Sneak peek: We will introduce **Activation Functions** and **Hidden Layers**.*\n",
    "\n",
    "> **2. The Iterative Process**\n",
    "> We've established that we cannot invert the matrix. How do we actually take those \"small steps\" toward the minimum without knowing the global solution?\n",
    "> * *Sneak peek: We will dive into **Gradient Descent** and **Backpropagation**.*\n",
    "\n",
    "> **3. Loss Function Justification**\n",
    "> We used the Squared Error $\\|X\\phi - y\\|^2$. Why this specific formula? Does a \"good\" loss function have mathematical properties that make optimization easier?\n",
    "> * *Sneak peek: We will explore **Maximum Likelihood Estimation (MLE)** and **Information Theory**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4634f",
   "metadata": {},
   "source": [
    "# 1D Classification\n",
    "\n",
    "This is a problem where we're provided with a set of instance with various number of features, which have classes. \n",
    "$$ \\left\\{ (x^{(i)}, c^{(i)}) \\quad |\\quad c^{(i)} \\in \\{1, \\dots ,k\\} \\quad \\forall  i \\ \\in \\{1, \\dots, N\\} \\right\\}$$\n",
    "\n",
    "A typical example would be to be able to classify a dog from a set of images of dogs and cats. <br>\n",
    "Our input data is pixels and our output is a classfier. <br>\n",
    "### Objectives\n",
    "\n",
    "1. We'll first cover the standard vanilla version of this problem known as binary-classification this is where $k=2$\n",
    "2. We'll go over a sequence of improving loss functions and explain about their properties\n",
    "3. We'll then deal with $k > 2$ also known as Multiclass Classification and learn the typical loss functions for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3708339",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "**Data**: Our training set $$\\{(x^{(i)}, c^{(i)})\\}_{i=1}^N \\quad | \\quad c^{(i)} \\in \\{-1, +1\\}$$\n",
    "\n",
    "**Model**: Is a Linear function of $x$ which has a threshold\n",
    "\n",
    "$$z = f[x, \\phi] = \\phi_0 + \\sum_{i=1}^dx_i\\phi_i =  \\phi^Tx$$\n",
    "\n",
    "$$y = g(z) = \n",
    "\\begin{cases}\n",
    "+1, &, z \\ge 0 \\\\\n",
    "-1, &, x \\lt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
