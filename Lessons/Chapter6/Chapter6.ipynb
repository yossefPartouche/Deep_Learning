{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2712ca7",
   "metadata": {},
   "source": [
    "# Gradients and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d9dd9",
   "metadata": {},
   "source": [
    "In the previous chapter we discussed an iterative algorithm to find the parameters that lead to mimization of the loss function.<br>\n",
    "The basic idea, is to initialise the parameters randomly and then by a series of small updates we decrease the average loss. <br>\n",
    "The key idea behind these changes are based on computing the gradients of the loss with respect to the parameters at the current position.<br>\n",
    "\n",
    "In this chapter we'll:\n",
    "\n",
    "1. Discuss how to efficiently calculate gradients efficiently\n",
    "2. Initialization of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0aef6",
   "metadata": {},
   "source": [
    "## Problem definitions\n",
    "\n",
    "Consider a network $f[x, \\phi]$ with multivariate input x, parameters $\\phi$, and three hidden layers $h_1, h_2, h_3$\n",
    "\n",
    "$$\\begin{align}\n",
    "h_1 &= a[\\beta_0 + \\Omega_0x] \\\\\n",
    "h_2 &= a[\\beta_1 + \\Omega_1h_1] \\\\\n",
    "h_3 &= a[\\beta_2 + \\Omega_1h_2] \\\\\n",
    "f[x, \\phi] &= \\beta_3 + \\Omega_3h_3\n",
    "\\end{align}$$\n",
    "\n",
    "Based on the SGD algorithm we (generally) apply the following update rule: \n",
    "\n",
    "$$\\phi_{t+1} = \\phi_{t} -\\alpha \\sum_{i \\in B_t}\\frac{\\partial l_i[\\phi_t]}{\\partial \\phi}$$\n",
    "\n",
    "$\\alpha$ is the learning rate <br>\n",
    "$B_t$ the batch indicies at iteration $t$ <br>\n",
    "\n",
    "To compute this update we have two main parameters to derive: \n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\beta_k} \\qquad \\text{and} \\qquad \\frac{\\partial l_i}{\\partial \\Omega_k} \\quad \\forall k \\in \\{0, 1, \\dots, K\\}$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
