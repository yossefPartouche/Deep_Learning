{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2712ca7",
   "metadata": {},
   "source": [
    "# Gradients and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d9dd9",
   "metadata": {},
   "source": [
    "In the previous chapter we discussed an iterative algorithm to find the parameters that lead to mimization of the loss function.<br>\n",
    "The basic idea, is to initialise the parameters randomly and then by a series of small updates we decrease the average loss. <br>\n",
    "The key idea behind these changes are based on computing the gradients of the loss with respect to the parameters at the current position.<br>\n",
    "\n",
    "In this chapter we'll:\n",
    "\n",
    "1. Discuss how to efficiently calculate gradients efficiently\n",
    "2. Initialization of the parameters\n",
    "\n",
    "NOTE: It may help to first view the `TensorDerivatives` notebook before reading this as it'll review certain derivatives of various dimensions making certain concepts easier to grasp in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0aef6",
   "metadata": {},
   "source": [
    "## Problem definitions\n",
    "\n",
    "Consider a network $f[x, \\phi]$ with multivariate input x, parameters $\\phi$, and three hidden layers $h_1, h_2, h_3$\n",
    "\n",
    "$$\\begin{align}\n",
    "h_1 &= a[\\beta_0 + \\Omega_0x] \\\\\n",
    "h_2 &= a[\\beta_1 + \\Omega_1h_1] \\\\\n",
    "h_3 &= a[\\beta_2 + \\Omega_1h_2] \\\\\n",
    "f[x, \\phi] &= \\beta_3 + \\Omega_3h_3\n",
    "\\end{align}$$\n",
    "\n",
    "Based on the SGD algorithm we (generally) apply the following update rule: \n",
    "\n",
    "$$\\phi_{t+1} = \\phi_{t} -\\alpha \\sum_{i \\in B_t}\\frac{\\partial l_i[\\phi_t]}{\\partial \\phi}$$\n",
    "\n",
    "$\\alpha$ is the learning rate <br>\n",
    "$B_t$ the batch indicies at iteration $t$ <br>\n",
    "\n",
    "To compute this update we have two main parameters to derive: \n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\beta_k} \\qquad \\text{and} \\qquad \\frac{\\partial l_i}{\\partial \\Omega_k} \\quad \\forall k \\in \\{0, 1, \\dots, K\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57974b48",
   "metadata": {},
   "source": [
    "## Computing Derivatives\n",
    "\n",
    "The derivative of the loss informs us how the loss changes when we make small changes to the parameters. \n",
    "\n",
    "The $\\textcolor{lightblue}{backpropagation \\ algorithm}$ computes these derivatives for us.\n",
    "\n",
    "**Observation 1**\n",
    "\n",
    "Each weight matrix $\\Omega_i$ is multiplied the activation at a hidden unit and adds the resuls to another hidden unit in the next layer, continuing this process until our output layer. <br>\n",
    "By the definition of the activation function, this means that certain parameters will be amplified or attenuated by the activation for each hidden layer.<br> The process of running the network for each data instance and thus storing the effects of the cumulative activations of the hidden unit is known as $\\textcolor{lightblue}{forward \\ pass}$.\n",
    "\n",
    "**Observation 2**\n",
    "\n",
    "The weights and biases are constantly being affected as they pass through the deep and interconnect network. At the end, the network produces an output in which we measure it's Loss.<br> We wish to understand how changing the parameters modifies the loss, which in turn means going back through each step of the layer that may have caused changed to the parameters.<br> This is known as $\\textcolor{lightblue}{backward \\ pass}$. \n",
    "\n",
    "Once we calculated the respective derivative we update the parameters and start again the $forward-pass$ and $backward-pass$.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img  src=\"../images/chap6/forwardpass.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63e906",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "### Example\n",
    "</div>\n",
    "\n",
    "$\\text{Let our model }$ $$f[x, \\phi] = \\beta_3 + \\omega_3 \\cdot \\cos[\\beta_2 + \\omega_2 \\cdot \\exp(\\beta_1 + \\omega_1 \\cdot \\sin[\\beta_0 + \\omega_0 \\cdot x])]$$\n",
    "\n",
    "$\\text{Our parameters }$ $$\\phi = \\{\\beta_0, \\omega_0, \\beta_1, \\omega_1, \\beta_2, \\omega_2, \\beta_3, \\omega_3\\}$$\n",
    "\n",
    "$\\text{Our Loss function }$ $$L[ \\phi ] = \\sum_i l_i \\quad | \\quad l_i = (f[x_i, \\phi] - y_i)^2$$\n",
    "\n",
    "Based on the above we wish to determine how the loss is afffected by a change in **each** of the parameters at each layer of the network: \n",
    "\n",
    "$$ \\frac{\\partial l_i}{\\partial \\beta_0}, \\ \\frac{\\partial l_i}{\\partial \\omega_0}, \\ \\frac{\\partial l_i}{\\partial \\beta_1}, \\ \\frac{\\partial l_i}{\\partial \\omega_1}, \\ \\frac{\\partial l_i}{\\partial \\beta_2}, \\frac{\\partial l_i}{\\partial \\omega_2}, \\frac{\\partial l_i}{\\partial \\beta_3}, \\frac{\\partial l_i}{\\partial \\omega_3}$$\n",
    "\n",
    "### Naïve Approach\n",
    "\n",
    "Compute directly using the chain rule...\n",
    "\n",
    "\n",
    "<div style=\"background-color:#fff3cd; color:#000; border-left:5px solid #ffc107; padding:10px; margin:10px 0;\">\n",
    "\n",
    "**⚠️ Problems with the Naïve Approach:**\n",
    "\n",
    "- Requires computing the full chain rule for each parameter separately  \n",
    "- Many computations are repeated (e.g., $(f[x_i, \\phi] - y_i)$ appears in every derivative)  \n",
    "- Computationally expensive for deep networks  \n",
    "- Intermediate terms like $\\sin[h_4]$, $\\exp(h_2)$, $\\cos[h_0]$ are recalculated multiple times  \n",
    "\n",
    "</div>\n",
    "\n",
    "**This motivates the need for the backpropagation algorithm**, which efficiently computes all derivatives by:\n",
    "1. Computing intermediate values once during the forward pass\n",
    "2. Reusing these values during a single backward pass\n",
    "3. Storing gradients at each layer and propagating them backwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f76e6b",
   "metadata": {},
   "source": [
    "### Backpropagation Approach\n",
    "\n",
    "Let's divide our above function into linear components denoted by $f_k$ and non-linear components denoted by $h_k$: \n",
    "\n",
    "$$\\begin{align}\n",
    "f_0 &= \\beta_0 + \\omega_0 \\cdot x_i \\\\\n",
    "h_1 &= \\sin(f_0) \\\\ \n",
    "f_1 &= \\beta_1 + \\omega_1 \\cdot h_1 \\\\\n",
    "h_2 &= \\exp[f_1] \\\\\n",
    "f_2 &= \\beta_2 + \\omega_2 \\cdot h_2 \\\\\n",
    "h_3 &= \\cos(f_2) \\\\\n",
    "f_3 &= \\beta_3 + \\omega_3 \\cdot h_3 \\\\\n",
    "l_i &= (f_3 - y_i)^2\n",
    "\\end{align}$$\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img  src=\"../images/chap6/fwdpass.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "\n",
    "</div>\n",
    "\n",
    "- We aim to minimize the number of computation. \n",
    "- Reuse Derivatives where possible \n",
    "- Compute isolated derivative Calculations.\n",
    "- Obtain the same result as the direct long chain approach. \n",
    "\n",
    "#### The Chain Rule \n",
    "\n",
    "$\\text{Suppose y is a function of u, and } u \\text{ is a function of } x: y = f(y), \\quad u = g(x) \\\\ \\text{Then the derivative of y with respect to x is: }$ \n",
    "\n",
    "$$\\boxed{\\frac{\\partial y}{\\partial x} = \\frac{\\partial f}{\\partial u}\\cdot\\frac{\\partial g}{\\partial x}}$$\n",
    "\n",
    "\n",
    "$\\text{For a function composed of many nested functions (such as a deep NN) the chain rule generalizes to:}$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\phi} = \\frac{\\partial l}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial z_{n-1}} \\cdot \\frac{\\partial z_{n-1}}{\\partial z_{n-2}} \\cdots \\frac{\\partial{z_1}}{\\partial \\phi}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e22c1",
   "metadata": {},
   "source": [
    "This means that we can sequentially apply the computation as we move backward along the graph.\n",
    "\n",
    "We'll work backwards from the loss, computing and reusing intermediate derivatives.\n",
    "\n",
    "**Recall our decomposition:**\n",
    "$$\\begin{align}\n",
    "f_0 &= \\beta_0 + \\omega_0 \\cdot x_i \\\\\n",
    "h_1 &= \\sin(f_0) \\\\ \n",
    "f_1 &= \\beta_1 + \\omega_1 \\cdot h_1 \\\\\n",
    "h_2 &= \\exp(f_1) \\\\\n",
    "f_2 &= \\beta_2 + \\omega_2 \\cdot h_2 \\\\\n",
    "h_3 &= \\cos(f_2) \\\\\n",
    "f_3 &= \\beta_3 + \\omega_3 \\cdot h_3 \\\\\n",
    "l_i &= (f_3 - y_i)^2\n",
    "\\end{align}$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Step 0: Compute the derivative of the loss\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial f_3} = 2(f_3 - y_i)$$\n",
    "\n",
    "**Store this value!** We'll reuse it for all subsequent calculations.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Layer 3: Computing $\\frac{\\partial l_i}{\\partial \\beta_3}$ and $\\frac{\\partial l_i}{\\partial \\omega_3}$\n",
    "\n",
    "**For $\\beta_3$:**\n",
    "\n",
    "Since $f_3 = \\beta_3 + \\omega_3 \\cdot h_3$, we have:\n",
    "$$\\frac{\\partial f_3}{\\partial \\beta_3} = 1$$\n",
    "\n",
    "By the chain rule:\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\beta_3} = \\frac{\\partial l_i}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial \\beta_3} = 2(f_3 - y_i) \\cdot 1 = 2(f_3 - y_i)}$$\n",
    "\n",
    "**For $\\omega_3$:**\n",
    "\n",
    "$$\\frac{\\partial f_3}{\\partial \\omega_3} = h_3$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\omega_3} = \\frac{\\partial l_i}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial \\omega_3} = 2(f_3 - y_i) \\cdot h_3}$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Moving to Layer 2: Compute $\\frac{\\partial l_i}{\\partial h_3}$\n",
    "\n",
    "We need this for the next layer:\n",
    "$$\\frac{\\partial l_i}{\\partial h_3} = \\frac{\\partial l_i}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial h_3} = 2(f_3 - y_i) \\cdot \\omega_3$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial f_2}$\n",
    "\n",
    "Since $h_3 = \\cos(f_2)$:\n",
    "$$\\frac{\\partial h_3}{\\partial f_2} = -\\sin(f_2)$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial f_2} = \\frac{\\partial l_i}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial f_2} = 2(f_3 - y_i) \\cdot \\omega_3 \\cdot (-\\sin(f_2))$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**For $\\beta_2$:**\n",
    "\n",
    "Since $f_2 = \\beta_2 + \\omega_2 \\cdot h_2$:\n",
    "$$\\frac{\\partial f_2}{\\partial \\beta_2} = 1$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\beta_2} = \\frac{\\partial l_i}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial \\beta_2} = -2\\omega_3(f_3 - y_i) \\sin(f_2)}$$\n",
    "\n",
    "**For $\\omega_2$:**\n",
    "\n",
    "$$\\frac{\\partial f_2}{\\partial \\omega_2} = h_2$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\omega_2} = \\frac{\\partial l_i}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial \\omega_2} = -2\\omega_3(f_3 - y_i) h_2 \\sin(f_2)}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Moving to Layer 1: Compute $\\frac{\\partial l_i}{\\partial h_2}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial h_2} = \\frac{\\partial l_i}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial h_2} = -2\\omega_3(f_3 - y_i) \\sin(f_2) \\cdot \\omega_2$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial f_1}$\n",
    "\n",
    "Since $h_2 = \\exp(f_1)$:\n",
    "$$\\frac{\\partial h_2}{\\partial f_1} = \\exp(f_1) = h_2$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial f_1} = \\frac{\\partial l_i}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial f_1} = -2\\omega_3\\omega_2(f_3 - y_i) h_2 \\sin(f_2)$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 1: Computing $\\frac{\\partial l_i}{\\partial \\beta_1}$ and $\\frac{\\partial l_i}{\\partial \\omega_1}$\n",
    "\n",
    "**For $\\beta_1$:**\n",
    "\n",
    "Since $f_1 = \\beta_1 + \\omega_1 \\cdot h_1$:\n",
    "$$\\frac{\\partial f_1}{\\partial \\beta_1} = 1$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\beta_1} = \\frac{\\partial l_i}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial \\beta_1} = -2\\omega_3\\omega_2(f_3 - y_i) h_2 \\sin(f_2)}$$\n",
    "\n",
    "**For $\\omega_1$:**\n",
    "\n",
    "$$\\frac{\\partial f_1}{\\partial \\omega_1} = h_1$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\omega_1} = \\frac{\\partial l_i}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial \\omega_1} = -2\\omega_3\\omega_2(f_3 - y_i) h_1 h_2 \\sin(f_2)}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Moving to Layer 0: Compute $\\frac{\\partial l_i}{\\partial h_1}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial h_1} = \\frac{\\partial l_i}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial h_1} = -2\\omega_3\\omega_2\\omega_1(f_3 - y_i) h_2 \\sin(f_2)$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial f_0}$\n",
    "\n",
    "Since $h_1 = \\sin(f_0)$:\n",
    "$$\\frac{\\partial h_1}{\\partial f_0} = \\cos(f_0)$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial f_0} = \\frac{\\partial l_i}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial f_0} = -2\\omega_3\\omega_2\\omega_1(f_3 - y_i) h_2 \\sin(f_2) \\cos(f_0)$$\n",
    "\n",
    "**Store this value!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 0: Computing $\\frac{\\partial l_i}{\\partial \\beta_0}$ and $\\frac{\\partial l_i}{\\partial \\omega_0}$\n",
    "\n",
    "**For $\\beta_0$:**\n",
    "\n",
    "Since $f_0 = \\beta_0 + \\omega_0 \\cdot x_i$:\n",
    "$$\\frac{\\partial f_0}{\\partial \\beta_0} = 1$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\beta_0} = \\frac{\\partial l_i}{\\partial f_0} \\cdot \\frac{\\partial f_0}{\\partial \\beta_0} = -2\\omega_3\\omega_2\\omega_1(f_3 - y_i) h_2 \\sin(f_2) \\cos(f_0)}$$\n",
    "\n",
    "**For $\\omega_0$:**\n",
    "\n",
    "$$\\frac{\\partial f_0}{\\partial \\omega_0} = x_i$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\omega_0} = \\frac{\\partial l_i}{\\partial f_0} \\cdot \\frac{\\partial f_0}{\\partial \\omega_0} = -2\\omega_3\\omega_2\\omega_1 x_i(f_3 - y_i) h_2 \\sin(f_2) \\cos(f_0)}$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Efficiency**: Each intermediate derivative (like $\\frac{\\partial l_i}{\\partial f_3}$, $\\frac{\\partial l_i}{\\partial f_2}$, etc.) is computed **only once** and reused for multiple parameter gradients.\n",
    "\n",
    "2. **Sequential Computation**: We move backwards through the network, computing gradients layer by layer.\n",
    "\n",
    "3. **Reuse**: Notice how $2(f_3 - y_i)$ appears in all gradients, $\\sin(f_2)$ appears in all gradients from layer 2 backwards, etc.\n",
    "\n",
    "4. **Storage**: During the forward pass, we store $f_0, h_1, f_1, h_2, f_2, h_3, f_3$. During the backward pass, we compute and store intermediate gradients.\n",
    "\n",
    "This is the essence of **backpropagation**: compute once, reuse everywhere!\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img  src=\"../images/chap6/bckwadpass.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c77948",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img  src=\"../images/chap6/forwardpass.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "\n",
    "</div>\n",
    "\n",
    "**Strong Reccomendation: Try to solve this on your own before reading on**\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "- **Input:** $\\mathbf{x} \\in \\mathbb{R}^{3}$ (3-dimensional input vector)\n",
    "- **Layer 1:** 4 hidden units → $\\mathbf{h}_1 \\in \\mathbb{R}^{4}$\n",
    "- **Layer 2:** 2 hidden units → $\\mathbf{h}_2 \\in \\mathbb{R}^{2}$\n",
    "- **Layer 3:** 3 hidden units → $\\mathbf{h}_3 \\in \\mathbb{R}^{3}$\n",
    "- **Output Layer:** $\\mathbf{f} \\in \\mathbb{R}^{2}$ (2-dimensional output)\n",
    "- **Loss:** $L = \\sum_i l_i$ where $l_i = \\|\\mathbf{f}(\\mathbf{x}_i, \\phi) - \\mathbf{y}_i\\|^2$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Forward Pass Equations\n",
    "\n",
    "**Layer 0 → Layer 1:**\n",
    "$$\\mathbf{f}_0 = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}$$\n",
    "$$\\mathbf{h}_1 = a[\\mathbf{f}_0]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^{4}$ (bias vector)\n",
    "- $\\boldsymbol{\\Omega}_0 \\in \\mathbb{R}^{4 \\times 3}$ (weight matrix)\n",
    "- $\\mathbf{f}_0 \\in \\mathbb{R}^{4}$ (pre-activation)\n",
    "- $\\mathbf{h}_1 \\in \\mathbb{R}^{4}$ (activation)\n",
    "\n",
    "**Layer 1 → Layer 2:**\n",
    "$$\\mathbf{f}_1 = \\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{h}_1$$\n",
    "$$\\mathbf{h}_2 = a[\\mathbf{f}_1]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_1 \\in \\mathbb{R}^{2}$ \n",
    "- $\\boldsymbol{\\Omega}_1 \\in \\mathbb{R}^{2 \\times 4}$\n",
    "- $\\mathbf{f}_1 \\in \\mathbb{R}^{2}$\n",
    "- $\\mathbf{h}_2 \\in \\mathbb{R}^{2}$\n",
    "\n",
    "**Layer 2 → Layer 3:**\n",
    "$$\\mathbf{f}_2 = \\boldsymbol{\\beta}_2 + \\boldsymbol{\\Omega}_2 \\mathbf{h}_2$$\n",
    "$$\\mathbf{h}_3 = a[\\mathbf{f}_2]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_2 \\in \\mathbb{R}^{3}$\n",
    "- $\\boldsymbol{\\Omega}_2 \\in \\mathbb{R}^{3 \\times 2}$\n",
    "- $\\mathbf{f}_2 \\in \\mathbb{R}^{3}$\n",
    "- $\\mathbf{h}_3 \\in \\mathbb{R}^{3}$\n",
    "\n",
    "**Layer 3 → Output:**\n",
    "$$\\mathbf{f}_3 = \\boldsymbol{\\beta}_3 + \\boldsymbol{\\Omega}_3 \\mathbf{h}_3$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_3 \\in \\mathbb{R}^{2}$\n",
    "- $\\boldsymbol{\\Omega}_3 \\in \\mathbb{R}^{2 \\times 3}$\n",
    "- $\\mathbf{f}_3 \\in \\mathbb{R}^{2}$ (final output)\n",
    "\n",
    "**Loss:**\n",
    "$$l_i = \\|\\mathbf{f}_3 - \\mathbf{y}_i\\|^2 = (\\mathbf{f}_3 - \\mathbf{y}_i)^T(\\mathbf{f}_3 - \\mathbf{y}_i)$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Backward Pass: Computing Gradients\n",
    "\n",
    "#### Step 0: Gradient of Loss w.r.t. Output\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_3} = 2(\\mathbf{f}_3 - \\mathbf{y}_i)$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 3: Gradients w.r.t. $\\boldsymbol{\\beta}_3$ and $\\boldsymbol{\\Omega}_3$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_3$:**\n",
    "\n",
    "Since $\\mathbf{f}_3 = \\boldsymbol{\\beta}_3 + \\boldsymbol{\\Omega}_3 \\mathbf{h}_3$:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{f}_3}{\\partial \\boldsymbol{\\beta}_3} = \\mathbf{I}_{2 \\times 2}$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_3} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_3} = 2(\\mathbf{f}_3 - \\mathbf{y}_i)}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_3$:**\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{f}_3}{\\partial \\boldsymbol{\\Omega}_3} = \\mathbf{h}_3^T$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_3} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_3} \\mathbf{h}_3^T = 2(\\mathbf{f}_3 - \\mathbf{y}_i) \\mathbf{h}_3^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2 \\times 3}$ (outer product)\n",
    "\n",
    "---\n",
    "\n",
    "#### Propagate to Layer 3: $\\frac{\\partial l_i}{\\partial \\mathbf{h}_3}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{h}_3} = \\boldsymbol{\\Omega}_3^T \\frac{\\partial l_i}{\\partial \\mathbf{f}_3} = 2\\boldsymbol{\\Omega}_3^T(\\mathbf{f}_3 - \\mathbf{y}_i)$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial \\mathbf{f}_2}$\n",
    "\n",
    "Since $\\mathbf{h}_3 = a[\\mathbf{f}_2]$:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{h}_3}{\\partial \\mathbf{f}_2} = \\text{diag}(a'[\\mathbf{f}_2])$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_2} = \\text{diag}(a'[\\mathbf{f}_2]) \\cdot \\frac{\\partial l_i}{\\partial \\mathbf{h}_3}$$\n",
    "\n",
    "Or element-wise:\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_2} = a'[\\mathbf{f}_2] \\odot \\frac{\\partial l_i}{\\partial \\mathbf{h}_3}$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Layer 2: Gradients w.r.t. $\\boldsymbol{\\beta}_2$ and $\\boldsymbol{\\Omega}_2$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_2$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_2} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_2}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_2$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_2} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_2} \\mathbf{h}_2^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3 \\times 2}$\n",
    "\n",
    "---\n",
    "\n",
    "#### Propagate to Layer 2: $\\frac{\\partial l_i}{\\partial \\mathbf{h}_2}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{h}_2} = \\boldsymbol{\\Omega}_2^T \\frac{\\partial l_i}{\\partial \\mathbf{f}_2}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial \\mathbf{f}_1}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_1} = a'[\\mathbf{f}_1] \\odot \\frac{\\partial l_i}{\\partial \\mathbf{h}_2}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 1: Gradients w.r.t. $\\boldsymbol{\\beta}_1$ and $\\boldsymbol{\\Omega}_1$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_1$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_1} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_1}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_1$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_1} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_1} \\mathbf{h}_1^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "#### Propagate to Layer 1: $\\frac{\\partial l_i}{\\partial \\mathbf{h}_1}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{h}_1} = \\boldsymbol{\\Omega}_1^T \\frac{\\partial l_i}{\\partial \\mathbf{f}_1}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l_i}{\\partial \\mathbf{f}_0}$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_0} = a'[\\mathbf{f}_0] \\odot \\frac{\\partial l_i}{\\partial \\mathbf{h}_1}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 0: Gradients w.r.t. $\\boldsymbol{\\beta}_0$ and $\\boldsymbol{\\Omega}_0$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_0$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_0} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_0}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_0$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_0} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_0} \\mathbf{x}^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Backpropagation Algorithm\n",
    "\n",
    "1. **Forward Pass:** Compute and store $\\mathbf{f}_0, \\mathbf{h}_1, \\mathbf{f}_1, \\mathbf{h}_2, \\mathbf{f}_2, \\mathbf{h}_3, \\mathbf{f}_3$\n",
    "\n",
    "2. **Backward Pass:** \n",
    "   - Start from $\\frac{\\partial l_i}{\\partial \\mathbf{f}_3}$\n",
    "   - For each layer $k = 3, 2, 1, 0$:\n",
    "     - Compute $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_k}$ and $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_k}$\n",
    "     - Propagate gradient backwards: $\\frac{\\partial l_i}{\\partial \\mathbf{h}_k} = \\boldsymbol{\\Omega}_k^T \\frac{\\partial l_i}{\\partial \\mathbf{f}_k}$\n",
    "     - Apply activation derivative: $\\frac{\\partial l_i}{\\partial \\mathbf{f}_{k-1}} = a'[\\mathbf{f}_{k-1}] \\odot \\frac{\\partial l_i}{\\partial \\mathbf{h}_k}$\n",
    "\n",
    "3. **Update Parameters:** Use computed gradients in SGD update rule\n",
    "\n",
    "---\n",
    "\n",
    "### Key Pattern: The Backpropagation Recursion\n",
    "\n",
    "For layer $k$:\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}_k} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_k}$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_k} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{h}_k} = \\boldsymbol{\\Omega}_{k+1}^T \\frac{\\partial l_i}{\\partial \\mathbf{f}_{k+1}}$$\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\mathbf{f}_{k-1}} = a'[\\mathbf{f}_{k-1}] \\odot \\frac{\\partial l_i}{\\partial \\mathbf{h}_k}$$\n",
    "\n",
    "This pattern repeats for every layer!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3747160",
   "metadata": {},
   "source": [
    "## Backpropagation in Branched Networks\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "### Branched Network Architecture Example\n",
    "\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img  src=\"../images/chap6/branched_net.png\" alt=\"2-Layer Net\" width=\"700\" />\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**Strong Recommendation: Try to solve this on your own before reading on**\n",
    "\n",
    "This example demonstrates backpropagation in a network that **branches and then rejoins**, which is common in modern architectures like ResNets, Inception networks, and U-Nets.\n",
    "\n",
    "---\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "- **Input:** $\\mathbf{x} \\in \\mathbb{R}^{3}$ (3-dimensional input vector)\n",
    "- **Layer 1:** 4 hidden units → $\\mathbf{h}_1 \\in \\mathbb{R}^{4}$\n",
    "- **Branch A:** 2 hidden units → $\\mathbf{h}_A \\in \\mathbb{R}^{2}$\n",
    "- **Branch B:** 3 hidden units → $\\mathbf{h}_B \\in \\mathbb{R}^{3}$\n",
    "- **Join Layer:** 2 hidden units → $\\mathbf{h}_J \\in \\mathbb{R}^{2}$\n",
    "- **Output Layer:** $\\mathbf{f} \\in \\mathbb{R}^{2}$ (2-dimensional output)\n",
    "- **Loss:** $l = \\|\\mathbf{f} - \\mathbf{y}\\|^2$\n",
    "\n",
    "**Key Insight:** After Layer 1, the computation splits into two parallel branches (A and B), which are then concatenated and fed into the Join Layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass Equations\n",
    "\n",
    "**Layer 1:**\n",
    "$$\\mathbf{f}_1 = \\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{x}$$\n",
    "$$\\mathbf{h}_1 = a_1[\\mathbf{f}_1]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_1 \\in \\mathbb{R}^{4}$ (bias vector)\n",
    "- $\\boldsymbol{\\Omega}_1 \\in \\mathbb{R}^{4 \\times 3}$ (weight matrix)\n",
    "- $\\mathbf{f}_1 \\in \\mathbb{R}^{4}$ (pre-activation)\n",
    "- $\\mathbf{h}_1 \\in \\mathbb{R}^{4}$ (activation)\n",
    "\n",
    "---\n",
    "\n",
    "**Branch A:**\n",
    "$$\\mathbf{f}_A = \\boldsymbol{\\beta}_A + \\boldsymbol{\\Omega}_A \\mathbf{h}_1$$\n",
    "$$\\mathbf{h}_A = a_A[\\mathbf{f}_A]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_A \\in \\mathbb{R}^{2}$\n",
    "- $\\boldsymbol{\\Omega}_A \\in \\mathbb{R}^{2 \\times 4}$\n",
    "- $\\mathbf{f}_A \\in \\mathbb{R}^{2}$\n",
    "- $\\mathbf{h}_A \\in \\mathbb{R}^{2}$\n",
    "\n",
    "---\n",
    "\n",
    "**Branch B (parallel to Branch A):**\n",
    "$$\\mathbf{f}_B = \\boldsymbol{\\beta}_B + \\boldsymbol{\\Omega}_B \\mathbf{h}_1$$\n",
    "$$\\mathbf{h}_B = a_B[\\mathbf{f}_B]$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_B \\in \\mathbb{R}^{3}$\n",
    "- $\\boldsymbol{\\Omega}_B \\in \\mathbb{R}^{3 \\times 4}$\n",
    "- $\\mathbf{f}_B \\in \\mathbb{R}^{3}$\n",
    "- $\\mathbf{h}_B \\in \\mathbb{R}^{3}$\n",
    "\n",
    "---\n",
    "\n",
    "**Join Layer (concatenate branches):**\n",
    "$$\\mathbf{f}_J = \\boldsymbol{\\beta}_J + \\boldsymbol{\\Omega}_J \\begin{bmatrix} \\mathbf{h}_A \\\\ \\mathbf{h}_B \\end{bmatrix}$$\n",
    "$$\\mathbf{h}_J = a_J[\\mathbf{f}_J]$$\n",
    "\n",
    "where:\n",
    "- $\\begin{bmatrix} \\mathbf{h}_A \\\\ \\mathbf{h}_B \\end{bmatrix} \\in \\mathbb{R}^{5}$ (concatenation of Branch A and B outputs)\n",
    "- $\\boldsymbol{\\beta}_J \\in \\mathbb{R}^{2}$\n",
    "- $\\boldsymbol{\\Omega}_J \\in \\mathbb{R}^{2 \\times 5}$\n",
    "- $\\mathbf{f}_J \\in \\mathbb{R}^{2}$\n",
    "- $\\mathbf{h}_J \\in \\mathbb{R}^{2}$\n",
    "\n",
    "---\n",
    "\n",
    "**Output Layer:**\n",
    "$$\\mathbf{f} = \\boldsymbol{\\beta}_O + \\boldsymbol{\\Omega}_O \\mathbf{h}_J$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\beta}_O \\in \\mathbb{R}^{2}$\n",
    "- $\\boldsymbol{\\Omega}_O \\in \\mathbb{R}^{2 \\times 2}$\n",
    "- $\\mathbf{f} \\in \\mathbb{R}^{2}$ (final output)\n",
    "\n",
    "**Loss:**\n",
    "$$l = \\|\\mathbf{f} - \\mathbf{y}\\|^2 = (\\mathbf{f} - \\mathbf{y})^T(\\mathbf{f} - \\mathbf{y})$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Backward Pass: Computing Gradients\n",
    "\n",
    "#### Step 0: Gradient of Loss w.r.t. Output\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{f}} = 2(\\mathbf{f} - \\mathbf{y})$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Output Layer: Gradients w.r.t. $\\boldsymbol{\\beta}_O$ and $\\boldsymbol{\\Omega}_O$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_O$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\beta}_O} = \\frac{\\partial l}{\\partial \\mathbf{f}} = 2(\\mathbf{f} - \\mathbf{y})}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_O$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\Omega}_O} = \\frac{\\partial l}{\\partial \\mathbf{f}} \\mathbf{h}_J^T = 2(\\mathbf{f} - \\mathbf{y}) \\mathbf{h}_J^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2 \\times 2}$ (outer product)\n",
    "\n",
    "---\n",
    "\n",
    "#### Propagate to Join Layer: $\\frac{\\partial l}{\\partial \\mathbf{h}_J}$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_J} = \\boldsymbol{\\Omega}_O^T \\frac{\\partial l}{\\partial \\mathbf{f}} = 2\\boldsymbol{\\Omega}_O^T(\\mathbf{f} - \\mathbf{y})$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l}{\\partial \\mathbf{f}_J}$\n",
    "\n",
    "Since $\\mathbf{h}_J = a_J[\\mathbf{f}_J]$:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{f}_J} = a_J'[\\mathbf{f}_J] \\odot \\frac{\\partial l}{\\partial \\mathbf{h}_J}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Join Layer: Gradients w.r.t. $\\boldsymbol{\\beta}_J$ and $\\boldsymbol{\\Omega}_J$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_J$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\beta}_J} = \\frac{\\partial l}{\\partial \\mathbf{f}_J}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_J$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\Omega}_J} = \\frac{\\partial l}{\\partial \\mathbf{f}_J} \\begin{bmatrix} \\mathbf{h}_A \\\\ \\mathbf{h}_B \\end{bmatrix}^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2 \\times 5}$ (outer product with concatenated vector)\n",
    "\n",
    "---\n",
    "\n",
    "#### Propagate to Concatenated Branches: $\\frac{\\partial l}{\\partial \\begin{bmatrix} \\mathbf{h}_A \\\\ \\mathbf{h}_B \\end{bmatrix}}$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\begin{bmatrix} \\mathbf{h}_A \\\\ \\mathbf{h}_B \\end{bmatrix}} = \\boldsymbol{\\Omega}_J^T \\frac{\\partial l}{\\partial \\mathbf{f}_J}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{5}$\n",
    "\n",
    "**Critical Step:** Split this gradient into two parts:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_A} = \\text{first 2 elements of } \\boldsymbol{\\Omega}_J^T \\frac{\\partial l}{\\partial \\mathbf{f}_J}$$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_B} = \\text{last 3 elements of } \\boldsymbol{\\Omega}_J^T \\frac{\\partial l}{\\partial \\mathbf{f}_J}$$\n",
    "\n",
    "**Store both!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch A: Compute $\\frac{\\partial l}{\\partial \\mathbf{f}_A}$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{f}_A} = a_A'[\\mathbf{f}_A] \\odot \\frac{\\partial l}{\\partial \\mathbf{h}_A}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch A: Gradients w.r.t. $\\boldsymbol{\\beta}_A$ and $\\boldsymbol{\\Omega}_A$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_A$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\beta}_A} = \\frac{\\partial l}{\\partial \\mathbf{f}_A}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_A$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\Omega}_A} = \\frac{\\partial l}{\\partial \\mathbf{f}_A} \\mathbf{h}_1^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{2 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch A: Propagate to $\\mathbf{h}_1$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(A)} = \\boldsymbol{\\Omega}_A^T \\frac{\\partial l}{\\partial \\mathbf{f}_A}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**Store this - but DON'T use it yet!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch B: Compute $\\frac{\\partial l}{\\partial \\mathbf{f}_B}$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{f}_B} = a_B'[\\mathbf{f}_B] \\odot \\frac{\\partial l}{\\partial \\mathbf{h}_B}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch B: Gradients w.r.t. $\\boldsymbol{\\beta}_B$ and $\\boldsymbol{\\Omega}_B$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_B$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\beta}_B} = \\frac{\\partial l}{\\partial \\mathbf{f}_B}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_B$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\Omega}_B} = \\frac{\\partial l}{\\partial \\mathbf{f}_B} \\mathbf{h}_1^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{3 \\times 4}$\n",
    "\n",
    "---\n",
    "\n",
    "#### Branch B: Propagate to $\\mathbf{h}_1$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(B)} = \\boldsymbol{\\Omega}_B^T \\frac{\\partial l}{\\partial \\mathbf{f}_B}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Critical Step: Merge Gradients at $\\mathbf{h}_1$**\n",
    "\n",
    "Since $\\mathbf{h}_1$ feeds into **both** Branch A and Branch B, we must **sum** the gradients from both paths:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\mathbf{h}_1} = \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(A)} + \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(B)}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**This is the key difference from sequential networks!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Compute $\\frac{\\partial l}{\\partial \\mathbf{f}_1}$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{f}_1} = a_1'[\\mathbf{f}_1] \\odot \\frac{\\partial l}{\\partial \\mathbf{h}_1}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**Store this!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 1: Gradients w.r.t. $\\boldsymbol{\\beta}_1$ and $\\boldsymbol{\\Omega}_1$\n",
    "\n",
    "**For $\\boldsymbol{\\beta}_1$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\beta}_1} = \\frac{\\partial l}{\\partial \\mathbf{f}_1}}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4}$\n",
    "\n",
    "**For $\\boldsymbol{\\Omega}_1$:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l}{\\partial \\boldsymbol{\\Omega}_1} = \\frac{\\partial l}{\\partial \\mathbf{f}_1} \\mathbf{x}^T}$$\n",
    "\n",
    "**Shape:** $\\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Backpropagation in Branched Networks\n",
    "\n",
    "1. **Forward Pass:** Compute and store $\\mathbf{f}_1, \\mathbf{h}_1, \\mathbf{f}_A, \\mathbf{h}_A, \\mathbf{f}_B, \\mathbf{h}_B, \\mathbf{f}_J, \\mathbf{h}_J, \\mathbf{f}$\n",
    "\n",
    "2. **Backward Pass:** \n",
    "   - Start from $\\frac{\\partial l}{\\partial \\mathbf{f}}$\n",
    "   - Propagate back through Output Layer and Join Layer\n",
    "   - **Split gradient** at the concatenation point into $\\frac{\\partial l}{\\partial \\mathbf{h}_A}$ and $\\frac{\\partial l}{\\partial \\mathbf{h}_B}$\n",
    "   - Compute gradients for Branch A and Branch B **independently**\n",
    "   - **Sum the gradients** that flow back to $\\mathbf{h}_1$: $\\frac{\\partial l}{\\partial \\mathbf{h}_1} = \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(A)} + \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(B)}$\n",
    "   - Continue backpropagation through Layer 1\n",
    "\n",
    "3. **Key Rule for Branching:** When a layer's output feeds into multiple paths, **sum all gradients** flowing back from those paths.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Sum the Gradients?\n",
    "\n",
    "By the **multivariate chain rule**, when a variable $\\mathbf{h}_1$ affects the loss through multiple paths:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mathbf{h}_1} = \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(\\text{via Branch A})} + \\frac{\\partial l}{\\partial \\mathbf{h}_1}^{(\\text{via Branch B})}$$\n",
    "\n",
    "This is because:\n",
    "- Loss depends on $\\mathbf{h}_A$, which depends on $\\mathbf{h}_1$\n",
    "- Loss also depends on $\\mathbf{h}_B$, which depends on $\\mathbf{h}_1$\n",
    "- Total change in loss = sum of changes through all paths\n",
    "\n",
    "This principle generalizes to any acyclic computational graph!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
