{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ff5338",
   "metadata": {},
   "source": [
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdccd1",
   "metadata": {},
   "source": [
    "The defining characteristic of this class of learning models, is that they learn purely from a set of observed data $\\{x_i\\}$ with no labels.\n",
    "\n",
    "That being said some of the models may have different goals such as: \n",
    "1. Generate plausible new samples from the dataset.\n",
    "2. Manipulate, denoise interpolate or compress instances.\n",
    "\n",
    "In this chapter we'll discuss the toxonomy of unsupervised learning models, their desired property of these models and how we measure performance. \n",
    "\n",
    "In sub-chapters of this file will present specific (Generative) models:\n",
    "1. Generative adversarial networks (GANs)\n",
    "2. Normalizing Flows\n",
    "3. Variational Autoencoders (VAEs)\n",
    "4. Diffusion Models\n",
    "\n",
    "These Generative Models can be sub-categorised:\n",
    "\n",
    "| Generative Type | Data | Goal | Exmaples |\n",
    "|-----------------|------|------|----------|\n",
    "| **Explicit Models** | $\\{\\mathbf{x_i}\\}$, i.i.d. sampled from some **unknown** distribution $P(\\mathbf{x})$| Estimate the proability function $q(\\mathbf{x}) \\approx p(\\mathbf{x})$ | VAE, Noramlizing Flow|\n",
    "| **Implicit Model** | $\\{\\mathbf{x_i}\\}$, i.i.d. sampled from some **unknown** distribution $P(\\mathbf{x})$ | Generate new samples $\\mathbf{x^*}$ ~ $Pr(\\mathbf{x})$ | GANs and Diffusion models | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a76c5",
   "metadata": {},
   "source": [
    "## Taxonomy of unsupervied Learning models\n",
    "\n",
    "- Define a mapping between the data examples $\\mathbf{x}$ and a set of unseen $latent$ variables $\\mathbf{z}$.\n",
    "\n",
    "$\\mathbf{z}$ capture underlying structure in the dataset and usually have a lower dimension than the original data, can be thought of as **compressed version** of $\\mathbf{x}$.\n",
    "\n",
    "This mapping works in both ways meaning: \n",
    "\n",
    "1. Some models map from $\\mathbf{x} \\to \\mathbf{z}$ - $k–Means: \\mathbf{z} \\in \\{1, 2, \\dots, K\\}$\n",
    "2. Other models map from the latent variables $\\mathbf{z}$ to data $\\mathbf{x}$ - $Generative Models$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ebb24",
   "metadata": {},
   "source": [
    "## Desried Properties of Generative Models\n",
    "\n",
    "|**Property** | **Explanation** |\n",
    "|-------------|-----------------|\n",
    "| **Efficient Sampling** | Generating samples from the model should be computationally **Inexpensive** and should make use of GPUs|\n",
    "| **High Quality Sampling** | The samples should be **indistinguishable** from the real data, which the model was trained |\n",
    "| **Coverage** | The model should be able to produce samples from the whole training distribution |\n",
    "| **Well-behaved Latent space**| Every Latent variable $\\mathbf{z}$ corresponds to a plausibble data example $\\mathbf{x}$ | \n",
    "| **Disentangled latent space** | If we manipulate a dimension in $\\mathbf{z}$ then it should correspond to some interpertable property of $\\mathbf{x^*}$ |\n",
    "|**Efficient Likelihood Computation** | If the model is probabilistic, then calculating the probabilitiy of new examples should be **accurate** and **efficient**|\n",
    "\n",
    "Below presents a table of the aformentioned properties pertained in the models to be discussed.\n",
    "\n",
    "| Model | Efficient | Sample Quality | Coverage | Well-behaved Latent Space | Disentangled Latent Space | Efficient Likelihood |\n",
    "|------|-----------|-----------------|----------|---------------------------|--------------------------|-----------------------|\n",
    "| GANs |  Yes | Yes | No | Yes | Unclear | Not Probabilistic | \n",
    "| VAEs | Yes | No | Unclear | Yes | Unclear | No | \n",
    "| Flows | Yes | No | Unclear | Yes | Unclear | Yes | \n",
    "| Diffusion | No | Yes | Unclear | No | No | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540d87e",
   "metadata": {},
   "source": [
    "## Quantifying Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89baebc2",
   "metadata": {},
   "source": [
    "### Test Likelihood\n",
    "\n",
    "**Why we don't measure on training data** \n",
    "It's ineffective to measure the training data likelihood beaucse a model could assing a very high probability to each training point and low on other areas, and of course the model would be able to generate the data it recieved, but this doesn't provide us with a good indication of the models capabilities. \n",
    "\n",
    "The test likelihood captures how well the model generalizes from the training data and also the coverage.\n",
    "\n",
    "This method isn't always revelvant: \n",
    "- GANs isn't probailistic \n",
    "- VAEs and Diffusion are not effecient in computing the likelihood\n",
    "\n",
    "### Inception Score\n",
    "\n",
    "This is a score specialised from images, and ideally for generative models trained on ImageNet database.\n",
    "\n",
    "We calculate the score using pre-trained classification model (such as Inception model). We pass the generated image to the Inception model:\n",
    "- The model should be able to classify one of the classes with a high probability. (Quality Sampling)\n",
    "- Given the generative model created $N$ images, it images it creates should be uniformly distributed across all classes (Coverage Sampling)\n",
    "\n",
    "$$IS = \\exp\\left[\\frac{1}{I} \\sum_{i=1}^ID_{KL}\\left[Pr(y | \\mathbf{x_i^*}) || Pr(y) \\right]\\right]$$\n",
    "\n",
    "This measures the distance between two dstributions\n",
    "\n",
    "In this case we're measuring the average distance between the classes in which the model is producing the images compared with the general distribution of the classes.\n",
    "\n",
    "### Fréchet Inception Distance:\n",
    "This issue with the $D_{KL}$ divergence is that the model, isn't symmetric, meaning $D_{KL}(A || B) \\ne D_{KL}( B || A)$ therefore the Fréchet distance, converts this divergence to be symmetrical.\n",
    "\n",
    "Note: That this measure isn't based on the original data, but rather the deepest activations of the Inception Network."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
