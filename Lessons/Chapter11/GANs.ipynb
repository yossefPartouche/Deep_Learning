{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497c17ed",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b31a6",
   "metadata": {},
   "source": [
    "**Main Idea** \n",
    "We have **two** Networks running: \n",
    "1. The $Generator$: Which creates samples by mapping random noise to the data-space.\n",
    "2. The  $Discriminator$: Which aims to classify if the image is fake or real.\n",
    "\n",
    "In the event that the $Discriminator$ Correctly identifies a fake image, this will be sent back as a signal to the $Generator$ to improve it's sampling quality.\n",
    "\n",
    "- We will see that training GAN isn't stable. \n",
    "- Although it's caparable of producing **Quality Samples**, it doesn't have great **Coverage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4788906",
   "metadata": {},
   "source": [
    "## Discrimination as a signal\n",
    "\n",
    "We generate new samples $\\{\\mathbf{x_j}^*\\}$ that are drawn from the same distribution as a set of real training data $\\{\\mathbf{x_i}\\}$\n",
    "\n",
    "**How we generate a new sample**\n",
    "1. Choose a **simple and known** dsitribution\n",
    "2. **Randomly sample a point** from that dsitribution.\n",
    "3. Pass this data through a network $\\mathbf{x_j^*} = \\mathbf{g}[\\mathbf{z_j}, \\theta ]$, the $Generator$.\n",
    "   - The network aims to find the parameters that makes $\\mathbf{x_j^*}$ look similar to training data $\\{\\mathbf{x_i}\\}$.\n",
    "4. Our $Discriminator$ network, aims to classify its input $\\{Real, Generated\\}$\n",
    "   - If the discriminator classifies a generated image as a fake, it sends the **Gradient signal** back to the generator during backpropagation.\n",
    "\n",
    "### GAN loss Function \n",
    "\n",
    "Our Discriminator: $f[\\mathbf{x}, \\phi] \\in \\mathbb{R}$\n",
    "- The **higher** the **value** the **more** it believes the image to be **Real**\n",
    "- The Goal of the discrimnator is to minimize loss, and this is a classic binary-classification problem.\n",
    "$$\n",
    "\\hat{\\phi} = \\text{arg}\\min_{\\phi} \\left[\\sum_i -(1-y_i)\\log\\left(1- \\sigma(f[\\mathbf{x}_i, \\phi])\\right) - y_i\\log\\left(\\sigma(f[\\mathbf{x}_i, \\phi])\\right)\\right]\n",
    "$$\n",
    "- We **define** $real$ examples $\\mathbf{x_i}$ to have label $y=1$ and $generated$ examples $\\mathbf{x_j^*}$:\n",
    "  \n",
    "$$\n",
    "\\hat{\\phi} = \\text{arg}\\min_{\\phi} \\left[\\sum_j -\\log\\left(1- \\sigma(f[\\mathbf{x_j^*}, \\phi])\\right) - \\sum_i\\log\\left(\\sigma(f[\\mathbf{x}_i, \\phi])\\right)\\right]\n",
    "$$\n",
    "\n",
    "Our Generator: $\\mathbf{x_j^*} = g[\\mathbf{z_j}, \\theta] \\in \\mathbb{R}^n$\n",
    "- The Generator wants to produce the largest value possible, but it's always being checked by the discriminator, so maximize it's minimum:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\text{arg}\\max_{\\theta}\\left[\\min_{\\phi} \\left[\\sum_j -\\log\\left(1- \\sigma(f[\\mathbf{x_j^*}, \\phi])\\right) - \\sum_i\\log\\left(\\sigma(f[\\mathbf{x}_i, \\phi])\\right)\\right]\\right]\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
