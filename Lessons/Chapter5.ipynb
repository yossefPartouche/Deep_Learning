{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20805bbd",
   "metadata": {},
   "source": [
    "# Fitting Models\n",
    "\n",
    "In the first few chapters we learnt the process of passing the data forward through the (Shallow/Deep) network. <br>\n",
    "In the previous chapter we discussed how to measure the missmatch between the network predictions and the ground truth for a training set. \n",
    "\n",
    "These are crucial but partial aspects of the model creation, indeed after recieveing an answer from the loss function we must correct our model to improve. <br>\n",
    "This is known as $\\text{Fitting or Training}$ the model.\n",
    "\n",
    "This is a process where we'll improve our paramters by: \n",
    "\n",
    "1. Computing the derivative with respect to the parameters.\n",
    "2. Adjust the parameters based on the gradients to reduce the loss.\n",
    "\n",
    "Note that in Chapter 1, we were able to find a closed for the linear regression problem, however, when the network becomes to complex, this becomes a less viable option. <br>\n",
    "Instead we'll follow an iterative approach which we'll show that under certain general conditions produces good approximation results.\n",
    "\n",
    "Here we focus on step (2) in the process of improving the parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa94f50",
   "metadata": {},
   "source": [
    "<div  align=\"center\">\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    " $$\\boxed{\\hat{\\phi} = \\text{argmin}_{\\phi}\\big[L[\\phi]\\big]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce388",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ed22e",
   "metadata": {},
   "source": [
    "$\\hspace{7cm} \\textbf{Step 0: Initialise parameters so some values: }$\n",
    "$$ \\phi = [\\phi_0, \\phi_1, \\dots, \\phi_k]^T$$\n",
    "\n",
    "\n",
    "$$\\boxed{\\begin{aligned}\n",
    "&\\textbf{Step 1: Up-hill - Compute the derivatives of the loss with respect to the parameters} \\\\\n",
    "&\\hspace{6cm} \\frac{\\partial L}{\\partial \\phi} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\phi_0} \\\\ \\frac{\\partial L}{\\partial \\phi_1} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial \\phi_k} \\end{bmatrix} \\\\[1em]\n",
    "&\\textbf{Step 2: Down-Hill - Update the parameters according to the rule} \\\\\n",
    "&\\hspace{6cm} \\phi = \\phi - \\alpha \\cdot \\frac{\\partial L}{\\partial \\phi}\n",
    "\\end{aligned}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5175",
   "metadata": {},
   "source": [
    "### 1D Linear Regression Example \n",
    "\n",
    "In the first chapter we showed the closed form, now we'll present the iterative solution to this problem.\n",
    "\n",
    "$$\\boxed{\\text{model: } f[x, \\phi] \\qquad \\text{parameters:} \\phi = [\\phi_0, \\phi_1]^T \\qquad \\text{Input: }x \\in \\mathbb{R}}$$\n",
    "\n",
    "$$\\boxed{y = f[x, \\phi] = \\phi_0 + \\phi_1 x}$$\n",
    "\n",
    "We know that our loss is the mean squared Error\n",
    "\n",
    "$$L[\\phi] = \\sum_{i=1}^N l_i = \\sum_{i=1}^N (f[x_i, \\phi] - y_i)^2 =  \\sum_{i=1}^N (\\phi_0 + \\phi_1 x - y_i)^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d986a",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1** \n",
    "\n",
    "$$\\frac{\\partial L[\\phi]}{\\partial \\phi} = \\frac{\\partial }{\\partial \\phi}\\sum_{i=1}^N l_i \\underset{\\text{linearity }}{=} \\sum_{i=1}^N\\frac{\\partial l_i}{\\partial \\phi} = \\sum_{i=1}^N \\begin{bmatrix} \\frac{\\partial l_i}{\\partial \\phi_0} \\\\ \\frac{\\partial l_i}{\\partial \\phi_1} \\end{bmatrix} = \\sum_{i=1}^N \\begin{bmatrix} 2\\cdot 1(\\phi_0 + \\phi_1x_i - y_i) \\\\ 2 x_i(\\phi_0 + \\phi_1x_i - y_i)\\cdot \\end{bmatrix}$$\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "$$ \\phi = \\phi - \\alpha \\frac{\\partial L}{\\partial \\phi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36eda4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "**Gradient Descent Visualization:**\n",
    "\n",
    "<img src=\"./images/chap5/1dgradDescent_reg.gif\" alt=\"Gradient Descent Animation\" width=\"500\" />\n",
    "<img src=\"./images/chap5/lossSpace.png\" alt=\"Loss Space\" width=\"390\" />\n",
    "\n",
    "*If the animation doesn't display, [click here to view the video](./images/chap5/1dgradDescent_reg.gif)*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772d1cc",
   "metadata": {},
   "source": [
    "### Gabor model Example \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bcaf3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The loss functions for linear regression problems always have a single well-defined minimum. This is known as the **convex property**. <br>\n",
    "The advantage of this property is that gradient descent is guaranteed to find this minimum. <br>\n",
    "\n",
    "However, the loss functions for most non-linear models are **non-convex**, meaning the loss landscape isn't \"bowl-shaped\".<br> This means the algorithm may get stuck in local minima and not necessarily find the global minimum. <br>\n",
    "\n",
    "**Analogy:** Imagine you're hiking down a mountainous region at night, surrounded by dense forest and vegetation, trying to reach the lowest point in the valley. The only tool you have is your sense of the ground beneath your feet, which tells you the local slope (steepness) at your current position. \n",
    "\n",
    "- In a **convex landscape** (linear regression), there's one clear valley, and walking downhill always leads you to the bottom.\n",
    "- In a **non-convex landscape** (deep networks), there are multiple valleys, hills, and plateaus. <br> Following the steepest descent might lead you to a nearby valley, but not necessarily the deepest one. You might get trapped in a local dip, unaware that a much deeper valley exists elsewhere.\n",
    "\n",
    "This is the fundamental challenge of training deep neural networks.\n",
    "\n",
    "We'll look at a simple nonlinear model with two parameters to understand the properties of non-convex loss functions:\n",
    "\n",
    "$$f[x, \\phi] = \\sin[\\phi_0 + 0.006 \\cdot \\phi_1 x] \\cdot \\exp\\left(\\frac{(\\phi_0 + 0.006 \\cdot \\phi_1 x)^2}{32.0}\\right)$$\n",
    "\n",
    "The loss function is the mean squared error: $$L[\\phi] = \\sum_{i=1}^N(f[x_i, \\phi] - y_i)^2$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273de24c",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "1. **Sine Component:** $\\sin[\\phi_0 + 0.006 \\cdot \\phi_1 x]$ \n",
    "   - Creates oscillations (waves) in the output\n",
    "   - $\\phi_0$ shifts the wave horizontally (phase shift)\n",
    "   - $\\phi_1$ controls the frequency (how quickly the wave oscillates)\n",
    "\n",
    "2. **Exponential Component:** $\\exp\\left(\\frac{(\\phi_0 + 0.006 \\cdot \\phi_1 x)^2}{32.0}\\right)$\n",
    "   - Acts as an envelope that amplifies the sine wave\n",
    "   - Grows as we move away from the origin\n",
    "   - The same linear term $(\\phi_0 + 0.006 \\cdot \\phi_1 x)$ controls the rate of growth\n",
    "\n",
    "3. **Combined Effect:**\n",
    "   - Produces a sinusoidal function whose **amplitude grows exponentially** as $|x| \\to \\infty$\n",
    "   - Both the oscillation frequency and growth rate increase together\n",
    "   - The parameters $\\phi_0$ and $\\phi_1$ jointly determine:\n",
    "     - Where the pattern is centered\n",
    "     - How rapidly it expands and oscillates\n",
    "   - This creates a **non-convex loss landscape** with many local minima, making optimization challenging\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**Visualisation**\n",
    "\n",
    "<style>\n",
    ".zoom-img {\n",
    "    transition: transform 0.3s ease;\n",
    "    cursor: pointer;\n",
    "}\n",
    ".zoom-img:hover {\n",
    "    transform: scale(1.5);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./images/chap5/Gfunc1.png\" alt=\"Gabor Function 1\" width=\"400\" class=\"zoom-img\"/><br/>\n",
    "      φ₀ = -50, φ₁ = 46\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./images/chap5/Gfunc2.png\" alt=\"Gabor Function 2\" width=\"400\" class=\"zoom-img\"/><br/>\n",
    "      φ₀ = 100, φ₁ = 46\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./images/chap5/Gfunc3.png\" alt=\"Gabor Function 3\" width=\"400\" class=\"zoom-img\"/><br/>\n",
    "      φ₀ = -10, φ₁ = 46\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./images/chap5/Gfunc4.png\" alt=\"Gabor Function 4\" width=\"400\" class=\"zoom-img\"/><br/>\n",
    "      φ₀ = -10, φ₁ = 24\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "*Different parameter combinations showing various wave patterns and growth rates*\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3c6ec",
   "metadata": {},
   "source": [
    "Now that we have a better understanding of the Gabor function construction, let's explore what happens when we try to fit it to data.\n",
    "\n",
    "Suppose we're provided with a training set $\\{x_i, y_i\\}_{i=1}^N$\n",
    "\n",
    "### Local Minima and Saddle Points\n",
    "\n",
    "The images below show corresponding points in the loss space and their associated parameter values in the data space, illustrating the challenges of non-convex optimization.\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. **Multiple Local Minima:** The loss landscape contains many points where the gradient is zero. When gradient descent reaches such a point, it halts, even though it may not be the global minimum.\n",
    "\n",
    "2. **Ambiguity Problem:** When the algorithm stops, we have no way to determine whether we've reached a local minimum or the global minimum—they look identical from the gradient's perspective.\n",
    "\n",
    "3. **Saddle Points:** Most algorithms use early stopping mechanisms, making it difficult to achieve true zero gradient. This means we might stop at a **saddle point**—a point where the gradient is near zero but isn't a minimum at all (like the top of a mountain pass). Saddle points have similar gradient characteristics to minima, making them indistinguishable during optimization.\n",
    "\n",
    "<div align=\"center\">\n",
    " <img src=\"./images/chap5/minimasSpoint.png\" alt=\"Gabor Function 4\" width=\"500\"/><br/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4207f",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c60acf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Motivation: Escaping Local Minima\n",
    "\n",
    "Two potential approaches to address the local minima problem would be to either: \n",
    "1. Exhaustively try out **all** possible parameter combinations\n",
    "2. Initialize the weights at multiple different starting positions\n",
    "\n",
    "However, the number of parameters and potential minima are extremely large when dealing with Deep Neural Networks, making these approaches computationally infeasible. \n",
    "\n",
    "**Stochastic Gradient Descent** (SGD) tries to remedy this problem by adding **controlled randomness** to the algorithm at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### Three Variants of Gradient Descent\n",
    "\n",
    "#### 1. Full Batch Gradient Descent\n",
    "\n",
    "So far in our algorithm, we've been using the **entire dataset** and then updating the parameters. This is known as $\\textcolor{lightblue}{\\text{Full Batch}}$ or $\\textcolor{lightblue}{\\text{Batch}}$ Gradient Descent.\n",
    "\n",
    "$$\\boxed{\\phi_{t+1} = \\phi_{t} - \\alpha \\cdot \\frac{\\partial L[\\phi_t]}{\\partial \\phi} = \\phi_{t} - \\alpha \\cdot \\sum_{i=1}^{N} \\frac{\\partial l_i[\\phi_t]}{\\partial \\phi}}$$\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Stable, smooth convergence\n",
    "- ✅ Guaranteed to find minimum in convex problems\n",
    "- ❌ Computationally expensive for large datasets\n",
    "- ❌ Can get stuck in local minima (non-convex problems)\n",
    "- ❌ Memory intensive\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Stochastic Gradient Descent (SGD) - Single Sample\n",
    "\n",
    "Instead of using all data, we can update parameters based on a **single training example** at each iteration:\n",
    "\n",
    "$$\\boxed{\\phi_{t+1} = \\phi_{t} - \\frac{\\alpha}{N} \\cdot \\frac{\\partial l_i}{\\partial \\phi}}$$\n",
    "\n",
    "$$\\text{Where } l_i \\text{ is the loss for a single randomly selected training example}$$\n",
    "\n",
    "**Why \"Stochastic\"?** The term means \"random\" or \"probabilistic\"—we randomly select which training example to use at each step.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Very fast updates\n",
    "- ✅ Can escape local minima (due to noise)\n",
    "- ✅ Low memory requirements\n",
    "- ❌ Very noisy gradient estimates\n",
    "- ❌ Erratic convergence path\n",
    "- ❌ May never fully converge\n",
    "\n",
    "The high noise level means the model doesn't obtain a global view of the data like in full batch gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Mini-Batch SGD (The Practical Compromise)\n",
    "\n",
    "To add a **moderate amount of randomness**, we choose a random subset of the training data and compute the gradient from this subset alone. This is known as $\\textcolor{lightblue}{\\text{mini-batch}}$ or simply $\\textcolor{lightblue}{\\text{batch}}$ gradient descent.\n",
    "\n",
    "$$\\boxed{\\phi_{t+1} = \\phi_{t} - \\alpha \\cdot \\frac{1}{|B_t|}\\sum_{i \\in B_t} \\frac{\\partial l_i[\\phi_t]}{\\partial \\phi}}$$\n",
    "\n",
    "$$\\text{Where } B_t \\text{ is our current batch containing } |B_t| \\text{ input-output pairs}$$\n",
    "\n",
    "**Sampling Strategy:**\n",
    "- We usually sample **without replacement** within an epoch\n",
    "- Once we've used a batch for a parameter update, we select a different unseen batch\n",
    "- Once we've iterated through the **entire training set**, this completes one $\\textcolor{lightblue}{\\text{Epoch}}$\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Balanced trade-off between speed and stability\n",
    "- ✅ Can still escape local minima\n",
    "- ✅ Efficient GPU utilization\n",
    "- ✅ Moderate memory requirements\n",
    "- ✅ Most commonly used in practice\n",
    "\n",
    "---\n",
    "\n",
    "### Properties of Mini-Batch SGD\n",
    "\n",
    "1. **Sensible Updates:** Even though we're adding randomness to the learning trajectory, the algorithm still improves the fit on average at each iteration, so the updates remain meaningful.\n",
    "\n",
    "2. **Fair Representation:** Since we're iterating through the training examples without replacement, each training example contributes equally to the optimization trajectory within an epoch.\n",
    "\n",
    "3. **Computational Efficiency:** Processing smaller batches is less computationally expensive than processing the entire dataset, allowing for more frequent parameter updates.\n",
    "\n",
    "4. **Escape Mechanism:** The randomness introduced by mini-batching provides a mechanism to escape local minima and saddle points that would trap standard gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Summary\n",
    "<div align=\"center\">\n",
    "\n",
    "| Method | Batch Size | Speed | Convergence | Memory | Use Case |\n",
    "|--------|-----------|-------|-------------|---------|----------|\n",
    "| **Full Batch** | $N$ (all data) | Slowest | Smooth | High | Small datasets, convex problems |\n",
    "| **SGD** | 1 | Fastest | Noisy | Lowest | Online learning, very large datasets |\n",
    "| **Mini-Batch** | $32-256$ | Fast | Balanced | Moderate | **Most deep learning applications** |\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3fa7e",
   "metadata": {},
   "source": [
    "### Implementation: How many Batches and Epochs?\n",
    "\n",
    "Choosing the right batch size and number of epochs is crucial for effective training. Here are the key considerations:\n",
    "\n",
    "\n",
    "#### Batch Size Selection \n",
    "\n",
    "| **Small batches (1-32):** | **Large batches (256-1024+):** | \n",
    "|---------------------------|--------------------------------|\n",
    "| - ✅ More noise → better exploration of loss landscape <br> - ✅ Can escape local minima more easily <br> - ✅ Less memory required <br> - ❌ Slower training (more updates needed) <br> - ❌ Noisy gradient estimates | - ✅ Faster training (fewer updates) <br> - ✅ More stable gradient estimates <br> - ✅ Better hardware utilization (GPUs) <br> - ❌ May get stuck in sharp minima <br> - ❌ Requires more memory <br> - ❌ Less exploration|\n",
    "\n",
    "**Common practice:** Batch sizes of 32, 64, 128, or 256 provide a good balance.\n",
    "\n",
    "#### Number of Epochs\n",
    "\n",
    "An **epoch** is one complete pass through the entire training dataset.\n",
    "\n",
    "**How many epochs?**\n",
    "- Too few → **underfitting** (model hasn't learned enough)\n",
    "- Too many → **overfitting** (model memorizes training data)\n",
    "\n",
    "**Practical approach:**\n",
    "1. Monitor training and validation loss\n",
    "2. Stop when validation loss stops improving (**early stopping**)\n",
    "3. Typical range: 10-100+ epochs depending on problem complexity\n",
    "\n",
    "#### Example Calculation\n",
    "\n",
    "Given:\n",
    "- Training set size: $N = 1000$ examples\n",
    "- Batch size: $B = 100$\n",
    "- Number of epochs: $E = 50$\n",
    "\n",
    "**Number of batches per epoch:** $\\frac{N}{B} = \\frac{1000}{100} = 10$ batches\n",
    "\n",
    "**Total parameter updates:** $10 \\times 50 = 500$ updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c473c9f",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fd848",
   "metadata": {},
   "source": [
    "As mentioned earlier our main problems with SGD are:\n",
    "1. **Narrow ravines:** The loss function is steep vertically but shallow horizonatlly\n",
    "2. **Zero Gradients:** Zero gradient in local minima and saffle points\n",
    "3. **Noisy Gradients:** The more promininant with smaller batches/ single instances\n",
    "    \n",
    "A modification to SGD is to add a $momentum$ term. where the parameters are updated with a wighted combination of the gradient computed from current bach and the previous step: \n",
    "\n",
    "$$ m_{t+1} = \\beta \\cdot m_t + (1 - \\beta)\\sum_{i \\in B_t} \\frac{\\partial l_i[\\phi]}{\\partial \\phi} \\\\ \\phi_{t+1} = \\phi_{t} - \\alpha \\cdot m_{t+1}$$\n",
    "\n",
    "$ \\beta \\in [0, 1): \\text{ Control the degree to which the gradient is smoothed over time}$ <br>\n",
    "$ \\alpha \\text{ : The learning rate}$\n",
    "\n",
    "**Effect**\n",
    "\n",
    "The efffective learning rate increases if all the gradients are aligned over multiple iterations but decreases when gradients changes repeatedly. <br> This leads to smoother trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8433e",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated momentum\n",
    "\n",
    "This is improvement to the stand-alone momentum by computing the gradients at the predicted points rather then the current point.\n",
    "\n",
    "$$m_{t+1} = \\beta \\cdot m_t + (1 - \\beta)\\sum_{i \\in B_t} \\frac{\\partial l_i[\\phi_{t} - \\alpha \\beta \\cdot m_{t}]}{\\partial \\phi} \\\\ \\phi_{t+1} = \\phi_{t} - \\alpha \\cdot m_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011369f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img  src=\"images/chap5/momentum.png\" alt=\"Momentum vs None \" width=\"500\" />\n",
    "<img  src=\"images/chap5/nesterov.png\" alt=\"Nesterov Momentum \" width=\"270\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
