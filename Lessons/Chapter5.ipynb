{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20805bbd",
   "metadata": {},
   "source": [
    "# Fitting Models\n",
    "\n",
    "In the first few chapters we learnt the process of passing the data forward through the (Shallow/Deep) network. \n",
    "In the previous chapter we discussed how to measure the missmatch between the network predictions and the ground truth for a training set. \n",
    "\n",
    "These are crucial but partial aspects of the model creation, indeed after recieveing an answer from the loss function we must correct our model to improve. \n",
    "This is known as $\\text{Fitting \\ or \\ Training}$ the model.\n",
    "\n",
    "This is a process where we'll improve our paramters by: \n",
    "\n",
    "1. Computing the derivative with respect to the parameters.\n",
    "2. Adjust the parameters based on the gradients to reduce the loss.\n",
    "\n",
    "Note that in Chapter 1, we were able to find a closed for the linear regression problem, however, when the network becomes to complex, this becomes a less viable option. <br>\n",
    "Instead we'll follow an iterative approach which we'll show that under certain general conditions produces good approximation results.\n",
    "\n",
    "Here we focus on step (2) in the process of improving the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa94f50",
   "metadata": {},
   "source": [
    "<div  align=\"center\">\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    " $$\\boxed{\\hat{\\phi} = \\text{argmin}_{\\phi}\\big[L[\\phi]\\big]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce388",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ed22e",
   "metadata": {},
   "source": [
    "$\\hspace{7cm} \\textbf{Step 0: Initialise parameters so some values: }$\n",
    "$$ \\phi = [\\phi_0, \\phi_1, \\dots, \\phi_k]^T$$\n",
    "\n",
    "\n",
    "$$\\boxed{\\begin{aligned}\n",
    "&\\textbf{Step 1: Up-hill - Compute the derivatives of the loss with respect to the parameters} \\\\\n",
    "&\\hspace{6cm} \\frac{\\partial L}{\\partial \\phi} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\phi_0} \\\\ \\frac{\\partial L}{\\partial \\phi_1} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial \\phi_k} \\end{bmatrix} \\\\[1em]\n",
    "&\\textbf{Step 2: Down-Hill - Update the parameters according to the rule} \\\\\n",
    "&\\hspace{6cm} \\phi = \\phi - \\alpha \\cdot \\frac{\\partial L}{\\partial \\phi}\n",
    "\\end{aligned}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5175",
   "metadata": {},
   "source": [
    "### 1D Linear Regression Example \n",
    "\n",
    "In the first chapter we showed the closed form, now we'll present the iterative solution to this problem.\n",
    "\n",
    "$$\\boxed{\\text{model: } f[x, \\phi] \\qquad \\text{parameters:} \\phi = [\\phi_0, \\phi_1]^T \\qquad \\text{Input: }x \\in \\mathbb{R}}$$\n",
    "\n",
    "$$\\boxed{y = f[x, \\phi] = \\phi_0 + \\phi_1 x}$$\n",
    "\n",
    "We know that our loss is the mean squared Error\n",
    "\n",
    "$$L[\\phi] = \\sum_{i=1}^N l_i = \\sum_{i=1}^N (f[x_i, \\phi] - y_i)^2 =  \\sum_{i=1}^N (\\phi_0 + \\phi_1 x - y_i)^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d986a",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1** \n",
    "\n",
    "$$\\frac{\\partial L[\\phi]}{\\partial \\phi} = \\frac{\\partial }{\\partial \\phi}\\sum_{i=1}^N l_i \\underset{\\text{linearity }}{=} \\sum_{i=1}^N\\frac{\\partial l_i}{\\partial \\phi} = \\sum_{i=1}^N \\begin{bmatrix} \\frac{\\partial l_i}{\\partial \\phi_0} \\\\ \\frac{\\partial l_i}{\\partial \\phi_1} \\end{bmatrix} = \\sum_{i=1}^N \\begin{bmatrix} 2\\cdot 1(\\phi_0 + \\phi_1x_i - y_i) \\\\ 2 x_i(\\phi_0 + \\phi_1x_i - y_i)\\cdot \\end{bmatrix}$$\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "$$ \\phi = \\phi - \\alpha \\frac{\\partial L}{\\partial \\phi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36eda4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "**Gradient Descent Visualization:**\n",
    "\n",
    "<img src=\"./images/chap5/1dgradDescent_reg.gif\" alt=\"Gradient Descent Animation\" width=\"500\" />\n",
    "<img src=\"./images/chap5/lossSpace.png\" alt=\"Loss Space\" width=\"390\" />\n",
    "\n",
    "*If the animation doesn't display, [click here to view the video](./images/chap5/1dgradDescent_reg.gif)*\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
