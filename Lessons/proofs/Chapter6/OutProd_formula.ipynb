{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ab43b8",
   "metadata": {},
   "source": [
    "**Tip: Follow this proof with small values and see how it comes together**\n",
    "## Proof: Why the Outer Product Formula Works\n",
    "\n",
    "### Theorem\n",
    "\n",
    "For a linear layer $\\mathbf{f}_k = \\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_k$, the gradient with respect to the weight matrix is:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_k} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T}$$\n",
    "\n",
    "This is **always** true, regardless of network depth or dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### Proof\n",
    "\n",
    "**Given:**\n",
    "- $\\mathbf{f}_k \\in \\mathbb{R}^{m}$ (output of layer $k$)\n",
    "- $\\boldsymbol{\\Omega}_k \\in \\mathbb{R}^{m \\times n}$ (weight matrix)\n",
    "- $\\mathbf{h}_k \\in \\mathbb{R}^{n}$ (input to layer $k$)\n",
    "- $\\mathbf{f}_k = \\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_k$\n",
    "\n",
    "**Step 1: Write the forward pass component-wise**\n",
    "\n",
    "For each output component $i \\in \\{1, \\ldots, m\\}$:\n",
    "\n",
    "$$f_{k,i} = \\beta_{k,i} + \\sum_{j=1}^{n} \\Omega_{k,ij} h_{k,j}$$\n",
    "\n",
    "**Step 2: Compute the partial derivative**\n",
    "\n",
    "Taking the derivative with respect to weight $\\Omega_{k,pq}$:\n",
    "\n",
    "$$\\frac{\\partial f_{k,i}}{\\partial \\Omega_{k,pq}} = \\begin{cases}\n",
    "h_{k,q} & \\text{if } i = p \\\\\n",
    "0 & \\text{if } i \\neq p\n",
    "\\end{cases}$$\n",
    "\n",
    "**Why?** Because $f_{k,i}$ only depends on row $i$ of $\\boldsymbol{\\Omega}_k$.\n",
    "\n",
    "**Step 3: Apply the chain rule**\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\Omega_{k,pq}} = \\sum_{i=1}^{m} \\frac{\\partial l_i}{\\partial f_{k,i}} \\cdot \\frac{\\partial f_{k,i}}{\\partial \\Omega_{k,pq}}$$\n",
    "\n",
    "Since $\\frac{\\partial f_{k,i}}{\\partial \\Omega_{k,pq}} = 0$ when $i \\neq p$, only one term survives:\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\Omega_{k,pq}} = \\frac{\\partial l_i}{\\partial f_{k,p}} \\cdot h_{k,q}$$\n",
    "\n",
    "**Step 4: Express in matrix form**\n",
    "\n",
    "Building the full gradient matrix element by element:\n",
    "\n",
    "$$\\left[\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_k}\\right]_{pq} = \\frac{\\partial l_i}{\\partial f_{k,p}} \\cdot h_{k,q}$$\n",
    "\n",
    "This is exactly the $(p,q)$ element of the outer product:\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_k} = \\begin{bmatrix}\n",
    "\\frac{\\partial l_i}{\\partial f_{k,1}} \\\\\n",
    "\\frac{\\partial l_i}{\\partial f_{k,2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l_i}{\\partial f_{k,m}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{k,1} & h_{k,2} & \\cdots & h_{k,n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$= \\frac{\\partial l_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T \\quad \\blacksquare$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Bypasses the Tensor\n",
    "\n",
    "**The key observation:** The intermediate Jacobian tensor $\\frac{\\partial \\mathbf{f}_k}{\\partial \\boldsymbol{\\Omega}_k}$ has a **very specific sparse structure**:\n",
    "\n",
    "$$\\left[\\frac{\\partial \\mathbf{f}_k}{\\partial \\boldsymbol{\\Omega}_k}\\right]_{i,p,q} = \\begin{cases}\n",
    "h_{k,q} & \\text{if } i = p \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "When we contract this tensor with $\\frac{\\partial l_i}{\\partial \\mathbf{f}_k}$, the sparsity pattern causes all cross-terms to vanish, leaving only the outer product!\n",
    "\n",
    "**Mathematically:**\n",
    "- **Without simplification:** Compute a $(m \\times m \\times n)$ tensor, then contract â†’ $O(m^2n)$ operations\n",
    "- **With outer product:** Compute two vectors and their outer product â†’ $O(m + n + mn)$ operations\n",
    "\n",
    "For large networks, this is a **massive** computational savings!\n",
    "\n",
    "---\n",
    "\n",
    "### General Pattern for All Layers\n",
    "\n",
    "This proof works **identically** for every layer in the network:\n",
    "\n",
    "| Layer | Weight Gradient |\n",
    "|-------|----------------|\n",
    "| $k=0$ | $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_0} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_0} \\mathbf{x}^T$ |\n",
    "| $k=1$ | $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_1} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_1} \\mathbf{h}_1^T$ |\n",
    "| $k=2$ | $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_2} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_2} \\mathbf{h}_2^T$ |\n",
    "| $k=3$ | $\\frac{\\partial l_i}{\\partial \\boldsymbol{\\Omega}_3} = \\frac{\\partial l_i}{\\partial \\mathbf{f}_3} \\mathbf{h}_3^T$ |\n",
    "\n",
    "The pattern is **universal** because the proof only relies on the structure of linear transformations, not on specific activation functions or network architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **The tensor exists theoretically** as $\\frac{\\partial \\mathbf{f}_k}{\\partial \\boldsymbol{\\Omega}_k} \\in \\mathbb{R}^{m \\times m \\times n}$\n",
    "\n",
    "2. **The tensor has special structure**: it's mostly zeros with $h_{k,q}$ appearing only when $i = p$\n",
    "\n",
    "3. **The chain rule contraction** with $\\frac{\\partial l_i}{\\partial \\mathbf{f}_k}$ exploits this structure\n",
    "\n",
    "4. **The result simplifies** to the outer product $\\frac{\\partial l_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T$\n",
    "\n",
    "5. **This is provably correct** for any linear layer in any neural network\n",
    "\n",
    "This is why backpropagation doesn't need to explicitly construct or store tensorsâ€”the mathematical structure guarantees that the outer product formula gives the correct answer efficiently! ðŸŽ¯\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4727f31",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "### Foundational Papers\n",
    "\n",
    "1. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).** \"Learning representations by back-propagating errors.\" *Nature*, 323(6088), 533-536.\n",
    "   - The original backpropagation paper that introduced the computational pattern\n",
    "\n",
    "2. **LeCun, Y., Bottou, L., Orr, G. B., & MÃ¼ller, K. R. (1998).** \"Efficient BackProp.\" In *Neural Networks: Tricks of the Trade* (pp. 9-50). Springer.\n",
    "   - Practical insights into efficient gradient computation\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "3. **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** *Deep Learning*. MIT Press.\n",
    "   - **Chapter 6.5:** Comprehensive treatment of backpropagation with matrix calculus\n",
    "   - Available online: https://www.deeplearningbook.org/\n",
    "\n",
    "4. **Bishop, C. M. (2006).** *Pattern Recognition and Machine Learning*. Springer.\n",
    "   - **Section 5.3:** Rigorous mathematical derivation of error backpropagation\n",
    "   - Excellent for understanding the theoretical foundations\n",
    "\n",
    "5. **Nielsen, M. (2015).** *Neural Networks and Deep Learning*. Determination Press.\n",
    "   - **Chapter 2:** Intuitive explanation of backpropagation with visual examples\n",
    "   - Free online: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "### Matrix Calculus References\n",
    "\n",
    "6. **Petersen, K. B., & Pedersen, M. S. (2012).** *The Matrix Cookbook*.\n",
    "   - **Section 2.4:** Matrix derivatives and the outer product formula\n",
    "   - Essential reference for matrix calculus in machine learning\n",
    "   - Available: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "\n",
    "7. **Magnus, J. R., & Neudecker, H. (2019).** *Matrix Differential Calculus with Applications in Statistics and Econometrics* (3rd ed.). Wiley.\n",
    "   - Comprehensive treatment of matrix derivatives\n",
    "\n",
    "### Modern Perspectives\n",
    "\n",
    "8. **Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018).** \"Automatic differentiation in machine learning: a survey.\" *Journal of Machine Learning Research*, 18(153), 1-43.\n",
    "   - Modern view of automatic differentiation and backpropagation\n",
    "   - Connects classical backprop to modern autodiff frameworks\n",
    "\n",
    "### Online Resources\n",
    "\n",
    "9. **Stanford CS231n: Convolutional Neural Networks for Visual Recognition**\n",
    "   - Lecture notes on backpropagation with clear derivations\n",
    "   - Available: http://cs231n.stanford.edu/\n",
    "\n",
    "10. **Olah, C. (2015).** \"Calculus on Computational Graphs: Backpropagation.\" *Colah's Blog*.\n",
    "    - Excellent visual explanation of backpropagation\n",
    "    - Available: https://colah.github.io/posts/2015-08-Backprop/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
