{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d487c0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Mathematical Formulation of test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617bd0a",
   "metadata": {},
   "source": [
    "$\\text{Consider a 1D regression problem where the data generation process has additive noise with variance } \\sigma^2 . \\\\ \\text{We can observe different outputs } y \\text{ for the same input } x \\text{, so for each } x \\ \\exists \\text{ a distrbution } Pr(y | x) \\text{ with expected value } \\mu[x]:$\n",
    "\n",
    "$$\\mu[x] = \\mathbb{E}_y[y[x]] = \\int y[x] Pr(y | x) dy$$\n",
    "\n",
    "$$\\sigma^2 = \\mathbb{E}_y[(\\mu[x] - y[x])^2]$$\n",
    "\n",
    "Where $y[x]$ is denoting output given input x. \n",
    "\n",
    "We now consider the loss function. \n",
    "\n",
    "$$\\begin{align} L[x] \n",
    "&= (f[x, \\phi] - y[x])^2 \\\\\n",
    "&= (f[x, \\phi]-\\mu[x] + \\mu[x] -y[x])^2 \\\\\n",
    "&= ((f[x, \\phi]-\\mu[x]) + (\\mu[x] -y[x]))^2 \\\\\n",
    "&= (f[x, \\phi]-\\mu[x])^2 + 2(f[x, \\phi]-\\mu[x])(\\mu[x] -y[x]) + (\\mu[x] -y[x])^2 \\\\ \n",
    "\\end{align}$$\n",
    "\n",
    "We now ask ourself  \"What is the expected loss?\"\n",
    "\n",
    "$$\\begin{align} \\mathbb{E}_y[L[x]]\n",
    "&= \\mathbb{E}_y \\left[(f[x, \\phi]-\\mu[x])^2 + 2(f[x, \\phi]-\\mu[x])(\\mu[x] -y[x]) + (\\mu[x] -y[x])^2\\right] \\\\ \n",
    "&= (f[x, \\phi]-\\mu[x])^2 + 2(f[x, \\phi]-\\mu[x])(\\mu[x] -\\mathbb{E}_y[y[x]])+ \\mathbb{E}_y[(\\mu[x] -y[x])^2 ]\\\\\n",
    "&= (f[x, \\phi]-\\mu[x])^2 + 2(f[x, \\phi]-\\mu[x])(0)+ \\mathbb{E}_y[(\\mu[x] -y[x])^2 ]\\\\\n",
    "&= (f[x, \\phi]-\\mu[x])^2  + \\sigma^2\n",
    "\\end{align}$$\n",
    "\n",
    "The first term can be further broken down into the bias and variance.\n",
    "\n",
    "The parameters $\\phi$ of the model $f[x, \\phi]$ depend on the training dataset $D = \\{x_i, y_i\\}$ so $f[x, \\phi] = f[x, \\phi[D]]$\n",
    "\n",
    "The training dataset is a random sample from the data generation process. \n",
    "\n",
    "The expected model output $f_{\\mu}[x]$ with respect to all possible datasets $D$ is therefore: \n",
    "\n",
    "$$f_{\\mu}[x] = \\mathbb{E}_D[f[x, \\phi[D]]]$$\n",
    "\n",
    "We now go back to the first term (We'll do the same trick as before):\n",
    "\n",
    "$$\\begin{align} (f[x, \\phi[D]] - \\mu[x])^2 \n",
    "&= (f[x, \\phi[D]] - f_{\\mu}[x] + f_{\\mu}[x] -\\mu[x])^2 \\\\ \n",
    "&= ((f[x, \\phi[D]] - f_{\\mu}[x]) + (f_{\\mu}[x] -\\mu[x]))^2 \\\\ \n",
    "&= (f[x, \\phi[D]] - f_{\\mu}[x])^2 + 2(f[x, \\phi[D]] - f_{\\mu}[x])(f_{\\mu}[x] -\\mu[x]) + (f_{\\mu}[x] -\\mu[x])^2\n",
    "\\end{align}$$\n",
    "\n",
    "We now ask ourself  \"What is the expected result?\"\n",
    "\n",
    "$$\\begin{align} \\mathbb{E}_D[(f[x, \\phi[D]] - \\mu[x])^2 ]\n",
    "&= \\mathbb{E}_D\\left[(f[x, \\phi[D]] - f_{\\mu}[x])^2 + 2(f[x, \\phi[D]] - f_{\\mu}[x])(f_{\\mu}[x] -\\mu[x]) + (f_{\\mu}[x] -\\mu[x])^2\\right] \\\\\n",
    "&= \\mathbb{E}_D\\left[(f[x, \\phi[D]] - f_{\\mu}[x])^2\\right] + 2(\\mathbb{E}_D[f[x, \\phi[D]]] - f_{\\mu}[x])(f_{\\mu}[x] -\\mu[x])+ (f_{\\mu}[x] -\\mu[x])^2 \\\\\n",
    "&= \\mathbb{E}_D\\left[(f[x, \\phi[D]] - f_{\\mu}[x])^2\\right] + 2(0)(f_{\\mu}[x] -\\mu[x])+ (f_{\\mu}[x] -\\mu[x])^2 \\\\\n",
    "&= \\underbrace{\\mathbb{E}_D\\left[(f[x, \\phi[D]] - f_{\\mu}[x])^2\\right]}_{\\text{Variance}} + \\underbrace{(f_{\\mu}[x] -\\mu[x])^2}_{\\text{Bias}^2}\n",
    "\\end{align}$$\n",
    "\n",
    "Substituting back into the above equation:\n",
    "\n",
    "$$ \\mathbb{E}_D \\left[\\mathbb{E}_y[L[x]]\\right] = \\underbrace{\\mathbb{E}_D\\left[(f[x, \\phi[D]] - f_{\\mu}[x])^2\\right]}_{\\text{Variance}} + \\underbrace{(f_{\\mu}[x] -\\mu[x])^2}_{\\text{Bias}^2} + \\underbrace{\\sigma^2}_{\\text{Irreducible Noise}}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
