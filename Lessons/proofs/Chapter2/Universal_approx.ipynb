{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0151b355",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# An elementary proof of a universal approximation theorem\n",
    "\n",
    "Chris Monico <br>\n",
    "Department of Mathematics and Statistics <br>\n",
    "Texas Tech University <br>\n",
    "e-mail: c.monico@ttu.edu <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db0d1e",
   "metadata": {},
   "source": [
    "Abstract\n",
    "\n",
    "In this short note, we give an elementary proof of a universal approximation theorem for neural networks with three hidden layers and increasing, continuous, bounded activation function. The result is weaker than the best known results, but the proof is elementary in the sense that no machinery beyond undergraduate analysis is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2e28d",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "There are several versions of universal approximation theorems known, including the very well-known ones from [1, 2, 3]. Each of them states that some collection of neural networks is dense in some space of continuous functions with respect to the uniform norm. In this short note, we present what we believe to be a new and atypically elementary proof of one such theorem. If $\\sigma$ is a 0-1 squashing function (a.k.a. a $sigmoidal \\ function$), we show that\n",
    "the collection of neural networks with three hidden layers and activation function σ (exceptat the output) is dense in the space C(K) of real-valued continuous functions on a compact set K ⊂ n. The result given here is weaker than the best known results, but the argument\n",
    "relies only on basic results about compact sets and continuous functions which are generally\n",
    "covered in an undergraduate analysis course; it is really nothing more than an exercise in\n",
    "“epsilon chasing,” though the underlying intuitive motivation is fairly natural."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
