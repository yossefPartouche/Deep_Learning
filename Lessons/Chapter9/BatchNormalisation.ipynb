{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b256bc",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d8243",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### What is Batch Normalization?\n",
    "\n",
    "**Batch Normalization (BatchNorm)** is a technique that normalizes the inputs to each layer during training, making the network easier to train and more stable.\n",
    "\n",
    "**Key Idea:** Normalize activations to have **mean ≈ 0** and **standard deviation ≈ 1**, then apply learnable scaling and shifting.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Do We Need Batch Normalization?\n",
    "\n",
    "**Problems it solves:**\n",
    "\n",
    "1. **Internal Covariate Shift**: As network trains, the distribution of layer inputs changes, forcing later layers to constantly adapt\n",
    "   - BatchNorm stabilizes these distributions\n",
    "\n",
    "2. **Vanishing/Exploding Gradients**: Helps maintain reasonable gradient magnitudes throughout the network\n",
    "   - Allows higher learning rates\n",
    "\n",
    "3. **Sensitivity to Initialization**: Reduces dependence on careful weight initialization\n",
    "\n",
    "4. **Regularization Effect**: Acts as a form of regularization, reducing overfitting\n",
    "   - Can reduce/eliminate need for Dropout\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Faster training (can use higher learning rates)\n",
    "- ✅ Less sensitive to initialization\n",
    "- ✅ Acts as regularization\n",
    "- ✅ Reduces internal covariate shift\n",
    "- ✅ Can eliminate need for Dropout in some cases\n",
    "\n",
    "---\n",
    "\n",
    "#### General Batch Normalization Algorithm\n",
    "\n",
    "**For a batch of activations**, BatchNorm performs 4 steps:\n",
    "\n",
    "| **Step** | **Operation** | **Formula** | **Purpose** |\n",
    "|----------|---------------|-------------|-------------|\n",
    "| **1. Compute Batch Statistics** | Calculate mean and variance across the batch | $$\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$ $$\\sigma^2_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$$ | Get statistics for normalization <br> $m$ = batch size |\n",
    "| **2. Normalize** | Subtract mean, divide by std deviation | $$\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}}$$ | Force distribution to mean=0, std=1 <br> $\\epsilon$ prevents division by zero (typically $10^{-5}$) |\n",
    "| **3. Scale** | Multiply by learnable parameter | $$y_i = \\gamma \\cdot \\hat{x}_i$$ | Allow network to **learn optimal scale** <br> $\\gamma$ is learned during training |\n",
    "| **4. Shift** | Add learnable parameter | $$y_i = \\gamma \\cdot \\hat{x}_i + \\beta$$ | Allow network to **learn optimal mean** <br> $\\beta$ is learned during training |\n",
    "\n",
    "**Complete Formula:**\n",
    "\n",
    "$$\\boxed{y_i = \\gamma \\cdot \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}} + \\beta}$$\n",
    "\n",
    "**Key Insight:** $\\gamma$ and $\\beta$ allow the network to undo the normalization if needed!\n",
    "- If $\\gamma = \\sqrt{\\sigma^2_{\\mathcal{B}}}$ and $\\beta = \\mu_{\\mathcal{B}}$, we recover the original values\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4be99",
   "metadata": {},
   "source": [
    "### Understanding the Learnable Parameters (γ and β)\n",
    "\n",
    "**Why Do We Need Learnable Parameters?**\n",
    "\n",
    "After normalization, all activations have mean=0 and std=1. <br>\n",
    "But this might not be optimal for learning! The learnable parameters $\\gamma$ (gamma) and $\\beta$ (beta) give the network **flexibility** to:\n",
    "\n",
    "1. **Undo the normalization if needed**\n",
    "2. **Learn the optimal distribution** for each feature/channel\n",
    "3. **Preserve representational power** of the network\n",
    "\n",
    "---\n",
    "\n",
    "#### The Mathematics Behind γ and β\n",
    "\n",
    "**After normalization**, we have:\n",
    "$$\\hat{x} = \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}}$$\n",
    "\n",
    "Where $\\hat{x}$ has mean=0 and std=1.\n",
    "\n",
    "**Then we apply scale and shift**:\n",
    "$$\\boxed{y = \\gamma \\cdot \\hat{x} + \\beta}$$\n",
    "\n",
    "**Key insight:** If the network learns:\n",
    "- $\\gamma = \\sqrt{\\sigma^2_{\\mathcal{B}}}$ (the original std)\n",
    "- $\\beta = \\mu_{\\mathcal{B}}$ (the original mean)\n",
    "\n",
    "Then: $y = \\sqrt{\\sigma^2_{\\mathcal{B}}} \\cdot \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}} + \\mu_{\\mathcal{B}} \\approx x$ (recovers original input!)\n",
    "\n",
    "**This means:** The network can learn to **disable** batch normalization if it's not helpful!\n",
    "\n",
    "---\n",
    "\n",
    "#### What Do γ and β Learn?\n",
    "\n",
    "| **Scenario** | **Learned Values** | **Effect** | **Interpretation** |\n",
    "|--------------|-------------------|------------|-------------------|\n",
    "| **Standard Normalization** | $\\gamma = 1$ <br> $\\beta = 0$ | $y = \\hat{x}$ | Keep normalized distribution: mean=0, std=1 <br> This is the initialization |\n",
    "| **Undo Normalization** | $\\gamma = \\sqrt{\\sigma^2_{\\mathcal{B}}}$ <br> $\\beta = \\mu_{\\mathcal{B}}$ | $y \\approx x$ | Recover original distribution <br> Network decides normalization isn't helpful |\n",
    "| **Increase Variance** | $\\gamma = 2$ <br> $\\beta = 0$ | $y = 2\\hat{x}$ | Distribution: mean=0, std=2 <br> Wider spread of values |\n",
    "| **Shift Mean** | $\\gamma = 1$ <br> $\\beta = 3$ | $y = \\hat{x} + 3$ | Distribution: mean=3, std=1 <br> Shift activation threshold |\n",
    "| **Custom Distribution** | $\\gamma = 0.5$ <br> $\\beta = -2$ | $y = 0.5\\hat{x} - 2$ | Distribution: mean=-2, std=0.5 <br> Network learns optimal values |\n",
    "\n",
    "---\n",
    "\n",
    "#### Detailed Example: How γ and β Work\n",
    "\n",
    "**Setup:**\n",
    "- Normalized values: $\\hat{x} = [-1.5, -0.5, 0, 0.5, 1.5]$ (mean=0, std≈1)\n",
    "- We'll see different $(\\gamma, \\beta)$ effects\n",
    "\n",
    "| **Parameters** | **Computation** | **Output** | **Distribution** |\n",
    "|----------------|-----------------|------------|------------------|\n",
    "| $\\gamma=1, \\beta=0$ <br> (Standard) | $y = 1 \\cdot \\hat{x} + 0$ | $[-1.5, -0.5, 0, 0.5, 1.5]$ | Mean = 0 <br> Std ≈ 1 <br> (unchanged) |\n",
    "| $\\gamma=2, \\beta=0$ <br> (Scale up) | $y = 2 \\cdot \\hat{x} + 0$ | $[-3, -1, 0, 1, 3]$ | Mean = 0 <br> Std ≈ 2 <br> (doubled variance) |\n",
    "| $\\gamma=1, \\beta=5$ <br> (Shift up) | $y = 1 \\cdot \\hat{x} + 5$ | $[3.5, 4.5, 5, 5.5, 6.5]$ | Mean = 5 <br> Std ≈ 1 <br> (shifted right) |\n",
    "| $\\gamma=0.5, \\beta=-1$ <br> (Scale & shift) | $y = 0.5 \\cdot \\hat{x} - 1$ | $[-1.75, -1.25, -1, -0.75, -0.25]$ | Mean = -1 <br> Std ≈ 0.5 <br> (compressed & shifted) |\n",
    "| $\\gamma=3, \\beta=10$ <br> (Large scale & shift) | $y = 3 \\cdot \\hat{x} + 10$ | $[5.5, 8.5, 10, 11.5, 14.5]$ | Mean = 10 <br> Std ≈ 3 <br> (very wide, high mean) |\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters: Activation Functions\n",
    "\n",
    "Different activation functions work best with different input distributions:\n",
    "\n",
    "| **Activation** | **Optimal Input Range** | **How γ, β Help** |\n",
    "|----------------|------------------------|-------------------|\n",
    "| **ReLU** | Positive values work best <br> (negatives → 0) | Learn $\\beta > 0$ to shift distribution positive <br> More neurons stay active |\n",
    "| **Sigmoid** | Works best around [-2, 2] <br> (saturates outside) | Learn $\\gamma$ to compress values <br> Learn $\\beta$ to center around 0 |\n",
    "| **Tanh** | Works best around [-1, 1] <br> (saturates outside) | Learn $\\gamma < 1$ to compress <br> Keep $\\beta \\approx 0$ |\n",
    "| **Leaky ReLU** | Works for any range | Less sensitive, but can still optimize distribution |\n",
    "\n",
    "**Example with ReLU:**\n",
    "\n",
    "```\n",
    "Scenario 1: Without learnable parameters\n",
    "  Normalized: [-1.5, -0.5, 0.5, 1.5]  (mean=0)\n",
    "  After ReLU: [0, 0, 0.5, 1.5]        (50% neurons dead!)\n",
    "\n",
    "Scenario 2: With learned β=2\n",
    "  Scaled: [-1.5, -0.5, 0.5, 1.5] + 2 = [0.5, 1.5, 2.5, 3.5]\n",
    "  After ReLU: [0.5, 1.5, 2.5, 3.5]   (all neurons active!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### How Are γ and β Learned?\n",
    "\n",
    "**During backpropagation**, gradients flow through:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial y_i} \\cdot \\hat{x}_i$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial y_i}$$\n",
    "\n",
    "**Intuition:**\n",
    "- If increasing $\\gamma$ reduces loss → $\\gamma$ increases (scale up)\n",
    "- If shifting $\\beta$ upward reduces loss → $\\beta$ increases (shift up)\n",
    "- Optimized using same optimizer as weights (SGD, Adam, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Number of Learnable Parameters\n",
    "\n",
    "| **Layer Type** | **Input Shape** | **Normalization Per** | **Parameters** | **Example** |\n",
    "|----------------|-----------------|----------------------|----------------|-------------|\n",
    "| **BatchNorm1d** | $(N, D)$ | Feature | $2D$ | $D=512$ features <br> → 512 $\\gamma$ + 512 $\\beta$ <br> = **1,024 params** |\n",
    "| **BatchNorm2d** | $(N, C, H, W)$ | Channel | $2C$ | $C=64$ channels <br> → 64 $\\gamma$ + 64 $\\beta$ <br> = **128 params** |\n",
    "\n",
    "**Note:** This is **tiny** compared to convolutional or linear layer parameters!\n",
    "\n",
    "**Example comparison:**\n",
    "```\n",
    "Conv2d(3, 64, kernel_size=3):\n",
    "  Parameters: 64 × 3 × 3 × 3 = 1,728\n",
    "\n",
    "BatchNorm2d(64):\n",
    "  Parameters: 64 × 2 = 128\n",
    "\n",
    "Ratio: 1,728 / 128 ≈ 13.5×\n",
    "BatchNorm adds minimal parameters!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Initialization of γ and β\n",
    "\n",
    "**Default initialization** (PyTorch, TensorFlow):\n",
    "- $\\gamma = 1$ (initialized as ones)\n",
    "- $\\beta = 0$ (initialized as zeros)\n",
    "\n",
    "**Why?**\n",
    "- Starts as **identity transformation**: $y = 1 \\cdot \\hat{x} + 0 = \\hat{x}$\n",
    "- Keeps normalized distribution initially\n",
    "- Lets the network **learn** to adjust if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae15b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Batch Normalization for Different Data Types\n",
    "\n",
    "The **dimension along which we compute statistics** varies by data type:\n",
    "\n",
    "| **Data Type** | **Input Shape** | **Normalization Dimension** | **Learnable Parameters** | **Use Case** |\n",
    "|---------------|-----------------|----------------------------|--------------------------|--------------|\n",
    "| **Fully Connected (1D)** | $(N, D)$ <br> $N$ = batch size <br> $D$ = features | Normalize across **batch dimension** $N$ <br> Each feature has its own $\\mu, \\sigma$ | $\\gamma, \\beta \\in \\mathbb{R}^D$ <br> One pair per feature | Dense/FC layers |\n",
    "| **Convolutional (2D)** | $(N, C, H, W)$ <br> $N$ = batch <br> $C$ = channels <br> $H, W$ = spatial | Normalize across **batch $N$ and spatial dimensions $H, W$** <br> Each channel has its own $\\mu, \\sigma$ | $\\gamma, \\beta \\in \\mathbb{R}^C$ <br> One pair per channel | CNNs, image data |\n",
    "| **Recurrent (1D sequence)** | $(N, T, D)$ <br> $N$ = batch <br> $T$ = time steps <br> $D$ = features | Normalize across **batch dimension** $N$ <br> Each feature at each timestep | $\\gamma, \\beta \\in \\mathbb{R}^D$ <br> One pair per feature | RNNs, LSTMs |\n",
    "\n",
    "---\n",
    "\n",
    "#### 1D Batch Normalization (Fully Connected Layers)\n",
    "\n",
    "**Input Shape:** $(N, D)$ where $N$ = batch size, $D$ = number of features\n",
    "\n",
    "**Normalization:** Across the **batch dimension** for each feature independently\n",
    "\n",
    "| **Example** | **Setup** | **Computation** | **Result** |\n",
    "|-------------|-----------|-----------------|------------|\n",
    "| **Simple Example** | **Input**: Batch of 3 samples, 2 features <br> $$X = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}$$ <br> Shape: $(3, 2)$ | **Feature 1** (column 1): <br> $\\mu_1 = \\frac{1+2+3}{3} = 2$ <br> $\\sigma^2_1 = \\frac{(1-2)^2+(2-2)^2+(3-2)^2}{3} = \\frac{2}{3}$ <br> $\\sigma_1 = \\sqrt{\\frac{2}{3}} \\approx 0.816$ <br><br> **Feature 2** (column 2): <br> $\\mu_2 = \\frac{4+5+6}{3} = 5$ <br> $\\sigma^2_2 = \\frac{(4-5)^2+(5-5)^2+(6-5)^2}{3} = \\frac{2}{3}$ <br> $\\sigma_2 = \\sqrt{\\frac{2}{3}} \\approx 0.816$ | **Normalized** (before $\\gamma, \\beta$): <br> $$\\hat{X} = \\begin{bmatrix} \\frac{1-2}{0.816} & \\frac{4-5}{0.816} \\\\ \\frac{2-2}{0.816} & \\frac{5-5}{0.816} \\\\ \\frac{3-2}{0.816} & \\frac{6-5}{0.816} \\end{bmatrix} = \\begin{bmatrix} -1.22 & -1.22 \\\\ 0 & 0 \\\\ 1.22 & 1.22 \\end{bmatrix}$$ <br><br> Each column has mean=0, std=1 |\n",
    "| **With Learnable Parameters** | Suppose: <br> $\\gamma = [2, 0.5]$ <br> $\\beta = [1, -1]$ | Apply: $y = \\gamma \\cdot \\hat{x} + \\beta$ <br><br> **Feature 1**: <br> $y_1 = 2 \\times (-1.22) + 1 = -1.44$ <br> $y_2 = 2 \\times 0 + 1 = 1$ <br> $y_3 = 2 \\times 1.22 + 1 = 3.44$ <br><br> **Feature 2**: <br> $y_1 = 0.5 \\times (-1.22) - 1 = -1.61$ <br> $y_2 = 0.5 \\times 0 - 1 = -1$ <br> $y_3 = 0.5 \\times 1.22 - 1 = -0.39$ | **Final Output**: <br> $$Y = \\begin{bmatrix} -1.44 & -1.61 \\\\ 1 & -1 \\\\ 3.44 & -0.39 \\end{bmatrix}$$ <br><br> Network learned to scale and shift each feature |\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input: batch_size=3, features=2\n",
    "x = torch.tensor([[1., 4.], [2., 5.], [3., 6.]])\n",
    "\n",
    "# BatchNorm1d: normalize across batch dimension\n",
    "bn = nn.BatchNorm1d(num_features=2)  # 2 features → 2 pairs of (γ, β)\n",
    "\n",
    "# Forward pass\n",
    "output = bn(x)\n",
    "print(output)\n",
    "\n",
    "# Parameters\n",
    "print(f\"Gamma (scale): {bn.weight.data}\")  # Shape: (2,)\n",
    "print(f\"Beta (shift): {bn.bias.data}\")     # Shape: (2,)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Parameters**: $2D$ learnable parameters ($D$ gammas + $D$ betas)\n",
    "- **Statistics**: Computed per feature across the batch\n",
    "- **Typical placement**: After linear layer, before activation\n",
    "\n",
    "---\n",
    "\n",
    "#### 2D Batch Normalization (Convolutional Layers)\n",
    "\n",
    "**Input Shape:** $(N, C, H, W)$ where:\n",
    "- $N$ = batch size\n",
    "- $C$ = number of channels\n",
    "- $H, W$ = spatial dimensions (height, width)\n",
    "\n",
    "**Normalization:** Across **batch dimension $N$** and **spatial dimensions $H, W$** for each channel independently\n",
    "\n",
    "**Key Difference from 1D:** Each channel gets one mean/variance computed across ALL spatial locations and ALL samples in the batch.\n",
    "\n",
    "| **Example** | **Setup** | **Computation** | **Result** |\n",
    "|-------------|-----------|-----------------|------------|\n",
    "| **Simple CNN Example** | **Input**: 2 samples, 2 channels, $2\\times2$ spatial <br> $$\\text{Sample 1, Channel 1} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$$ $$\\text{Sample 1, Channel 2} = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}$$ $$\\text{Sample 2, Channel 1} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}$$ $$\\text{Sample 2, Channel 2} = \\begin{bmatrix} 6 & 7 \\\\ 8 & 9 \\end{bmatrix}$$ <br> Shape: $(2, 2, 2, 2)$ | **Channel 1** (across both samples, all spatial positions): <br> Values: $[1, 2, 3, 4, 2, 3, 4, 5]$ (8 values total) <br> $\\mu_1 = \\frac{1+2+3+4+2+3+4+5}{8} = 3$ <br> $\\sigma^2_1 = \\frac{\\sum (x_i - 3)^2}{8} = 1.5$ <br> $\\sigma_1 = \\sqrt{1.5} \\approx 1.22$ <br><br> **Channel 2** (across both samples, all spatial positions): <br> Values: $[5, 6, 7, 8, 6, 7, 8, 9]$ (8 values total) <br> $\\mu_2 = \\frac{5+6+7+8+6+7+8+9}{8} = 7$ <br> $\\sigma^2_2 = \\frac{\\sum (x_i - 7)^2}{8} = 1.5$ <br> $\\sigma_2 = \\sqrt{1.5} \\approx 1.22$ | **Normalized** (before $\\gamma, \\beta$): <br><br> **Sample 1, Channel 1**: <br> $$\\begin{bmatrix} \\frac{1-3}{1.22} & \\frac{2-3}{1.22} \\\\ \\frac{3-3}{1.22} & \\frac{4-3}{1.22} \\end{bmatrix} = \\begin{bmatrix} -1.64 & -0.82 \\\\ 0 & 0.82 \\end{bmatrix}$$ <br><br> All values in channel 1 (across all samples) now have mean=0, std=1 <br><br> Same process for channel 2 |\n",
    "| **Visualization** | **Input dimensions**: <br> - Batch: $N = 4$ <br> - Channels: $C = 64$ <br> - Spatial: $H = 32, W = 32$ <br><br> Total activations: $4 \\times 64 \\times 32 \\times 32$ | **For Channel 1**: <br> - Collect all $4 \\times 32 \\times 32 = 4{,}096$ values <br> - Compute $\\mu_1, \\sigma_1$ from these 4,096 values <br> - Normalize all 4,096 values using this $\\mu_1, \\sigma_1$ <br><br> **For Channel 2**: <br> - Collect all $4 \\times 32 \\times 32 = 4{,}096$ values <br> - Compute $\\mu_2, \\sigma_2$ <br> - Normalize <br><br> Repeat for all 64 channels | **Output dimensions**: Same as input <br> $(4, 64, 32, 32)$ <br><br> **Parameters**: <br> - $\\gamma \\in \\mathbb{R}^{64}$ (one per channel) <br> - $\\beta \\in \\mathbb{R}^{64}$ (one per channel) <br> - Total: $128$ learnable parameters |\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input: batch_size=2, channels=3, height=4, width=4\n",
    "x = torch.randn(2, 3, 4, 4)\n",
    "\n",
    "# BatchNorm2d: normalize across batch and spatial dimensions\n",
    "bn = nn.BatchNorm2d(num_features=3)  # 3 channels → 3 pairs of (γ, β)\n",
    "\n",
    "# Forward pass\n",
    "output = bn(x)\n",
    "\n",
    "# Parameters\n",
    "print(f\"Gamma (scale): {bn.weight.data}\")  # Shape: (3,) - one per channel\n",
    "print(f\"Beta (shift): {bn.bias.data}\")     # Shape: (3,) - one per channel\n",
    "\n",
    "# Each channel is normalized independently\n",
    "# Channel 0: normalized across 2*4*4 = 32 values\n",
    "# Channel 1: normalized across 2*4*4 = 32 values\n",
    "# Channel 2: normalized across 2*4*4 = 32 values\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Parameters**: $2C$ learnable parameters ($C$ gammas + $C$ betas)\n",
    "- **Statistics**: Each channel has one $\\mu$ and one $\\sigma^2$ computed across:\n",
    "  - All samples in batch ($N$ dimension)\n",
    "  - All spatial locations ($H \\times W$ dimensions)\n",
    "- **Typical placement**: After convolution, before activation\n",
    "- **Spatial sharing**: Same $\\gamma$ and $\\beta$ applied to all spatial locations within a channel\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparison of Normalization Dimensions\n",
    "\n",
    "| **Aspect** | **BatchNorm1d (FC layers)** | **BatchNorm2d (Conv layers)** |\n",
    "|------------|----------------------------|------------------------------|\n",
    "| **Input Shape** | $(N, D)$ <br> $N$ = batch, $D$ = features | $(N, C, H, W)$ <br> $N$ = batch, $C$ = channels, $H, W$ = spatial |\n",
    "| **Normalize Over** | Batch dimension $N$ | Batch dimension $N$ + Spatial dimensions $H, W$ |\n",
    "| **Statistics per** | Feature (column) | Channel |\n",
    "| **Number of means/variances** | $D$ (one per feature) | $C$ (one per channel) |\n",
    "| **Learnable Parameters** | $\\gamma, \\beta \\in \\mathbb{R}^D$ <br> Total: $2D$ | $\\gamma, \\beta \\in \\mathbb{R}^C$ <br> Total: $2C$ |\n",
    "| **Values per statistic** | $N$ values <br> (all samples for one feature) | $N \\times H \\times W$ values <br> (all samples, all spatial locations for one channel) |\n",
    "| **Example** | Batch of 32, 512 features: <br> - 512 means <br> - 512 variances <br> - 1,024 learnable params | Batch of 32, 64 channels, $28 \\times 28$: <br> - 64 means <br> - 64 variances <br> - 128 learnable params <br> - Each statistic computed from $32 \\times 28 \\times 28 = 25{,}088$ values |\n",
    "\n",
    "---\n",
    "\n",
    "#### Training vs. Inference\n",
    "\n",
    "**During Training:**\n",
    "- Compute $\\mu, \\sigma^2$ from **current batch**\n",
    "- Update **running estimates** of population statistics using exponential moving average:\n",
    "  $$\\mu_{\\text{running}} \\leftarrow (1 - \\text{momentum}) \\cdot \\mu_{\\text{running}} + \\text{momentum} \\cdot \\mu_{\\mathcal{B}}$$\n",
    "  $$\\sigma^2_{\\text{running}} \\leftarrow (1 - \\text{momentum}) \\cdot \\sigma^2_{\\text{running}} + \\text{momentum} \\cdot \\sigma^2_{\\mathcal{B}}$$\n",
    "  - Default momentum: 0.1\n",
    "\n",
    "**During Inference:**\n",
    "- Use **fixed running statistics** (not batch statistics)\n",
    "- Reason: Batch size might be 1, or statistics might be unstable\n",
    "- Formula remains the same:\n",
    "  $$y = \\gamma \\cdot \\frac{x - \\mu_{\\text{running}}}{\\sqrt{\\sigma^2_{\\text{running}} + \\epsilon}} + \\beta$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Where to Place Batch Normalization?\n",
    "\n",
    "**Original Paper (2015):** Before activation\n",
    "```\n",
    "Conv/Linear → BatchNorm → ReLU\n",
    "```\n",
    "\n",
    "**Modern Practice:** After activation (often works better)\n",
    "```\n",
    "Conv/Linear → ReLU → BatchNorm\n",
    "```\n",
    "\n",
    "**Complete CNN Block with BatchNorm:**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Option 1: Before activation (original)\n",
    "block1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Option 2: After activation (modern)\n",
    "block2 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64)\n",
    ")\n",
    "\n",
    "# Option 3: With pooling\n",
    "block3 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical Considerations\n",
    "\n",
    "| **Aspect** | **Recommendation** | **Reason** |\n",
    "|------------|-------------------|------------|\n",
    "| **Batch Size** | ≥ 16, ideally 32+ | Small batches → unstable statistics <br> Use GroupNorm or LayerNorm for small batches |\n",
    "| **Learning Rate** | Can use higher rates (e.g., 0.01 instead of 0.001) | BatchNorm stabilizes training |\n",
    "| **Dropout** | Often unnecessary with BatchNorm | BatchNorm already provides regularization |\n",
    "| **Initialization** | Less critical | BatchNorm reduces sensitivity to initialization |\n",
    "| **Momentum** | Default 0.1 works well | Controls running statistics update rate |\n",
    "\n",
    "---\n",
    "\n",
    "#### Complete Example: Full Forward Pass\n",
    "\n",
    "**Setup:**\n",
    "- Input: $(2, 3, 4, 4)$ (2 samples, 3 channels, $4 \\times 4$ spatial)\n",
    "- BatchNorm2d with 3 channels\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 3, 4, 4)  # Shape: (N=2, C=3, H=4, W=4)\n",
    "\n",
    "# BatchNorm layer\n",
    "bn = nn.BatchNorm2d(num_features=3)\n",
    "\n",
    "# Set some example parameters\n",
    "bn.weight.data = torch.tensor([2.0, 1.5, 0.5])  # γ for each channel\n",
    "bn.bias.data = torch.tensor([1.0, -1.0, 0.0])   # β for each channel\n",
    "\n",
    "# Forward pass\n",
    "bn.train()  # Training mode: use batch statistics\n",
    "output = bn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Same: (2, 3, 4, 4)\n",
    "\n",
    "# For channel 0:\n",
    "# 1. Compute μ, σ² from 2*4*4 = 32 values\n",
    "# 2. Normalize: (x - μ) / σ\n",
    "# 3. Scale: 2.0 * normalized\n",
    "# 4. Shift: + 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "1. **BatchNorm normalizes activations** to stabilize training and allow higher learning rates\n",
    "\n",
    "2. **Different variants** for different data types:\n",
    "   - **1D (BatchNorm1d)**: Fully connected layers, normalize across batch per feature\n",
    "   - **2D (BatchNorm2d)**: Convolutional layers, normalize across batch + spatial dimensions per channel\n",
    "\n",
    "3. **Learnable parameters** ($\\gamma, \\beta$) allow network to undo normalization if needed\n",
    "\n",
    "4. **Training vs. Inference**:\n",
    "   - Training: Use batch statistics\n",
    "   - Inference: Use running statistics\n",
    "\n",
    "5. **Benefits**: Faster training, less sensitive to initialization, acts as regularization\n",
    "\n",
    "6. **Placement**: After Conv/Linear, typically before activation (but after activation also works)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc2cab",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "\n",
    "| **Aspect**         | **LayerNorm**                                      |\n",
    "|--------------------|----------------------------------------------------|\n",
    "| **Normalizes over**| Features (per sample, not batch)                   |\n",
    "| **Input shape**    | Any (e.g. $(N, D)$ for FC, $(N, T, D)$ for RNNs)   |\n",
    "| **Statistics per** | Each sample (mean, std across features)            |\n",
    "| **Learnable params**| $\\gamma, \\beta \\in \\mathbb{R}^D$ (one per feature)|\n",
    "| **Batch size needed**| No (works with batch size 1)                     |\n",
    "| **Typical use**    | RNNs, Transformers, small batch sizes              |\n",
    "| **Benefit**        | Stable normalization for sequence models           |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
